{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPWycLKR-Erv"
      },
      "source": [
        "#Pre Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jkg-82k-dTt"
      },
      "source": [
        "##Imports lib and connect with Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnzJYfcm-glv",
        "outputId": "9ef537d9-7c4f-4e67-b256-903123669c49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.21.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from wandb) (25.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.34.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.14.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 193
        }
      ],
      "source": [
        "# Import các library, classs và hàm cần thiết\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import datasets\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "# Cài và import wandb\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHgv4j9P4een"
      },
      "outputs": [],
      "source": [
        "# # Cài kaggle và tải data\n",
        "\n",
        "# !pip install kaggle\n",
        "# !mkdir ~/.kaggle\n",
        "\n",
        "# # Upload file kaggle.json vào Colab, rồi chạy:\n",
        "\n",
        "# !cp kaggle.json ~/.kaggle/\n",
        "# !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# # Download dataset CelebA\n",
        "\n",
        "# !kaggle datasets download -d jessicali9530/celeba-dataset\n",
        "\n",
        "# # Giải nén\n",
        "\n",
        "# !unzip celeba-dataset.zip -d /content/celeba"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBpwcrjIurEj"
      },
      "source": [
        "##Setup Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15V_zWtNusv4",
        "outputId": "c2d056ed-9bef-42cf-f99b-73a61cc4bde0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['img_align_celeba']\n"
          ]
        }
      ],
      "source": [
        "# Định nghĩa root để ảnh\n",
        "celeba_root = '/content/celeba/img_align_celeba/'\n",
        "print(os.listdir(celeba_root)[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu5qdbRl-lW-"
      },
      "source": [
        "##Add Arguments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fkx-8Hqp-nRk",
        "outputId": "d73096ac-df91-47f7-aba9-061ee734097c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(n_epochs=200, batch_size=64, lr=0.0001, b1=0.5, b2=0.999, n_cpu=8, latent_dim=100, img_size=64, channels=3, sample_interval=100)\n"
          ]
        }
      ],
      "source": [
        "# Thiết lập các tham số cho mô hình GAN\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--n_epochs\", type=int, default=200, help=\"number of epochs of training\")\n",
        "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"size of the batches\")\n",
        "parser.add_argument(\"--lr\", type=float, default=0.0001, help=\"adam: learning rate\")\n",
        "parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
        "parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
        "parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\n",
        "parser.add_argument(\"--latent_dim\", type=int, default=100, help=\"dimensionality of the latent space\")\n",
        "parser.add_argument(\"--img_size\", type=int, default=64, help=\"size of each image dimension\")\n",
        "parser.add_argument(\"--channels\", type=int, default=3, help=\"number of image channels\")\n",
        "parser.add_argument(\"--sample_interval\", type=int, default=100, help=\"interval between image sampling\")\n",
        "opt = parser.parse_args(args=[])\n",
        "print(opt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SY6g-SjxFQvG"
      },
      "source": [
        "##Wandb Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "OVI3lXnLFUBn",
        "outputId": "1a073ae4-d47e-4ae7-9947-a18ada63776f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>fake_loss</td><td>████▇▅▅▅▅▅▃▃▅▅▄▄▅▄▄▃▃▄▁▂▂▅▃▅▅▅▅▂▂▂▄▅▆▇▇▅</td></tr><tr><td>gradient_penalty</td><td>███▂▂▁▂▁▁▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/D</td><td>█▅▄▁▂▅▄▄▃▅▃▂▃▃▃▃▂▂▃▃▂▃▅▅▄▄▇▆▅▃▅▄▆▄▄▄▄▄▅▆</td></tr><tr><td>loss/G</td><td>▁▁▁▁▄▄▅▅▄▅▆▇▇▆▅▆▆▇▅▆█▇▆▅▄▄▄▅▅▅█▄▄▅▅▆▄▃▅▅</td></tr><tr><td>real_loss</td><td>▇▇██▇▆▆▆▅▅▄▄▄▄▄▂▃▃▃▄▃▂▂▂▂▂▂▂▂▃▁▁▂▄▄▂▂▄▅▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>fake_loss</td><td>-37.52464</td></tr><tr><td>gradient_penalty</td><td>0.02586</td></tr><tr><td>loss/D</td><td>7.25076</td></tr><tr><td>loss/G</td><td>41.57787</td></tr><tr><td>real_loss</td><td>-44.51676</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Emoji Creation 2</strong> at: <a href='https://wandb.ai/khuengv2007-institut-polytechnique-de-paris/GAN%202/runs/cji31543' target=\"_blank\">https://wandb.ai/khuengv2007-institut-polytechnique-de-paris/GAN%202/runs/cji31543</a><br> View project at: <a href='https://wandb.ai/khuengv2007-institut-polytechnique-de-paris/GAN%202' target=\"_blank\">https://wandb.ai/khuengv2007-institut-polytechnique-de-paris/GAN%202</a><br>Synced 5 W&B file(s), 8 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250816_050718-cji31543/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250816_051016-i6n4s34v</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/khuengv2007-institut-polytechnique-de-paris/GAN%202/runs/i6n4s34v' target=\"_blank\">Emoji Creation 2</a></strong> to <a href='https://wandb.ai/khuengv2007-institut-polytechnique-de-paris/GAN%202' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/khuengv2007-institut-polytechnique-de-paris/GAN%202' target=\"_blank\">https://wandb.ai/khuengv2007-institut-polytechnique-de-paris/GAN%202</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/khuengv2007-institut-polytechnique-de-paris/GAN%202/runs/i6n4s34v' target=\"_blank\">https://wandb.ai/khuengv2007-institut-polytechnique-de-paris/GAN%202/runs/i6n4s34v</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/khuengv2007-institut-polytechnique-de-paris/GAN%202/runs/i6n4s34v?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7acb37b4e410>"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ],
      "source": [
        "wandb.finish()\n",
        "wandb.init(\n",
        "    project = \"GAN 2\",\n",
        "    name = \"Emoji Creation 2\",\n",
        "    config = opt,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwP2S8iS-qpx"
      },
      "source": [
        "##Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F01ruxiI-sNV"
      },
      "outputs": [],
      "source": [
        "# tạo thư mục images để lưu các ảnh kết quả\n",
        "\n",
        "os.makedirs(\"images\", exist_ok=True)\n",
        "\n",
        "# Thiết lập kích thước ảnh và check xem có sử dụng GPU hay không\n",
        "\n",
        "img_shape = (opt.channels, opt.img_size, opt.img_size)\n",
        "\n",
        "cuda = True if torch.cuda.is_available() else False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFPw5ELy-xGh"
      },
      "source": [
        "#Model Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZuttP2COB2h"
      },
      "source": [
        "##Weight Init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4z2fKlUODZU"
      },
      "outputs": [],
      "source": [
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find(\"Linear\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "        if m.bias is not None:\n",
        "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "    elif classname.find(\"BatchNorm2d\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias.data, 0.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlwVCEFY-2pt"
      },
      "source": [
        "##Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9LKs7L5-0VA"
      },
      "outputs": [],
      "source": [
        "# Định nghĩa class Generator (Máy biến nhiễu thành ảnh)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.init_size = opt.img_size // 4\n",
        "        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n",
        "\n",
        "        self.conv_blocks = nn.Sequential(\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128, 0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64, 0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        out = self.l1(z)\n",
        "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
        "        img = self.conv_blocks(out)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de1c4-M1DbvW"
      },
      "source": [
        "##Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXcKm3HODeEg"
      },
      "outputs": [],
      "source": [
        "# Định nghĩa class Discriminator (Thám tử phân biệt)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):  # Sửa __init__\n",
        "        super(Discriminator, self).__init__()\n",
        "        def discriminator_block(in_filters, out_filters, bn=False):  # Bỏ bn\n",
        "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1),\n",
        "                     nn.LeakyReLU(0.2, inplace=True),\n",
        "                     nn.Dropout2d(0.25)]\n",
        "            # Không add BatchNorm\n",
        "            return block\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *discriminator_block(opt.channels, 64, bn=False),\n",
        "            *discriminator_block(64, 128),\n",
        "            *discriminator_block(128, 256),\n",
        "            *discriminator_block(256, 512),\n",
        "        )\n",
        "\n",
        "        ds_size = opt.img_size // 2 ** 4\n",
        "        # Không dùng sigmoid\n",
        "        self.adv_layer = nn.Linear(512 * ds_size ** 2, 1)\n",
        "\n",
        "    def forward(self, img):\n",
        "        out = self.model(img)\n",
        "        out = out.view(out.shape[0], -1)\n",
        "        validity = self.adv_layer(out)  # score thực số\n",
        "        return validity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBG-xf4Y_FW9"
      },
      "source": [
        "#Traning Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a659dAh_Heb"
      },
      "source": [
        "##Loss and Init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRGqouzn_IgK",
        "outputId": "43852585-be32-4bf2-bf56-b34184e33e3f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (model): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (2): Dropout2d(p=0.25, inplace=False)\n",
              "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (5): Dropout2d(p=0.25, inplace=False)\n",
              "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (8): Dropout2d(p=0.25, inplace=False)\n",
              "    (9): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (11): Dropout2d(p=0.25, inplace=False)\n",
              "  )\n",
              "  (adv_layer): Linear(in_features=8192, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 202
        }
      ],
      "source": [
        "# # Loss function\n",
        "# adversarial_loss = torch.nn.BCELoss()\n",
        "\n",
        "# Khởi tạo generator and discriminator và thêm wandb để track\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "wandb.watch(generator, log=\"all\", log_freq=200)\n",
        "wandb.watch(discriminator, log=\"all\", log_freq=200)\n",
        "\n",
        "# Chuyển mô hình sang sử dụng GPU nếu có\n",
        "if cuda:\n",
        "    generator.cuda()\n",
        "    discriminator.cuda()\n",
        "    adversarial_loss.cuda()\n",
        "\n",
        "# Initialize weights\n",
        "generator.apply(weights_init_normal)\n",
        "discriminator.apply(weights_init_normal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PLZG54Z_LvP"
      },
      "source": [
        "##Prep Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLqcruYc_NDg"
      },
      "outputs": [],
      "source": [
        "# Chuẩn bị và nạp dữ liệu MNIST\n",
        "\n",
        "transform=transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize(opt.img_size),\n",
        "                transforms.CenterCrop(opt.img_size),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])\n",
        "              ]\n",
        "        )\n",
        "\n",
        "dataset = ImageFolder(root=celeba_root, transform=transform)\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=opt.batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=cuda,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lp-0vGj0_OnI"
      },
      "source": [
        "##Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liIb4pEn_S91"
      },
      "outputs": [],
      "source": [
        "# Định nghĩa và cấu hình Optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.00002, betas=(opt.b1, opt.b2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh55a3_u_VB2"
      },
      "source": [
        "##Tensor Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCiR2JR4_XCP"
      },
      "outputs": [],
      "source": [
        "# Định dạng Tensor sử dụng GPU nếu có và device type\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2jTMz9X_Z1H"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY06C9Y0_a81"
      },
      "source": [
        "#Traning Process"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
        "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
        "    alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=real_samples.device)\n",
        "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
        "    d_interpolates = D(interpolates)\n",
        "    fake = torch.ones(d_interpolates.size(), device=real_samples.device)\n",
        "\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=d_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=fake,\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True\n",
        "    )[0]\n",
        "\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "    return gradient_penalty"
      ],
      "metadata": {
        "id": "qzd6gxtCIEQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgLpSYSB_jdD",
        "outputId": "26b3f480-9f58-437b-cc29-89641edc1f09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "[Epoch 0/200] [Batch 1285/3166] [D loss: 2.917835] [G loss: 6.113167] [gradient_penalty: 0.018703]\n",
            "[Epoch 0/200] [Batch 1286/3166] [D loss: 2.389601] [G loss: 6.625112] [gradient_penalty: 0.014385]\n",
            "[Epoch 0/200] [Batch 1287/3166] [D loss: 2.952439] [G loss: 6.809624] [gradient_penalty: 0.014692]\n",
            "[Epoch 0/200] [Batch 1288/3166] [D loss: 2.280562] [G loss: 5.920480] [gradient_penalty: 0.012368]\n",
            "[Epoch 0/200] [Batch 1289/3166] [D loss: 3.172843] [G loss: 6.432850] [gradient_penalty: 0.020655]\n",
            "[Epoch 0/200] [Batch 1290/3166] [D loss: 2.790025] [G loss: 6.383141] [gradient_penalty: 0.015091]\n",
            "[Epoch 0/200] [Batch 1291/3166] [D loss: 1.449858] [G loss: 6.948136] [gradient_penalty: 0.013151]\n",
            "[Epoch 0/200] [Batch 1292/3166] [D loss: 2.723833] [G loss: 7.439883] [gradient_penalty: 0.016502]\n",
            "[Epoch 0/200] [Batch 1293/3166] [D loss: 2.124119] [G loss: 7.312096] [gradient_penalty: 0.014089]\n",
            "[Epoch 0/200] [Batch 1294/3166] [D loss: 2.243116] [G loss: 7.546869] [gradient_penalty: 0.017248]\n",
            "[Epoch 0/200] [Batch 1295/3166] [D loss: 2.342886] [G loss: 7.619391] [gradient_penalty: 0.019593]\n",
            "[Epoch 0/200] [Batch 1296/3166] [D loss: 2.117035] [G loss: 7.408864] [gradient_penalty: 0.016709]\n",
            "[Epoch 0/200] [Batch 1297/3166] [D loss: 2.244167] [G loss: 7.933568] [gradient_penalty: 0.013371]\n",
            "[Epoch 0/200] [Batch 1298/3166] [D loss: 2.374789] [G loss: 8.005567] [gradient_penalty: 0.015447]\n",
            "[Epoch 0/200] [Batch 1299/3166] [D loss: 1.825360] [G loss: 7.209963] [gradient_penalty: 0.013408]\n",
            "[Epoch 0/200] [Batch 1300/3166] [D loss: 1.721598] [G loss: 7.642677] [gradient_penalty: 0.014758]\n",
            "log\n",
            "[Epoch 0/200] [Batch 1301/3166] [D loss: 2.536309] [G loss: 7.817525] [gradient_penalty: 0.014377]\n",
            "[Epoch 0/200] [Batch 1302/3166] [D loss: 3.018798] [G loss: 7.508625] [gradient_penalty: 0.016884]\n",
            "[Epoch 0/200] [Batch 1303/3166] [D loss: 3.267401] [G loss: 7.627826] [gradient_penalty: 0.015176]\n",
            "[Epoch 0/200] [Batch 1304/3166] [D loss: 2.214609] [G loss: 7.194793] [gradient_penalty: 0.014868]\n",
            "[Epoch 0/200] [Batch 1305/3166] [D loss: 3.212010] [G loss: 7.509006] [gradient_penalty: 0.014100]\n",
            "[Epoch 0/200] [Batch 1306/3166] [D loss: 3.627309] [G loss: 7.202558] [gradient_penalty: 0.022258]\n",
            "[Epoch 0/200] [Batch 1307/3166] [D loss: 2.261248] [G loss: 7.675715] [gradient_penalty: 0.014881]\n",
            "[Epoch 0/200] [Batch 1308/3166] [D loss: 2.861001] [G loss: 7.469689] [gradient_penalty: 0.016691]\n",
            "[Epoch 0/200] [Batch 1309/3166] [D loss: 2.515215] [G loss: 6.899547] [gradient_penalty: 0.012401]\n",
            "[Epoch 0/200] [Batch 1310/3166] [D loss: 2.997999] [G loss: 7.142900] [gradient_penalty: 0.012402]\n",
            "[Epoch 0/200] [Batch 1311/3166] [D loss: 3.318604] [G loss: 6.811908] [gradient_penalty: 0.016286]\n",
            "[Epoch 0/200] [Batch 1312/3166] [D loss: 3.150694] [G loss: 6.968929] [gradient_penalty: 0.011431]\n",
            "[Epoch 0/200] [Batch 1313/3166] [D loss: 3.037732] [G loss: 7.099584] [gradient_penalty: 0.016176]\n",
            "[Epoch 0/200] [Batch 1314/3166] [D loss: 3.627108] [G loss: 7.142359] [gradient_penalty: 0.017336]\n",
            "[Epoch 0/200] [Batch 1315/3166] [D loss: 2.728064] [G loss: 7.366596] [gradient_penalty: 0.014963]\n",
            "[Epoch 0/200] [Batch 1316/3166] [D loss: 3.045325] [G loss: 7.169103] [gradient_penalty: 0.017225]\n",
            "[Epoch 0/200] [Batch 1317/3166] [D loss: 3.489982] [G loss: 7.013382] [gradient_penalty: 0.016564]\n",
            "[Epoch 0/200] [Batch 1318/3166] [D loss: 3.534995] [G loss: 7.294421] [gradient_penalty: 0.019290]\n",
            "[Epoch 0/200] [Batch 1319/3166] [D loss: 2.805217] [G loss: 7.561410] [gradient_penalty: 0.014377]\n",
            "[Epoch 0/200] [Batch 1320/3166] [D loss: 2.651777] [G loss: 6.869318] [gradient_penalty: 0.016650]\n",
            "[Epoch 0/200] [Batch 1321/3166] [D loss: 2.982209] [G loss: 7.494130] [gradient_penalty: 0.020042]\n",
            "[Epoch 0/200] [Batch 1322/3166] [D loss: 2.456025] [G loss: 7.325307] [gradient_penalty: 0.021348]\n",
            "[Epoch 0/200] [Batch 1323/3166] [D loss: 1.855357] [G loss: 7.407370] [gradient_penalty: 0.015914]\n",
            "[Epoch 0/200] [Batch 1324/3166] [D loss: 3.169286] [G loss: 7.491981] [gradient_penalty: 0.017916]\n",
            "[Epoch 0/200] [Batch 1325/3166] [D loss: 2.309520] [G loss: 7.437991] [gradient_penalty: 0.020892]\n",
            "[Epoch 0/200] [Batch 1326/3166] [D loss: 2.857273] [G loss: 7.004990] [gradient_penalty: 0.021840]\n",
            "[Epoch 0/200] [Batch 1327/3166] [D loss: 3.969915] [G loss: 7.039757] [gradient_penalty: 0.020671]\n",
            "[Epoch 0/200] [Batch 1328/3166] [D loss: 3.158678] [G loss: 7.262481] [gradient_penalty: 0.022192]\n",
            "[Epoch 0/200] [Batch 1329/3166] [D loss: 5.822338] [G loss: 6.754113] [gradient_penalty: 0.083514]\n",
            "[Epoch 0/200] [Batch 1330/3166] [D loss: 3.414614] [G loss: 7.178596] [gradient_penalty: 0.021285]\n",
            "[Epoch 0/200] [Batch 1331/3166] [D loss: 3.525736] [G loss: 6.761293] [gradient_penalty: 0.023554]\n",
            "[Epoch 0/200] [Batch 1332/3166] [D loss: 2.933299] [G loss: 7.012945] [gradient_penalty: 0.018203]\n",
            "[Epoch 0/200] [Batch 1333/3166] [D loss: 4.372786] [G loss: 7.205131] [gradient_penalty: 0.023726]\n",
            "[Epoch 0/200] [Batch 1334/3166] [D loss: 3.417476] [G loss: 7.269047] [gradient_penalty: 0.026196]\n",
            "[Epoch 0/200] [Batch 1335/3166] [D loss: 2.810167] [G loss: 7.299548] [gradient_penalty: 0.012645]\n",
            "[Epoch 0/200] [Batch 1336/3166] [D loss: 3.577439] [G loss: 7.539065] [gradient_penalty: 0.020652]\n",
            "[Epoch 0/200] [Batch 1337/3166] [D loss: 3.339419] [G loss: 7.547512] [gradient_penalty: 0.026139]\n",
            "[Epoch 0/200] [Batch 1338/3166] [D loss: 2.038621] [G loss: 7.801871] [gradient_penalty: 0.015353]\n",
            "[Epoch 0/200] [Batch 1339/3166] [D loss: 1.661405] [G loss: 7.621430] [gradient_penalty: 0.024016]\n",
            "[Epoch 0/200] [Batch 1340/3166] [D loss: 3.015309] [G loss: 8.358007] [gradient_penalty: 0.021816]\n",
            "[Epoch 0/200] [Batch 1341/3166] [D loss: 2.981669] [G loss: 8.171618] [gradient_penalty: 0.019005]\n",
            "[Epoch 0/200] [Batch 1342/3166] [D loss: 1.874484] [G loss: 9.030222] [gradient_penalty: 0.016964]\n",
            "[Epoch 0/200] [Batch 1343/3166] [D loss: 2.981091] [G loss: 8.716305] [gradient_penalty: 0.032371]\n",
            "[Epoch 0/200] [Batch 1344/3166] [D loss: 1.579596] [G loss: 9.359936] [gradient_penalty: 0.021568]\n",
            "[Epoch 0/200] [Batch 1345/3166] [D loss: 2.786496] [G loss: 8.999217] [gradient_penalty: 0.026372]\n",
            "[Epoch 0/200] [Batch 1346/3166] [D loss: 0.832744] [G loss: 9.287057] [gradient_penalty: 0.018020]\n",
            "[Epoch 0/200] [Batch 1347/3166] [D loss: 2.513803] [G loss: 8.750998] [gradient_penalty: 0.022650]\n",
            "[Epoch 0/200] [Batch 1348/3166] [D loss: 1.884326] [G loss: 8.999269] [gradient_penalty: 0.021006]\n",
            "[Epoch 0/200] [Batch 1349/3166] [D loss: 2.079106] [G loss: 8.173018] [gradient_penalty: 0.019327]\n",
            "[Epoch 0/200] [Batch 1350/3166] [D loss: 2.958436] [G loss: 8.790527] [gradient_penalty: 0.015143]\n",
            "[Epoch 0/200] [Batch 1351/3166] [D loss: 3.030746] [G loss: 8.304749] [gradient_penalty: 0.025159]\n",
            "[Epoch 0/200] [Batch 1352/3166] [D loss: 3.573340] [G loss: 8.080057] [gradient_penalty: 0.018342]\n",
            "[Epoch 0/200] [Batch 1353/3166] [D loss: 3.661952] [G loss: 7.286119] [gradient_penalty: 0.013806]\n",
            "[Epoch 0/200] [Batch 1354/3166] [D loss: 3.147826] [G loss: 7.995373] [gradient_penalty: 0.015268]\n",
            "[Epoch 0/200] [Batch 1355/3166] [D loss: 4.306475] [G loss: 7.455034] [gradient_penalty: 0.016060]\n",
            "[Epoch 0/200] [Batch 1356/3166] [D loss: 3.528732] [G loss: 7.426192] [gradient_penalty: 0.019243]\n",
            "[Epoch 0/200] [Batch 1357/3166] [D loss: 4.087616] [G loss: 7.739594] [gradient_penalty: 0.014823]\n",
            "[Epoch 0/200] [Batch 1358/3166] [D loss: 3.384564] [G loss: 7.843047] [gradient_penalty: 0.018649]\n",
            "[Epoch 0/200] [Batch 1359/3166] [D loss: 3.846461] [G loss: 8.249691] [gradient_penalty: 0.025276]\n",
            "[Epoch 0/200] [Batch 1360/3166] [D loss: 3.278843] [G loss: 7.764619] [gradient_penalty: 0.015673]\n",
            "[Epoch 0/200] [Batch 1361/3166] [D loss: 3.686767] [G loss: 8.298656] [gradient_penalty: 0.019855]\n",
            "[Epoch 0/200] [Batch 1362/3166] [D loss: 3.455931] [G loss: 8.191805] [gradient_penalty: 0.016249]\n",
            "[Epoch 0/200] [Batch 1363/3166] [D loss: 3.167259] [G loss: 8.830842] [gradient_penalty: 0.023250]\n",
            "[Epoch 0/200] [Batch 1364/3166] [D loss: 2.096251] [G loss: 8.819786] [gradient_penalty: 0.015548]\n",
            "[Epoch 0/200] [Batch 1365/3166] [D loss: 2.793303] [G loss: 9.036272] [gradient_penalty: 0.018155]\n",
            "[Epoch 0/200] [Batch 1366/3166] [D loss: 2.169361] [G loss: 8.939042] [gradient_penalty: 0.020085]\n",
            "[Epoch 0/200] [Batch 1367/3166] [D loss: 2.351371] [G loss: 9.786076] [gradient_penalty: 0.020187]\n",
            "[Epoch 0/200] [Batch 1368/3166] [D loss: 2.132717] [G loss: 9.127620] [gradient_penalty: 0.019780]\n",
            "[Epoch 0/200] [Batch 1369/3166] [D loss: 1.730379] [G loss: 9.769769] [gradient_penalty: 0.017360]\n",
            "[Epoch 0/200] [Batch 1370/3166] [D loss: 2.363100] [G loss: 9.181222] [gradient_penalty: 0.017321]\n",
            "[Epoch 0/200] [Batch 1371/3166] [D loss: 2.153908] [G loss: 9.693992] [gradient_penalty: 0.015146]\n",
            "[Epoch 0/200] [Batch 1372/3166] [D loss: 1.540876] [G loss: 9.713753] [gradient_penalty: 0.020201]\n",
            "[Epoch 0/200] [Batch 1373/3166] [D loss: 2.773701] [G loss: 9.816374] [gradient_penalty: 0.022920]\n",
            "[Epoch 0/200] [Batch 1374/3166] [D loss: 1.983855] [G loss: 9.861006] [gradient_penalty: 0.013084]\n",
            "[Epoch 0/200] [Batch 1375/3166] [D loss: 2.842455] [G loss: 9.440841] [gradient_penalty: 0.022142]\n",
            "[Epoch 0/200] [Batch 1376/3166] [D loss: 2.942544] [G loss: 8.987535] [gradient_penalty: 0.022551]\n",
            "[Epoch 0/200] [Batch 1377/3166] [D loss: 2.721650] [G loss: 8.863363] [gradient_penalty: 0.021235]\n",
            "[Epoch 0/200] [Batch 1378/3166] [D loss: 3.039156] [G loss: 9.051618] [gradient_penalty: 0.023152]\n",
            "[Epoch 0/200] [Batch 1379/3166] [D loss: 3.616610] [G loss: 8.686785] [gradient_penalty: 0.021183]\n",
            "[Epoch 0/200] [Batch 1380/3166] [D loss: 3.133050] [G loss: 7.917194] [gradient_penalty: 0.023249]\n",
            "[Epoch 0/200] [Batch 1381/3166] [D loss: 3.530590] [G loss: 8.028200] [gradient_penalty: 0.024301]\n",
            "[Epoch 0/200] [Batch 1382/3166] [D loss: 3.125778] [G loss: 7.467209] [gradient_penalty: 0.022069]\n",
            "[Epoch 0/200] [Batch 1383/3166] [D loss: 3.287855] [G loss: 7.896007] [gradient_penalty: 0.023131]\n",
            "[Epoch 0/200] [Batch 1384/3166] [D loss: 3.058059] [G loss: 7.511862] [gradient_penalty: 0.026448]\n",
            "[Epoch 0/200] [Batch 1385/3166] [D loss: 3.844925] [G loss: 7.512966] [gradient_penalty: 0.022991]\n",
            "[Epoch 0/200] [Batch 1386/3166] [D loss: 2.946678] [G loss: 8.074957] [gradient_penalty: 0.020885]\n",
            "[Epoch 0/200] [Batch 1387/3166] [D loss: 2.968930] [G loss: 7.445135] [gradient_penalty: 0.024501]\n",
            "[Epoch 0/200] [Batch 1388/3166] [D loss: 3.469456] [G loss: 9.041668] [gradient_penalty: 0.023831]\n",
            "[Epoch 0/200] [Batch 1389/3166] [D loss: 3.198218] [G loss: 8.236757] [gradient_penalty: 0.017835]\n",
            "[Epoch 0/200] [Batch 1390/3166] [D loss: 2.941737] [G loss: 8.565267] [gradient_penalty: 0.018897]\n",
            "[Epoch 0/200] [Batch 1391/3166] [D loss: 3.200010] [G loss: 8.924915] [gradient_penalty: 0.024085]\n",
            "[Epoch 0/200] [Batch 1392/3166] [D loss: 1.735624] [G loss: 8.756979] [gradient_penalty: 0.018700]\n",
            "[Epoch 0/200] [Batch 1393/3166] [D loss: 1.217167] [G loss: 9.180566] [gradient_penalty: 0.015978]\n",
            "[Epoch 0/200] [Batch 1394/3166] [D loss: 1.274445] [G loss: 10.646559] [gradient_penalty: 0.020925]\n",
            "[Epoch 0/200] [Batch 1395/3166] [D loss: 2.497823] [G loss: 9.630838] [gradient_penalty: 0.017538]\n",
            "[Epoch 0/200] [Batch 1396/3166] [D loss: 2.069150] [G loss: 10.566525] [gradient_penalty: 0.014501]\n",
            "[Epoch 0/200] [Batch 1397/3166] [D loss: 3.053225] [G loss: 9.770514] [gradient_penalty: 0.012560]\n",
            "[Epoch 0/200] [Batch 1398/3166] [D loss: 1.102236] [G loss: 9.873703] [gradient_penalty: 0.016866]\n",
            "[Epoch 0/200] [Batch 1399/3166] [D loss: 3.646966] [G loss: 9.728616] [gradient_penalty: 0.019186]\n",
            "[Epoch 0/200] [Batch 1400/3166] [D loss: 2.747919] [G loss: 10.172048] [gradient_penalty: 0.020647]\n",
            "log\n",
            "[Epoch 0/200] [Batch 1401/3166] [D loss: 3.521021] [G loss: 9.674426] [gradient_penalty: 0.019260]\n",
            "[Epoch 0/200] [Batch 1402/3166] [D loss: 2.836901] [G loss: 9.163885] [gradient_penalty: 0.014654]\n",
            "[Epoch 0/200] [Batch 1403/3166] [D loss: 2.122048] [G loss: 9.569859] [gradient_penalty: 0.013032]\n",
            "[Epoch 0/200] [Batch 1404/3166] [D loss: 3.386419] [G loss: 9.214359] [gradient_penalty: 0.013348]\n",
            "[Epoch 0/200] [Batch 1405/3166] [D loss: 2.980015] [G loss: 8.948277] [gradient_penalty: 0.017533]\n",
            "[Epoch 0/200] [Batch 1406/3166] [D loss: 3.576920] [G loss: 8.569665] [gradient_penalty: 0.018262]\n",
            "[Epoch 0/200] [Batch 1407/3166] [D loss: 3.123239] [G loss: 9.419245] [gradient_penalty: 0.016824]\n",
            "[Epoch 0/200] [Batch 1408/3166] [D loss: 3.020869] [G loss: 8.779309] [gradient_penalty: 0.015851]\n",
            "[Epoch 0/200] [Batch 1409/3166] [D loss: 3.577883] [G loss: 9.653420] [gradient_penalty: 0.018350]\n",
            "[Epoch 0/200] [Batch 1410/3166] [D loss: 3.396730] [G loss: 9.075367] [gradient_penalty: 0.014693]\n",
            "[Epoch 0/200] [Batch 1411/3166] [D loss: 2.900038] [G loss: 9.367624] [gradient_penalty: 0.014032]\n",
            "[Epoch 0/200] [Batch 1412/3166] [D loss: 2.863874] [G loss: 8.859288] [gradient_penalty: 0.020108]\n",
            "[Epoch 0/200] [Batch 1413/3166] [D loss: 4.118556] [G loss: 8.817064] [gradient_penalty: 0.019487]\n",
            "[Epoch 0/200] [Batch 1414/3166] [D loss: 3.070788] [G loss: 9.101315] [gradient_penalty: 0.014434]\n",
            "[Epoch 0/200] [Batch 1415/3166] [D loss: 2.377224] [G loss: 9.430412] [gradient_penalty: 0.016657]\n",
            "[Epoch 0/200] [Batch 1416/3166] [D loss: 2.730478] [G loss: 9.670397] [gradient_penalty: 0.014278]\n",
            "[Epoch 0/200] [Batch 1417/3166] [D loss: 3.074790] [G loss: 9.651175] [gradient_penalty: 0.018891]\n",
            "[Epoch 0/200] [Batch 1418/3166] [D loss: 3.313787] [G loss: 9.178433] [gradient_penalty: 0.015900]\n",
            "[Epoch 0/200] [Batch 1419/3166] [D loss: 2.869380] [G loss: 9.530902] [gradient_penalty: 0.021561]\n",
            "[Epoch 0/200] [Batch 1420/3166] [D loss: 1.673269] [G loss: 10.322321] [gradient_penalty: 0.015119]\n",
            "[Epoch 0/200] [Batch 1421/3166] [D loss: 2.273335] [G loss: 9.568496] [gradient_penalty: 0.015661]\n",
            "[Epoch 0/200] [Batch 1422/3166] [D loss: 2.020998] [G loss: 10.280790] [gradient_penalty: 0.011707]\n",
            "[Epoch 0/200] [Batch 1423/3166] [D loss: 1.664234] [G loss: 9.694021] [gradient_penalty: 0.014482]\n",
            "[Epoch 0/200] [Batch 1424/3166] [D loss: 2.470013] [G loss: 9.688227] [gradient_penalty: 0.015322]\n",
            "[Epoch 0/200] [Batch 1425/3166] [D loss: 1.742683] [G loss: 9.667630] [gradient_penalty: 0.016499]\n",
            "[Epoch 0/200] [Batch 1426/3166] [D loss: 2.604813] [G loss: 9.750729] [gradient_penalty: 0.021242]\n",
            "[Epoch 0/200] [Batch 1427/3166] [D loss: 1.882946] [G loss: 10.436913] [gradient_penalty: 0.019110]\n",
            "[Epoch 0/200] [Batch 1428/3166] [D loss: 3.746707] [G loss: 9.684202] [gradient_penalty: 0.027407]\n",
            "[Epoch 0/200] [Batch 1429/3166] [D loss: 2.407032] [G loss: 9.361091] [gradient_penalty: 0.018348]\n",
            "[Epoch 0/200] [Batch 1430/3166] [D loss: 2.954508] [G loss: 9.551832] [gradient_penalty: 0.018068]\n",
            "[Epoch 0/200] [Batch 1431/3166] [D loss: 2.544808] [G loss: 9.655811] [gradient_penalty: 0.021812]\n",
            "[Epoch 0/200] [Batch 1432/3166] [D loss: 2.695382] [G loss: 9.150915] [gradient_penalty: 0.023118]\n",
            "[Epoch 0/200] [Batch 1433/3166] [D loss: 3.579120] [G loss: 9.700520] [gradient_penalty: 0.024327]\n",
            "[Epoch 0/200] [Batch 1434/3166] [D loss: 3.279831] [G loss: 9.159352] [gradient_penalty: 0.022219]\n",
            "[Epoch 0/200] [Batch 1435/3166] [D loss: 3.785640] [G loss: 8.977324] [gradient_penalty: 0.021239]\n",
            "[Epoch 0/200] [Batch 1436/3166] [D loss: 4.193511] [G loss: 9.497944] [gradient_penalty: 0.022934]\n",
            "[Epoch 0/200] [Batch 1437/3166] [D loss: 4.896597] [G loss: 8.630621] [gradient_penalty: 0.028125]\n",
            "[Epoch 0/200] [Batch 1438/3166] [D loss: 3.379958] [G loss: 8.859902] [gradient_penalty: 0.019214]\n",
            "[Epoch 0/200] [Batch 1439/3166] [D loss: 3.644808] [G loss: 8.238043] [gradient_penalty: 0.024982]\n",
            "[Epoch 0/200] [Batch 1440/3166] [D loss: 3.803113] [G loss: 9.024796] [gradient_penalty: 0.020799]\n",
            "[Epoch 0/200] [Batch 1441/3166] [D loss: 3.446606] [G loss: 8.158577] [gradient_penalty: 0.020345]\n",
            "[Epoch 0/200] [Batch 1442/3166] [D loss: 3.559281] [G loss: 8.567934] [gradient_penalty: 0.017270]\n",
            "[Epoch 0/200] [Batch 1443/3166] [D loss: 2.693013] [G loss: 8.271231] [gradient_penalty: 0.016829]\n",
            "[Epoch 0/200] [Batch 1444/3166] [D loss: 3.888653] [G loss: 8.621546] [gradient_penalty: 0.018016]\n",
            "[Epoch 0/200] [Batch 1445/3166] [D loss: 3.696186] [G loss: 8.819509] [gradient_penalty: 0.021727]\n",
            "[Epoch 0/200] [Batch 1446/3166] [D loss: 3.719628] [G loss: 8.357314] [gradient_penalty: 0.023866]\n",
            "[Epoch 0/200] [Batch 1447/3166] [D loss: 2.100995] [G loss: 9.095429] [gradient_penalty: 0.020326]\n",
            "[Epoch 0/200] [Batch 1448/3166] [D loss: 3.425962] [G loss: 8.971775] [gradient_penalty: 0.019674]\n",
            "[Epoch 0/200] [Batch 1449/3166] [D loss: 2.560324] [G loss: 9.322885] [gradient_penalty: 0.018643]\n",
            "[Epoch 0/200] [Batch 1450/3166] [D loss: 3.276969] [G loss: 9.681274] [gradient_penalty: 0.020288]\n",
            "[Epoch 0/200] [Batch 1451/3166] [D loss: 2.523094] [G loss: 8.735666] [gradient_penalty: 0.018784]\n",
            "[Epoch 0/200] [Batch 1452/3166] [D loss: 2.348230] [G loss: 9.498035] [gradient_penalty: 0.013551]\n",
            "[Epoch 0/200] [Batch 1453/3166] [D loss: 2.989156] [G loss: 8.875263] [gradient_penalty: 0.017385]\n",
            "[Epoch 0/200] [Batch 1454/3166] [D loss: 2.270051] [G loss: 9.228225] [gradient_penalty: 0.014246]\n",
            "[Epoch 0/200] [Batch 1455/3166] [D loss: 2.219565] [G loss: 10.192757] [gradient_penalty: 0.013173]\n",
            "[Epoch 0/200] [Batch 1456/3166] [D loss: 3.189484] [G loss: 9.546734] [gradient_penalty: 0.013228]\n",
            "[Epoch 0/200] [Batch 1457/3166] [D loss: 3.025049] [G loss: 9.852703] [gradient_penalty: 0.020661]\n",
            "[Epoch 0/200] [Batch 1458/3166] [D loss: 2.327121] [G loss: 9.514664] [gradient_penalty: 0.015240]\n",
            "[Epoch 0/200] [Batch 1459/3166] [D loss: 1.488450] [G loss: 9.191116] [gradient_penalty: 0.017171]\n",
            "[Epoch 0/200] [Batch 1460/3166] [D loss: 2.506307] [G loss: 9.652002] [gradient_penalty: 0.011672]\n",
            "[Epoch 0/200] [Batch 1461/3166] [D loss: 2.991916] [G loss: 9.145229] [gradient_penalty: 0.012947]\n",
            "[Epoch 0/200] [Batch 1462/3166] [D loss: 2.927568] [G loss: 9.110838] [gradient_penalty: 0.015004]\n",
            "[Epoch 0/200] [Batch 1463/3166] [D loss: 3.297149] [G loss: 8.555778] [gradient_penalty: 0.014522]\n",
            "[Epoch 0/200] [Batch 1464/3166] [D loss: 2.816660] [G loss: 9.930455] [gradient_penalty: 0.012646]\n",
            "[Epoch 0/200] [Batch 1465/3166] [D loss: 3.753110] [G loss: 8.772236] [gradient_penalty: 0.016227]\n",
            "[Epoch 0/200] [Batch 1466/3166] [D loss: 2.553745] [G loss: 9.028359] [gradient_penalty: 0.015123]\n",
            "[Epoch 0/200] [Batch 1467/3166] [D loss: 3.544483] [G loss: 9.262596] [gradient_penalty: 0.020230]\n",
            "[Epoch 0/200] [Batch 1468/3166] [D loss: 3.739037] [G loss: 9.391115] [gradient_penalty: 0.019362]\n",
            "[Epoch 0/200] [Batch 1469/3166] [D loss: 3.542594] [G loss: 9.050629] [gradient_penalty: 0.015228]\n",
            "[Epoch 0/200] [Batch 1470/3166] [D loss: 2.792308] [G loss: 8.895474] [gradient_penalty: 0.012808]\n",
            "[Epoch 0/200] [Batch 1471/3166] [D loss: 3.011769] [G loss: 9.072038] [gradient_penalty: 0.017894]\n",
            "[Epoch 0/200] [Batch 1472/3166] [D loss: 2.567876] [G loss: 9.793977] [gradient_penalty: 0.012496]\n",
            "[Epoch 0/200] [Batch 1473/3166] [D loss: 3.892365] [G loss: 9.530907] [gradient_penalty: 0.020098]\n",
            "[Epoch 0/200] [Batch 1474/3166] [D loss: 2.268513] [G loss: 8.962408] [gradient_penalty: 0.019153]\n",
            "[Epoch 0/200] [Batch 1475/3166] [D loss: 3.258779] [G loss: 9.126945] [gradient_penalty: 0.019694]\n",
            "[Epoch 0/200] [Batch 1476/3166] [D loss: 2.579223] [G loss: 9.886355] [gradient_penalty: 0.013918]\n",
            "[Epoch 0/200] [Batch 1477/3166] [D loss: 3.100678] [G loss: 9.022158] [gradient_penalty: 0.014050]\n",
            "[Epoch 0/200] [Batch 1478/3166] [D loss: 2.023496] [G loss: 9.106522] [gradient_penalty: 0.015467]\n",
            "[Epoch 0/200] [Batch 1479/3166] [D loss: 2.495915] [G loss: 9.210821] [gradient_penalty: 0.009101]\n",
            "[Epoch 0/200] [Batch 1480/3166] [D loss: 1.608179] [G loss: 9.571218] [gradient_penalty: 0.014581]\n",
            "[Epoch 0/200] [Batch 1481/3166] [D loss: 2.460420] [G loss: 9.599601] [gradient_penalty: 0.013644]\n",
            "[Epoch 0/200] [Batch 1482/3166] [D loss: 2.152581] [G loss: 9.351921] [gradient_penalty: 0.016731]\n",
            "[Epoch 0/200] [Batch 1483/3166] [D loss: 2.896274] [G loss: 8.925753] [gradient_penalty: 0.020274]\n",
            "[Epoch 0/200] [Batch 1484/3166] [D loss: 2.822941] [G loss: 8.835062] [gradient_penalty: 0.016371]\n",
            "[Epoch 0/200] [Batch 1485/3166] [D loss: 3.254219] [G loss: 8.998545] [gradient_penalty: 0.017574]\n",
            "[Epoch 0/200] [Batch 1486/3166] [D loss: 2.724445] [G loss: 8.897688] [gradient_penalty: 0.016645]\n",
            "[Epoch 0/200] [Batch 1487/3166] [D loss: 3.096523] [G loss: 8.288061] [gradient_penalty: 0.016772]\n",
            "[Epoch 0/200] [Batch 1488/3166] [D loss: 3.291422] [G loss: 8.132404] [gradient_penalty: 0.018687]\n",
            "[Epoch 0/200] [Batch 1489/3166] [D loss: 2.303807] [G loss: 8.007610] [gradient_penalty: 0.017039]\n",
            "[Epoch 0/200] [Batch 1490/3166] [D loss: 3.231946] [G loss: 8.144378] [gradient_penalty: 0.021880]\n",
            "[Epoch 0/200] [Batch 1491/3166] [D loss: 3.520249] [G loss: 7.993974] [gradient_penalty: 0.020951]\n",
            "[Epoch 0/200] [Batch 1492/3166] [D loss: 3.380405] [G loss: 7.855906] [gradient_penalty: 0.020292]\n",
            "[Epoch 0/200] [Batch 1493/3166] [D loss: 3.736062] [G loss: 7.532246] [gradient_penalty: 0.018776]\n",
            "[Epoch 0/200] [Batch 1494/3166] [D loss: 4.068623] [G loss: 7.421354] [gradient_penalty: 0.023456]\n",
            "[Epoch 0/200] [Batch 1495/3166] [D loss: 3.137701] [G loss: 7.390903] [gradient_penalty: 0.022430]\n",
            "[Epoch 0/200] [Batch 1496/3166] [D loss: 3.275025] [G loss: 7.613848] [gradient_penalty: 0.017685]\n",
            "[Epoch 0/200] [Batch 1497/3166] [D loss: 3.308955] [G loss: 8.070117] [gradient_penalty: 0.021581]\n",
            "[Epoch 0/200] [Batch 1498/3166] [D loss: 3.586458] [G loss: 8.052110] [gradient_penalty: 0.019673]\n",
            "[Epoch 0/200] [Batch 1499/3166] [D loss: 3.606620] [G loss: 7.634835] [gradient_penalty: 0.017706]\n",
            "[Epoch 0/200] [Batch 1500/3166] [D loss: 3.450826] [G loss: 7.669015] [gradient_penalty: 0.018867]\n",
            "log\n",
            "[Epoch 0/200] [Batch 1501/3166] [D loss: 2.776389] [G loss: 7.547174] [gradient_penalty: 0.018448]\n",
            "[Epoch 0/200] [Batch 1502/3166] [D loss: 2.836559] [G loss: 7.954350] [gradient_penalty: 0.017118]\n",
            "[Epoch 0/200] [Batch 1503/3166] [D loss: 2.807321] [G loss: 8.712487] [gradient_penalty: 0.015979]\n",
            "[Epoch 0/200] [Batch 1504/3166] [D loss: 1.906308] [G loss: 8.383851] [gradient_penalty: 0.018092]\n",
            "[Epoch 0/200] [Batch 1505/3166] [D loss: 2.700958] [G loss: 8.858859] [gradient_penalty: 0.019243]\n",
            "[Epoch 0/200] [Batch 1506/3166] [D loss: 2.866723] [G loss: 8.342064] [gradient_penalty: 0.017966]\n",
            "[Epoch 0/200] [Batch 1507/3166] [D loss: 1.914057] [G loss: 8.799358] [gradient_penalty: 0.015607]\n",
            "[Epoch 0/200] [Batch 1508/3166] [D loss: 2.517352] [G loss: 8.529001] [gradient_penalty: 0.017925]\n",
            "[Epoch 0/200] [Batch 1509/3166] [D loss: 1.923767] [G loss: 8.558825] [gradient_penalty: 0.014477]\n",
            "[Epoch 0/200] [Batch 1510/3166] [D loss: 2.769740] [G loss: 8.985690] [gradient_penalty: 0.016099]\n",
            "[Epoch 0/200] [Batch 1511/3166] [D loss: 3.569682] [G loss: 8.325704] [gradient_penalty: 0.018973]\n",
            "[Epoch 0/200] [Batch 1512/3166] [D loss: 2.786356] [G loss: 8.659531] [gradient_penalty: 0.018944]\n",
            "[Epoch 0/200] [Batch 1513/3166] [D loss: 1.280036] [G loss: 8.664300] [gradient_penalty: 0.015580]\n",
            "[Epoch 0/200] [Batch 1514/3166] [D loss: 2.254191] [G loss: 7.894125] [gradient_penalty: 0.015316]\n",
            "[Epoch 0/200] [Batch 1515/3166] [D loss: 2.077606] [G loss: 8.079739] [gradient_penalty: 0.014455]\n",
            "[Epoch 0/200] [Batch 1516/3166] [D loss: 2.079334] [G loss: 7.368711] [gradient_penalty: 0.013103]\n",
            "[Epoch 0/200] [Batch 1517/3166] [D loss: 2.286803] [G loss: 8.119835] [gradient_penalty: 0.014493]\n",
            "[Epoch 0/200] [Batch 1518/3166] [D loss: 2.407145] [G loss: 8.169196] [gradient_penalty: 0.013439]\n",
            "[Epoch 0/200] [Batch 1519/3166] [D loss: 2.518469] [G loss: 7.746128] [gradient_penalty: 0.012221]\n",
            "[Epoch 0/200] [Batch 1520/3166] [D loss: 3.160347] [G loss: 7.816851] [gradient_penalty: 0.016957]\n",
            "[Epoch 0/200] [Batch 1521/3166] [D loss: 3.169288] [G loss: 7.935211] [gradient_penalty: 0.018561]\n",
            "[Epoch 0/200] [Batch 1522/3166] [D loss: 3.347937] [G loss: 7.701346] [gradient_penalty: 0.017884]\n",
            "[Epoch 0/200] [Batch 1523/3166] [D loss: 2.438329] [G loss: 7.869116] [gradient_penalty: 0.013914]\n",
            "[Epoch 0/200] [Batch 1524/3166] [D loss: 3.273001] [G loss: 7.955584] [gradient_penalty: 0.019535]\n",
            "[Epoch 0/200] [Batch 1525/3166] [D loss: 2.303317] [G loss: 7.945743] [gradient_penalty: 0.011548]\n",
            "[Epoch 0/200] [Batch 1526/3166] [D loss: 2.245363] [G loss: 7.785768] [gradient_penalty: 0.016990]\n",
            "[Epoch 0/200] [Batch 1527/3166] [D loss: 2.333901] [G loss: 7.715558] [gradient_penalty: 0.011119]\n",
            "[Epoch 0/200] [Batch 1528/3166] [D loss: 2.804936] [G loss: 7.355330] [gradient_penalty: 0.014894]\n",
            "[Epoch 0/200] [Batch 1529/3166] [D loss: 2.077512] [G loss: 8.337145] [gradient_penalty: 0.012832]\n",
            "[Epoch 0/200] [Batch 1530/3166] [D loss: 2.188358] [G loss: 8.025905] [gradient_penalty: 0.018686]\n",
            "[Epoch 0/200] [Batch 1531/3166] [D loss: 2.283008] [G loss: 8.330489] [gradient_penalty: 0.019375]\n",
            "[Epoch 0/200] [Batch 1532/3166] [D loss: 2.832787] [G loss: 7.992803] [gradient_penalty: 0.019750]\n",
            "[Epoch 0/200] [Batch 1533/3166] [D loss: 1.967758] [G loss: 8.176497] [gradient_penalty: 0.014762]\n",
            "[Epoch 0/200] [Batch 1534/3166] [D loss: 1.930008] [G loss: 8.082866] [gradient_penalty: 0.017789]\n",
            "[Epoch 0/200] [Batch 1535/3166] [D loss: 2.060404] [G loss: 8.292620] [gradient_penalty: 0.024521]\n",
            "[Epoch 0/200] [Batch 1536/3166] [D loss: 1.811584] [G loss: 8.400913] [gradient_penalty: 0.015908]\n",
            "[Epoch 0/200] [Batch 1537/3166] [D loss: 2.752755] [G loss: 7.853841] [gradient_penalty: 0.021192]\n",
            "[Epoch 0/200] [Batch 1538/3166] [D loss: 2.546965] [G loss: 8.274727] [gradient_penalty: 0.021089]\n",
            "[Epoch 0/200] [Batch 1539/3166] [D loss: 3.008160] [G loss: 8.426615] [gradient_penalty: 0.024209]\n",
            "[Epoch 0/200] [Batch 1540/3166] [D loss: 2.212845] [G loss: 8.045763] [gradient_penalty: 0.020662]\n",
            "[Epoch 0/200] [Batch 1541/3166] [D loss: 2.655117] [G loss: 7.814809] [gradient_penalty: 0.024105]\n",
            "[Epoch 0/200] [Batch 1542/3166] [D loss: 3.215803] [G loss: 7.700117] [gradient_penalty: 0.023131]\n",
            "[Epoch 0/200] [Batch 1543/3166] [D loss: 2.209085] [G loss: 7.384283] [gradient_penalty: 0.016947]\n",
            "[Epoch 0/200] [Batch 1544/3166] [D loss: 3.417233] [G loss: 7.746832] [gradient_penalty: 0.024930]\n",
            "[Epoch 0/200] [Batch 1545/3166] [D loss: 3.545566] [G loss: 7.191826] [gradient_penalty: 0.019831]\n",
            "[Epoch 0/200] [Batch 1546/3166] [D loss: 2.833641] [G loss: 7.747569] [gradient_penalty: 0.022154]\n",
            "[Epoch 0/200] [Batch 1547/3166] [D loss: 3.276352] [G loss: 7.448174] [gradient_penalty: 0.018584]\n",
            "[Epoch 0/200] [Batch 1548/3166] [D loss: 2.345551] [G loss: 7.468788] [gradient_penalty: 0.016010]\n",
            "[Epoch 0/200] [Batch 1549/3166] [D loss: 3.462314] [G loss: 7.302707] [gradient_penalty: 0.015326]\n",
            "[Epoch 0/200] [Batch 1550/3166] [D loss: 3.044294] [G loss: 6.820496] [gradient_penalty: 0.022548]\n",
            "[Epoch 0/200] [Batch 1551/3166] [D loss: 3.817082] [G loss: 6.862067] [gradient_penalty: 0.023845]\n",
            "[Epoch 0/200] [Batch 1552/3166] [D loss: 2.306998] [G loss: 7.259648] [gradient_penalty: 0.015010]\n",
            "[Epoch 0/200] [Batch 1553/3166] [D loss: 2.634379] [G loss: 7.099750] [gradient_penalty: 0.013845]\n",
            "[Epoch 0/200] [Batch 1554/3166] [D loss: 3.188215] [G loss: 7.442677] [gradient_penalty: 0.015695]\n",
            "[Epoch 0/200] [Batch 1555/3166] [D loss: 2.709107] [G loss: 7.376070] [gradient_penalty: 0.016959]\n",
            "[Epoch 0/200] [Batch 1556/3166] [D loss: 1.532810] [G loss: 7.346574] [gradient_penalty: 0.016480]\n",
            "[Epoch 0/200] [Batch 1557/3166] [D loss: 2.225995] [G loss: 7.423382] [gradient_penalty: 0.019078]\n",
            "[Epoch 0/200] [Batch 1558/3166] [D loss: 1.999136] [G loss: 6.889485] [gradient_penalty: 0.015248]\n",
            "[Epoch 0/200] [Batch 1559/3166] [D loss: 1.737717] [G loss: 7.727666] [gradient_penalty: 0.016114]\n",
            "[Epoch 0/200] [Batch 1560/3166] [D loss: 2.373634] [G loss: 7.824759] [gradient_penalty: 0.016393]\n",
            "[Epoch 0/200] [Batch 1561/3166] [D loss: 1.811125] [G loss: 7.497720] [gradient_penalty: 0.015490]\n",
            "[Epoch 0/200] [Batch 1562/3166] [D loss: 1.691527] [G loss: 7.535344] [gradient_penalty: 0.012816]\n",
            "[Epoch 0/200] [Batch 1563/3166] [D loss: 2.915508] [G loss: 8.006842] [gradient_penalty: 0.012108]\n",
            "[Epoch 0/200] [Batch 1564/3166] [D loss: 1.589140] [G loss: 7.981419] [gradient_penalty: 0.014110]\n",
            "[Epoch 0/200] [Batch 1565/3166] [D loss: 2.474123] [G loss: 7.498932] [gradient_penalty: 0.015770]\n",
            "[Epoch 0/200] [Batch 1566/3166] [D loss: 1.665848] [G loss: 7.398295] [gradient_penalty: 0.009510]\n",
            "[Epoch 0/200] [Batch 1567/3166] [D loss: 2.005369] [G loss: 7.727225] [gradient_penalty: 0.015459]\n",
            "[Epoch 0/200] [Batch 1568/3166] [D loss: 1.074109] [G loss: 7.420594] [gradient_penalty: 0.012580]\n",
            "[Epoch 0/200] [Batch 1569/3166] [D loss: 2.351950] [G loss: 7.850796] [gradient_penalty: 0.016689]\n",
            "[Epoch 0/200] [Batch 1570/3166] [D loss: 1.373771] [G loss: 7.656830] [gradient_penalty: 0.014278]\n",
            "[Epoch 0/200] [Batch 1571/3166] [D loss: 3.200492] [G loss: 7.739746] [gradient_penalty: 0.017083]\n",
            "[Epoch 0/200] [Batch 1572/3166] [D loss: 2.593413] [G loss: 7.297480] [gradient_penalty: 0.012621]\n",
            "[Epoch 0/200] [Batch 1573/3166] [D loss: 2.745665] [G loss: 6.901874] [gradient_penalty: 0.015593]\n",
            "[Epoch 0/200] [Batch 1574/3166] [D loss: 3.644957] [G loss: 7.441101] [gradient_penalty: 0.018102]\n",
            "[Epoch 0/200] [Batch 1575/3166] [D loss: 2.480630] [G loss: 6.604531] [gradient_penalty: 0.010108]\n",
            "[Epoch 0/200] [Batch 1576/3166] [D loss: 2.064173] [G loss: 7.276972] [gradient_penalty: 0.014505]\n",
            "[Epoch 0/200] [Batch 1577/3166] [D loss: 3.000494] [G loss: 7.271693] [gradient_penalty: 0.011033]\n",
            "[Epoch 0/200] [Batch 1578/3166] [D loss: 2.281147] [G loss: 7.469771] [gradient_penalty: 0.011651]\n",
            "[Epoch 0/200] [Batch 1579/3166] [D loss: 3.029918] [G loss: 6.795185] [gradient_penalty: 0.014390]\n",
            "[Epoch 0/200] [Batch 1580/3166] [D loss: 3.227977] [G loss: 7.728892] [gradient_penalty: 0.020433]\n",
            "[Epoch 0/200] [Batch 1581/3166] [D loss: 3.069664] [G loss: 7.710323] [gradient_penalty: 0.016750]\n",
            "[Epoch 0/200] [Batch 1582/3166] [D loss: 2.818377] [G loss: 7.240401] [gradient_penalty: 0.025208]\n",
            "[Epoch 0/200] [Batch 1583/3166] [D loss: 2.865563] [G loss: 7.277040] [gradient_penalty: 0.013410]\n",
            "[Epoch 0/200] [Batch 1584/3166] [D loss: 2.079803] [G loss: 7.331794] [gradient_penalty: 0.011263]\n",
            "[Epoch 0/200] [Batch 1585/3166] [D loss: 2.086368] [G loss: 7.567133] [gradient_penalty: 0.012519]\n",
            "[Epoch 0/200] [Batch 1586/3166] [D loss: 2.674988] [G loss: 7.683707] [gradient_penalty: 0.015626]\n",
            "[Epoch 0/200] [Batch 1587/3166] [D loss: 2.166983] [G loss: 8.070909] [gradient_penalty: 0.010483]\n",
            "[Epoch 0/200] [Batch 1588/3166] [D loss: 2.462501] [G loss: 7.639564] [gradient_penalty: 0.019114]\n",
            "[Epoch 0/200] [Batch 1589/3166] [D loss: 3.225501] [G loss: 7.802690] [gradient_penalty: 0.018386]\n",
            "[Epoch 0/200] [Batch 1590/3166] [D loss: 2.040799] [G loss: 7.564870] [gradient_penalty: 0.013099]\n",
            "[Epoch 0/200] [Batch 1591/3166] [D loss: 1.819523] [G loss: 7.508054] [gradient_penalty: 0.015618]\n",
            "[Epoch 0/200] [Batch 1592/3166] [D loss: 2.264611] [G loss: 7.451451] [gradient_penalty: 0.017795]\n",
            "[Epoch 0/200] [Batch 1593/3166] [D loss: 2.450602] [G loss: 7.834352] [gradient_penalty: 0.021484]\n",
            "[Epoch 0/200] [Batch 1594/3166] [D loss: 2.070813] [G loss: 7.564704] [gradient_penalty: 0.014776]\n",
            "[Epoch 0/200] [Batch 1595/3166] [D loss: 1.932966] [G loss: 7.146930] [gradient_penalty: 0.012324]\n",
            "[Epoch 0/200] [Batch 1596/3166] [D loss: 3.039602] [G loss: 7.543178] [gradient_penalty: 0.020127]\n",
            "[Epoch 0/200] [Batch 1597/3166] [D loss: 2.895288] [G loss: 7.660306] [gradient_penalty: 0.023328]\n",
            "[Epoch 0/200] [Batch 1598/3166] [D loss: 2.948313] [G loss: 7.069021] [gradient_penalty: 0.017155]\n",
            "[Epoch 0/200] [Batch 1599/3166] [D loss: 3.258236] [G loss: 6.863262] [gradient_penalty: 0.019305]\n",
            "[Epoch 0/200] [Batch 1600/3166] [D loss: 1.912714] [G loss: 7.212481] [gradient_penalty: 0.015699]\n",
            "log\n",
            "[Epoch 0/200] [Batch 1601/3166] [D loss: 2.926767] [G loss: 6.514669] [gradient_penalty: 0.015820]\n",
            "[Epoch 0/200] [Batch 1602/3166] [D loss: 3.333416] [G loss: 7.072663] [gradient_penalty: 0.018314]\n",
            "[Epoch 0/200] [Batch 1603/3166] [D loss: 3.356347] [G loss: 6.738316] [gradient_penalty: 0.019330]\n",
            "[Epoch 0/200] [Batch 1604/3166] [D loss: 3.361780] [G loss: 6.760896] [gradient_penalty: 0.015788]\n",
            "[Epoch 0/200] [Batch 1605/3166] [D loss: 3.035038] [G loss: 6.774569] [gradient_penalty: 0.015971]\n",
            "[Epoch 0/200] [Batch 1606/3166] [D loss: 2.979556] [G loss: 6.910683] [gradient_penalty: 0.017472]\n",
            "[Epoch 0/200] [Batch 1607/3166] [D loss: 3.165035] [G loss: 6.650516] [gradient_penalty: 0.016743]\n",
            "[Epoch 0/200] [Batch 1608/3166] [D loss: 2.839243] [G loss: 6.961553] [gradient_penalty: 0.017004]\n",
            "[Epoch 0/200] [Batch 1609/3166] [D loss: 2.719964] [G loss: 6.645806] [gradient_penalty: 0.014047]\n",
            "[Epoch 0/200] [Batch 1610/3166] [D loss: 2.790653] [G loss: 6.214977] [gradient_penalty: 0.014040]\n",
            "[Epoch 0/200] [Batch 1611/3166] [D loss: 2.513149] [G loss: 7.209282] [gradient_penalty: 0.020584]\n",
            "[Epoch 0/200] [Batch 1612/3166] [D loss: 2.866252] [G loss: 6.778278] [gradient_penalty: 0.019682]\n",
            "[Epoch 0/200] [Batch 1613/3166] [D loss: 2.557324] [G loss: 7.462754] [gradient_penalty: 0.018149]\n",
            "[Epoch 0/200] [Batch 1614/3166] [D loss: 1.960637] [G loss: 6.724757] [gradient_penalty: 0.015095]\n",
            "[Epoch 0/200] [Batch 1615/3166] [D loss: 2.996596] [G loss: 7.024940] [gradient_penalty: 0.019543]\n",
            "[Epoch 0/200] [Batch 1616/3166] [D loss: 2.636724] [G loss: 6.467949] [gradient_penalty: 0.017744]\n",
            "[Epoch 0/200] [Batch 1617/3166] [D loss: 2.788600] [G loss: 6.834347] [gradient_penalty: 0.017024]\n",
            "[Epoch 0/200] [Batch 1618/3166] [D loss: 1.864400] [G loss: 6.827528] [gradient_penalty: 0.011376]\n",
            "[Epoch 0/200] [Batch 1619/3166] [D loss: 2.458752] [G loss: 6.878354] [gradient_penalty: 0.011287]\n",
            "[Epoch 0/200] [Batch 1620/3166] [D loss: 2.715849] [G loss: 6.848017] [gradient_penalty: 0.015934]\n",
            "[Epoch 0/200] [Batch 1621/3166] [D loss: 3.341080] [G loss: 6.461266] [gradient_penalty: 0.014130]\n",
            "[Epoch 0/200] [Batch 1622/3166] [D loss: 2.470650] [G loss: 6.641406] [gradient_penalty: 0.010878]\n",
            "[Epoch 0/200] [Batch 1623/3166] [D loss: 3.240395] [G loss: 6.811284] [gradient_penalty: 0.010989]\n",
            "[Epoch 0/200] [Batch 1624/3166] [D loss: 2.751335] [G loss: 6.497000] [gradient_penalty: 0.014014]\n",
            "[Epoch 0/200] [Batch 1625/3166] [D loss: 3.919662] [G loss: 6.322554] [gradient_penalty: 0.015893]\n",
            "[Epoch 0/200] [Batch 1626/3166] [D loss: 3.306777] [G loss: 6.152225] [gradient_penalty: 0.016959]\n",
            "[Epoch 0/200] [Batch 1627/3166] [D loss: 3.187310] [G loss: 6.501480] [gradient_penalty: 0.015464]\n",
            "[Epoch 0/200] [Batch 1628/3166] [D loss: 2.908238] [G loss: 6.519258] [gradient_penalty: 0.016610]\n",
            "[Epoch 0/200] [Batch 1629/3166] [D loss: 2.487787] [G loss: 6.488338] [gradient_penalty: 0.014919]\n",
            "[Epoch 0/200] [Batch 1630/3166] [D loss: 3.074389] [G loss: 6.155680] [gradient_penalty: 0.016546]\n",
            "[Epoch 0/200] [Batch 1631/3166] [D loss: 2.997502] [G loss: 6.387529] [gradient_penalty: 0.013011]\n",
            "[Epoch 0/200] [Batch 1632/3166] [D loss: 2.312020] [G loss: 6.792264] [gradient_penalty: 0.014318]\n",
            "[Epoch 0/200] [Batch 1633/3166] [D loss: 3.139823] [G loss: 6.400434] [gradient_penalty: 0.016493]\n",
            "[Epoch 0/200] [Batch 1634/3166] [D loss: 2.678459] [G loss: 6.103521] [gradient_penalty: 0.012179]\n",
            "[Epoch 0/200] [Batch 1635/3166] [D loss: 2.369788] [G loss: 6.119167] [gradient_penalty: 0.018310]\n",
            "[Epoch 0/200] [Batch 1636/3166] [D loss: 2.569237] [G loss: 6.060468] [gradient_penalty: 0.014212]\n",
            "[Epoch 0/200] [Batch 1637/3166] [D loss: 2.737895] [G loss: 6.610056] [gradient_penalty: 0.012641]\n",
            "[Epoch 0/200] [Batch 1638/3166] [D loss: 2.129527] [G loss: 6.464406] [gradient_penalty: 0.020151]\n",
            "[Epoch 0/200] [Batch 1639/3166] [D loss: 2.108387] [G loss: 6.795671] [gradient_penalty: 0.016230]\n",
            "[Epoch 0/200] [Batch 1640/3166] [D loss: 3.000512] [G loss: 6.665610] [gradient_penalty: 0.014103]\n",
            "[Epoch 0/200] [Batch 1641/3166] [D loss: 2.239212] [G loss: 6.712115] [gradient_penalty: 0.014050]\n",
            "[Epoch 0/200] [Batch 1642/3166] [D loss: 2.128443] [G loss: 6.833504] [gradient_penalty: 0.017488]\n",
            "[Epoch 0/200] [Batch 1643/3166] [D loss: 2.806287] [G loss: 6.533589] [gradient_penalty: 0.017314]\n",
            "[Epoch 0/200] [Batch 1644/3166] [D loss: 2.034530] [G loss: 6.724112] [gradient_penalty: 0.014196]\n",
            "[Epoch 0/200] [Batch 1645/3166] [D loss: 2.086881] [G loss: 6.947252] [gradient_penalty: 0.015862]\n",
            "[Epoch 0/200] [Batch 1646/3166] [D loss: 1.976904] [G loss: 6.631119] [gradient_penalty: 0.017546]\n",
            "[Epoch 0/200] [Batch 1647/3166] [D loss: 2.580405] [G loss: 7.019766] [gradient_penalty: 0.016196]\n",
            "[Epoch 0/200] [Batch 1648/3166] [D loss: 2.694512] [G loss: 6.502812] [gradient_penalty: 0.019647]\n",
            "[Epoch 0/200] [Batch 1649/3166] [D loss: 3.410711] [G loss: 6.506931] [gradient_penalty: 0.023887]\n",
            "[Epoch 0/200] [Batch 1650/3166] [D loss: 2.835216] [G loss: 6.922984] [gradient_penalty: 0.022733]\n",
            "[Epoch 0/200] [Batch 1651/3166] [D loss: 2.328926] [G loss: 6.718034] [gradient_penalty: 0.013923]\n",
            "[Epoch 0/200] [Batch 1652/3166] [D loss: 2.516516] [G loss: 6.756899] [gradient_penalty: 0.017727]\n",
            "[Epoch 0/200] [Batch 1653/3166] [D loss: 3.150210] [G loss: 6.555439] [gradient_penalty: 0.013367]\n",
            "[Epoch 0/200] [Batch 1654/3166] [D loss: 2.241871] [G loss: 6.532992] [gradient_penalty: 0.012734]\n",
            "[Epoch 0/200] [Batch 1655/3166] [D loss: 2.381422] [G loss: 6.400798] [gradient_penalty: 0.013442]\n",
            "[Epoch 0/200] [Batch 1656/3166] [D loss: 1.767452] [G loss: 6.537066] [gradient_penalty: 0.018071]\n",
            "[Epoch 0/200] [Batch 1657/3166] [D loss: 2.848185] [G loss: 6.716421] [gradient_penalty: 0.014197]\n",
            "[Epoch 0/200] [Batch 1658/3166] [D loss: 2.560229] [G loss: 6.882255] [gradient_penalty: 0.012061]\n",
            "[Epoch 0/200] [Batch 1659/3166] [D loss: 2.049194] [G loss: 6.391671] [gradient_penalty: 0.016056]\n",
            "[Epoch 0/200] [Batch 1660/3166] [D loss: 2.861883] [G loss: 6.946639] [gradient_penalty: 0.018093]\n",
            "[Epoch 0/200] [Batch 1661/3166] [D loss: 2.954317] [G loss: 6.475687] [gradient_penalty: 0.014213]\n",
            "[Epoch 0/200] [Batch 1662/3166] [D loss: 3.312575] [G loss: 6.838022] [gradient_penalty: 0.012189]\n",
            "[Epoch 0/200] [Batch 1663/3166] [D loss: 2.361681] [G loss: 6.748956] [gradient_penalty: 0.017261]\n",
            "[Epoch 0/200] [Batch 1664/3166] [D loss: 2.707313] [G loss: 6.851450] [gradient_penalty: 0.013148]\n",
            "[Epoch 0/200] [Batch 1665/3166] [D loss: 2.333017] [G loss: 7.313121] [gradient_penalty: 0.018483]\n",
            "[Epoch 0/200] [Batch 1666/3166] [D loss: 2.608221] [G loss: 6.340755] [gradient_penalty: 0.014312]\n",
            "[Epoch 0/200] [Batch 1667/3166] [D loss: 2.692189] [G loss: 6.821192] [gradient_penalty: 0.015305]\n",
            "[Epoch 0/200] [Batch 1668/3166] [D loss: 2.427942] [G loss: 6.529347] [gradient_penalty: 0.014461]\n",
            "[Epoch 0/200] [Batch 1669/3166] [D loss: 3.149749] [G loss: 6.781025] [gradient_penalty: 0.018445]\n",
            "[Epoch 0/200] [Batch 1670/3166] [D loss: 2.996981] [G loss: 6.981310] [gradient_penalty: 0.015674]\n",
            "[Epoch 0/200] [Batch 1671/3166] [D loss: 2.151949] [G loss: 6.660555] [gradient_penalty: 0.015194]\n",
            "[Epoch 0/200] [Batch 1672/3166] [D loss: 1.931389] [G loss: 6.937378] [gradient_penalty: 0.010817]\n",
            "[Epoch 0/200] [Batch 1673/3166] [D loss: 3.440929] [G loss: 6.608785] [gradient_penalty: 0.013228]\n",
            "[Epoch 0/200] [Batch 1674/3166] [D loss: 2.520703] [G loss: 6.753357] [gradient_penalty: 0.014688]\n",
            "[Epoch 0/200] [Batch 1675/3166] [D loss: 2.965248] [G loss: 7.009380] [gradient_penalty: 0.013896]\n",
            "[Epoch 0/200] [Batch 1676/3166] [D loss: 2.973407] [G loss: 6.854988] [gradient_penalty: 0.013834]\n",
            "[Epoch 0/200] [Batch 1677/3166] [D loss: 3.116804] [G loss: 6.836108] [gradient_penalty: 0.011337]\n",
            "[Epoch 0/200] [Batch 1678/3166] [D loss: 2.682073] [G loss: 7.060746] [gradient_penalty: 0.013869]\n",
            "[Epoch 0/200] [Batch 1679/3166] [D loss: 2.954409] [G loss: 6.515248] [gradient_penalty: 0.018288]\n",
            "[Epoch 0/200] [Batch 1680/3166] [D loss: 2.814417] [G loss: 6.769060] [gradient_penalty: 0.017448]\n",
            "[Epoch 0/200] [Batch 1681/3166] [D loss: 3.458603] [G loss: 6.509081] [gradient_penalty: 0.021718]\n",
            "[Epoch 0/200] [Batch 1682/3166] [D loss: 2.412977] [G loss: 6.666411] [gradient_penalty: 0.013649]\n",
            "[Epoch 0/200] [Batch 1683/3166] [D loss: 2.559179] [G loss: 6.892711] [gradient_penalty: 0.019414]\n",
            "[Epoch 0/200] [Batch 1684/3166] [D loss: 2.684265] [G loss: 6.861274] [gradient_penalty: 0.013036]\n",
            "[Epoch 0/200] [Batch 1685/3166] [D loss: 3.576555] [G loss: 6.693995] [gradient_penalty: 0.014360]\n",
            "[Epoch 0/200] [Batch 1686/3166] [D loss: 2.466246] [G loss: 6.540363] [gradient_penalty: 0.013514]\n",
            "[Epoch 0/200] [Batch 1687/3166] [D loss: 2.575192] [G loss: 6.772767] [gradient_penalty: 0.013515]\n",
            "[Epoch 0/200] [Batch 1688/3166] [D loss: 2.360623] [G loss: 6.659924] [gradient_penalty: 0.012870]\n",
            "[Epoch 0/200] [Batch 1689/3166] [D loss: 2.335913] [G loss: 6.214249] [gradient_penalty: 0.011078]\n",
            "[Epoch 0/200] [Batch 1690/3166] [D loss: 2.130750] [G loss: 6.813766] [gradient_penalty: 0.020616]\n",
            "[Epoch 0/200] [Batch 1691/3166] [D loss: 3.004144] [G loss: 6.380747] [gradient_penalty: 0.016265]\n",
            "[Epoch 0/200] [Batch 1692/3166] [D loss: 3.065272] [G loss: 6.281808] [gradient_penalty: 0.017598]\n",
            "[Epoch 0/200] [Batch 1693/3166] [D loss: 3.034775] [G loss: 6.779309] [gradient_penalty: 0.019742]\n",
            "[Epoch 0/200] [Batch 1694/3166] [D loss: 2.886073] [G loss: 6.989496] [gradient_penalty: 0.016195]\n",
            "[Epoch 0/200] [Batch 1695/3166] [D loss: 2.712473] [G loss: 6.700609] [gradient_penalty: 0.013456]\n",
            "[Epoch 0/200] [Batch 1696/3166] [D loss: 2.319628] [G loss: 6.787191] [gradient_penalty: 0.013072]\n",
            "[Epoch 0/200] [Batch 1697/3166] [D loss: 2.209641] [G loss: 6.499941] [gradient_penalty: 0.012397]\n",
            "[Epoch 0/200] [Batch 1698/3166] [D loss: 2.339959] [G loss: 6.411399] [gradient_penalty: 0.016666]\n",
            "[Epoch 0/200] [Batch 1699/3166] [D loss: 2.659828] [G loss: 7.244571] [gradient_penalty: 0.017685]\n",
            "[Epoch 0/200] [Batch 1700/3166] [D loss: 2.420227] [G loss: 6.782271] [gradient_penalty: 0.019936]\n",
            "log\n",
            "[Epoch 0/200] [Batch 1701/3166] [D loss: 3.368482] [G loss: 7.105978] [gradient_penalty: 0.016566]\n",
            "[Epoch 0/200] [Batch 1702/3166] [D loss: 2.107134] [G loss: 6.657278] [gradient_penalty: 0.012753]\n",
            "[Epoch 0/200] [Batch 1703/3166] [D loss: 3.319439] [G loss: 6.704109] [gradient_penalty: 0.014738]\n",
            "[Epoch 0/200] [Batch 1704/3166] [D loss: 2.658863] [G loss: 7.045176] [gradient_penalty: 0.015391]\n",
            "[Epoch 0/200] [Batch 1705/3166] [D loss: 2.375238] [G loss: 6.753712] [gradient_penalty: 0.018571]\n",
            "[Epoch 0/200] [Batch 1706/3166] [D loss: 2.184309] [G loss: 6.840124] [gradient_penalty: 0.016582]\n",
            "[Epoch 0/200] [Batch 1707/3166] [D loss: 2.681160] [G loss: 6.928246] [gradient_penalty: 0.013502]\n",
            "[Epoch 0/200] [Batch 1708/3166] [D loss: 2.331823] [G loss: 6.945028] [gradient_penalty: 0.012680]\n",
            "[Epoch 0/200] [Batch 1709/3166] [D loss: 2.081786] [G loss: 6.944678] [gradient_penalty: 0.015371]\n",
            "[Epoch 0/200] [Batch 1710/3166] [D loss: 2.443808] [G loss: 6.660454] [gradient_penalty: 0.015968]\n",
            "[Epoch 0/200] [Batch 1711/3166] [D loss: 2.176325] [G loss: 6.957219] [gradient_penalty: 0.016034]\n",
            "[Epoch 0/200] [Batch 1712/3166] [D loss: 3.203345] [G loss: 6.631762] [gradient_penalty: 0.016087]\n",
            "[Epoch 0/200] [Batch 1713/3166] [D loss: 2.412098] [G loss: 7.303060] [gradient_penalty: 0.014742]\n",
            "[Epoch 0/200] [Batch 1714/3166] [D loss: 1.845410] [G loss: 6.983281] [gradient_penalty: 0.011560]\n",
            "[Epoch 0/200] [Batch 1715/3166] [D loss: 2.637968] [G loss: 7.015594] [gradient_penalty: 0.011545]\n",
            "[Epoch 0/200] [Batch 1716/3166] [D loss: 2.299925] [G loss: 7.062411] [gradient_penalty: 0.013052]\n",
            "[Epoch 0/200] [Batch 1717/3166] [D loss: 3.464987] [G loss: 7.034706] [gradient_penalty: 0.028316]\n",
            "[Epoch 0/200] [Batch 1718/3166] [D loss: 2.769632] [G loss: 6.783445] [gradient_penalty: 0.012030]\n",
            "[Epoch 0/200] [Batch 1719/3166] [D loss: 2.668712] [G loss: 7.086611] [gradient_penalty: 0.017716]\n",
            "[Epoch 0/200] [Batch 1720/3166] [D loss: 2.824926] [G loss: 6.868534] [gradient_penalty: 0.018025]\n",
            "[Epoch 0/200] [Batch 1721/3166] [D loss: 2.580468] [G loss: 7.078645] [gradient_penalty: 0.013029]\n",
            "[Epoch 0/200] [Batch 1722/3166] [D loss: 2.271086] [G loss: 7.649659] [gradient_penalty: 0.011470]\n",
            "[Epoch 0/200] [Batch 1723/3166] [D loss: 2.616682] [G loss: 7.551653] [gradient_penalty: 0.017529]\n",
            "[Epoch 0/200] [Batch 1724/3166] [D loss: 2.428331] [G loss: 7.034881] [gradient_penalty: 0.016050]\n",
            "[Epoch 0/200] [Batch 1725/3166] [D loss: 3.093254] [G loss: 7.078909] [gradient_penalty: 0.012689]\n",
            "[Epoch 0/200] [Batch 1726/3166] [D loss: 1.822387] [G loss: 6.683391] [gradient_penalty: 0.014461]\n",
            "[Epoch 0/200] [Batch 1727/3166] [D loss: 2.470640] [G loss: 7.521068] [gradient_penalty: 0.008971]\n",
            "[Epoch 0/200] [Batch 1728/3166] [D loss: 2.563468] [G loss: 7.526974] [gradient_penalty: 0.013684]\n",
            "[Epoch 0/200] [Batch 1729/3166] [D loss: 3.072113] [G loss: 6.588723] [gradient_penalty: 0.018422]\n",
            "[Epoch 0/200] [Batch 1730/3166] [D loss: 2.841610] [G loss: 6.882696] [gradient_penalty: 0.014808]\n",
            "[Epoch 0/200] [Batch 1731/3166] [D loss: 2.902502] [G loss: 7.021090] [gradient_penalty: 0.013497]\n",
            "[Epoch 0/200] [Batch 1732/3166] [D loss: 2.498885] [G loss: 7.081538] [gradient_penalty: 0.016721]\n",
            "[Epoch 0/200] [Batch 1733/3166] [D loss: 2.801307] [G loss: 6.891723] [gradient_penalty: 0.014859]\n",
            "[Epoch 0/200] [Batch 1734/3166] [D loss: 2.646341] [G loss: 7.033877] [gradient_penalty: 0.010293]\n",
            "[Epoch 0/200] [Batch 1735/3166] [D loss: 2.578177] [G loss: 6.545640] [gradient_penalty: 0.011380]\n",
            "[Epoch 0/200] [Batch 1736/3166] [D loss: 3.014078] [G loss: 7.100952] [gradient_penalty: 0.015576]\n",
            "[Epoch 0/200] [Batch 1737/3166] [D loss: 2.536164] [G loss: 6.544915] [gradient_penalty: 0.012300]\n",
            "[Epoch 0/200] [Batch 1738/3166] [D loss: 2.978927] [G loss: 6.786105] [gradient_penalty: 0.018524]\n",
            "[Epoch 0/200] [Batch 1739/3166] [D loss: 2.258462] [G loss: 6.629432] [gradient_penalty: 0.019006]\n",
            "[Epoch 0/200] [Batch 1740/3166] [D loss: 2.649827] [G loss: 7.262017] [gradient_penalty: 0.013472]\n",
            "[Epoch 0/200] [Batch 1741/3166] [D loss: 2.297668] [G loss: 6.955664] [gradient_penalty: 0.010675]\n",
            "[Epoch 0/200] [Batch 1742/3166] [D loss: 2.828733] [G loss: 6.844450] [gradient_penalty: 0.013282]\n",
            "[Epoch 0/200] [Batch 1743/3166] [D loss: 1.994252] [G loss: 6.904114] [gradient_penalty: 0.018842]\n",
            "[Epoch 0/200] [Batch 1744/3166] [D loss: 2.290243] [G loss: 7.510432] [gradient_penalty: 0.013278]\n",
            "[Epoch 0/200] [Batch 1745/3166] [D loss: 2.666198] [G loss: 7.072166] [gradient_penalty: 0.018100]\n",
            "[Epoch 0/200] [Batch 1746/3166] [D loss: 2.093794] [G loss: 6.987492] [gradient_penalty: 0.011344]\n",
            "[Epoch 0/200] [Batch 1747/3166] [D loss: 2.547343] [G loss: 7.127259] [gradient_penalty: 0.013527]\n",
            "[Epoch 0/200] [Batch 1748/3166] [D loss: 3.542722] [G loss: 6.953400] [gradient_penalty: 0.016862]\n",
            "[Epoch 0/200] [Batch 1749/3166] [D loss: 1.753328] [G loss: 7.525121] [gradient_penalty: 0.012734]\n",
            "[Epoch 0/200] [Batch 1750/3166] [D loss: 2.136080] [G loss: 7.257605] [gradient_penalty: 0.015262]\n",
            "[Epoch 0/200] [Batch 1751/3166] [D loss: 3.057879] [G loss: 6.914852] [gradient_penalty: 0.019641]\n",
            "[Epoch 0/200] [Batch 1752/3166] [D loss: 2.085737] [G loss: 7.430346] [gradient_penalty: 0.014316]\n",
            "[Epoch 0/200] [Batch 1753/3166] [D loss: 1.499790] [G loss: 7.485827] [gradient_penalty: 0.013233]\n",
            "[Epoch 0/200] [Batch 1754/3166] [D loss: 2.584946] [G loss: 7.668757] [gradient_penalty: 0.016051]\n",
            "[Epoch 0/200] [Batch 1755/3166] [D loss: 2.669636] [G loss: 7.414344] [gradient_penalty: 0.016826]\n",
            "[Epoch 0/200] [Batch 1756/3166] [D loss: 2.792103] [G loss: 7.177313] [gradient_penalty: 0.014273]\n",
            "[Epoch 0/200] [Batch 1757/3166] [D loss: 2.747938] [G loss: 7.267406] [gradient_penalty: 0.014710]\n",
            "[Epoch 0/200] [Batch 1758/3166] [D loss: 2.349676] [G loss: 6.549734] [gradient_penalty: 0.014740]\n",
            "[Epoch 0/200] [Batch 1759/3166] [D loss: 1.942906] [G loss: 7.388803] [gradient_penalty: 0.014132]\n",
            "[Epoch 0/200] [Batch 1760/3166] [D loss: 2.371984] [G loss: 7.218563] [gradient_penalty: 0.012682]\n",
            "[Epoch 0/200] [Batch 1761/3166] [D loss: 3.449478] [G loss: 7.229849] [gradient_penalty: 0.016006]\n",
            "[Epoch 0/200] [Batch 1762/3166] [D loss: 2.739244] [G loss: 7.429406] [gradient_penalty: 0.011894]\n",
            "[Epoch 0/200] [Batch 1763/3166] [D loss: 2.609196] [G loss: 7.326024] [gradient_penalty: 0.013070]\n",
            "[Epoch 0/200] [Batch 1764/3166] [D loss: 2.545647] [G loss: 6.744016] [gradient_penalty: 0.012850]\n",
            "[Epoch 0/200] [Batch 1765/3166] [D loss: 2.433594] [G loss: 7.170941] [gradient_penalty: 0.011535]\n",
            "[Epoch 0/200] [Batch 1766/3166] [D loss: 1.822441] [G loss: 6.602185] [gradient_penalty: 0.012536]\n",
            "[Epoch 0/200] [Batch 1767/3166] [D loss: 2.644737] [G loss: 7.522074] [gradient_penalty: 0.011616]\n",
            "[Epoch 0/200] [Batch 1768/3166] [D loss: 3.305049] [G loss: 6.920727] [gradient_penalty: 0.012134]\n",
            "[Epoch 0/200] [Batch 1769/3166] [D loss: 3.036882] [G loss: 7.301223] [gradient_penalty: 0.011189]\n",
            "[Epoch 0/200] [Batch 1770/3166] [D loss: 3.177920] [G loss: 7.235750] [gradient_penalty: 0.011525]\n",
            "[Epoch 0/200] [Batch 1771/3166] [D loss: 2.886524] [G loss: 6.989030] [gradient_penalty: 0.013468]\n",
            "[Epoch 0/200] [Batch 1772/3166] [D loss: 2.456562] [G loss: 7.135869] [gradient_penalty: 0.012954]\n",
            "[Epoch 0/200] [Batch 1773/3166] [D loss: 3.161252] [G loss: 7.170836] [gradient_penalty: 0.015385]\n",
            "[Epoch 0/200] [Batch 1774/3166] [D loss: 2.989503] [G loss: 6.945321] [gradient_penalty: 0.015993]\n",
            "[Epoch 0/200] [Batch 1775/3166] [D loss: 2.651366] [G loss: 7.253052] [gradient_penalty: 0.012693]\n",
            "[Epoch 0/200] [Batch 1776/3166] [D loss: 2.319742] [G loss: 6.598832] [gradient_penalty: 0.008941]\n",
            "[Epoch 0/200] [Batch 1777/3166] [D loss: 2.026072] [G loss: 6.686093] [gradient_penalty: 0.014531]\n",
            "[Epoch 0/200] [Batch 1778/3166] [D loss: 2.640610] [G loss: 6.845619] [gradient_penalty: 0.013077]\n",
            "[Epoch 0/200] [Batch 1779/3166] [D loss: 2.158117] [G loss: 6.628922] [gradient_penalty: 0.013646]\n",
            "[Epoch 0/200] [Batch 1780/3166] [D loss: 2.866379] [G loss: 6.862626] [gradient_penalty: 0.014442]\n",
            "[Epoch 0/200] [Batch 1781/3166] [D loss: 1.687918] [G loss: 6.372037] [gradient_penalty: 0.012240]\n",
            "[Epoch 0/200] [Batch 1782/3166] [D loss: 3.115366] [G loss: 6.914644] [gradient_penalty: 0.015661]\n",
            "[Epoch 0/200] [Batch 1783/3166] [D loss: 2.014396] [G loss: 7.120449] [gradient_penalty: 0.012367]\n",
            "[Epoch 0/200] [Batch 1784/3166] [D loss: 2.682039] [G loss: 6.799664] [gradient_penalty: 0.016226]\n",
            "[Epoch 0/200] [Batch 1785/3166] [D loss: 2.600019] [G loss: 6.454895] [gradient_penalty: 0.012620]\n",
            "[Epoch 0/200] [Batch 1786/3166] [D loss: 2.532055] [G loss: 6.945406] [gradient_penalty: 0.016871]\n",
            "[Epoch 0/200] [Batch 1787/3166] [D loss: 2.417360] [G loss: 6.929814] [gradient_penalty: 0.013227]\n",
            "[Epoch 0/200] [Batch 1788/3166] [D loss: 2.832197] [G loss: 7.081020] [gradient_penalty: 0.015705]\n",
            "[Epoch 0/200] [Batch 1789/3166] [D loss: 2.576573] [G loss: 7.076612] [gradient_penalty: 0.009716]\n",
            "[Epoch 0/200] [Batch 1790/3166] [D loss: 2.262732] [G loss: 7.427441] [gradient_penalty: 0.013292]\n",
            "[Epoch 0/200] [Batch 1791/3166] [D loss: 2.769015] [G loss: 6.650609] [gradient_penalty: 0.015292]\n",
            "[Epoch 0/200] [Batch 1792/3166] [D loss: 2.815559] [G loss: 6.317677] [gradient_penalty: 0.014309]\n",
            "[Epoch 0/200] [Batch 1793/3166] [D loss: 3.218801] [G loss: 6.739143] [gradient_penalty: 0.016446]\n",
            "[Epoch 0/200] [Batch 1794/3166] [D loss: 2.262110] [G loss: 6.902848] [gradient_penalty: 0.015940]\n",
            "[Epoch 0/200] [Batch 1795/3166] [D loss: 1.703788] [G loss: 6.396270] [gradient_penalty: 0.011724]\n",
            "[Epoch 0/200] [Batch 1796/3166] [D loss: 2.868662] [G loss: 6.722448] [gradient_penalty: 0.009216]\n",
            "[Epoch 0/200] [Batch 1797/3166] [D loss: 3.138632] [G loss: 6.312708] [gradient_penalty: 0.015003]\n",
            "[Epoch 0/200] [Batch 1798/3166] [D loss: 2.425396] [G loss: 6.778112] [gradient_penalty: 0.012969]\n",
            "[Epoch 0/200] [Batch 1799/3166] [D loss: 2.866420] [G loss: 6.487462] [gradient_penalty: 0.012861]\n",
            "[Epoch 0/200] [Batch 1800/3166] [D loss: 2.833221] [G loss: 6.834657] [gradient_penalty: 0.015619]\n",
            "log\n",
            "[Epoch 0/200] [Batch 1801/3166] [D loss: 2.947459] [G loss: 6.640817] [gradient_penalty: 0.012924]\n",
            "[Epoch 0/200] [Batch 1802/3166] [D loss: 2.349285] [G loss: 6.567147] [gradient_penalty: 0.016108]\n",
            "[Epoch 0/200] [Batch 1803/3166] [D loss: 2.764353] [G loss: 6.268460] [gradient_penalty: 0.012168]\n",
            "[Epoch 0/200] [Batch 1804/3166] [D loss: 2.353885] [G loss: 6.794887] [gradient_penalty: 0.014374]\n",
            "[Epoch 0/200] [Batch 1805/3166] [D loss: 2.363281] [G loss: 7.178709] [gradient_penalty: 0.012542]\n",
            "[Epoch 0/200] [Batch 1806/3166] [D loss: 3.040095] [G loss: 6.368127] [gradient_penalty: 0.017217]\n",
            "[Epoch 0/200] [Batch 1807/3166] [D loss: 2.336101] [G loss: 7.080150] [gradient_penalty: 0.017112]\n",
            "[Epoch 0/200] [Batch 1808/3166] [D loss: 2.964127] [G loss: 6.775700] [gradient_penalty: 0.017955]\n",
            "[Epoch 0/200] [Batch 1809/3166] [D loss: 2.451029] [G loss: 6.148117] [gradient_penalty: 0.013041]\n",
            "[Epoch 0/200] [Batch 1810/3166] [D loss: 2.423892] [G loss: 6.604666] [gradient_penalty: 0.012877]\n",
            "[Epoch 0/200] [Batch 1811/3166] [D loss: 2.426541] [G loss: 6.567323] [gradient_penalty: 0.015218]\n",
            "[Epoch 0/200] [Batch 1812/3166] [D loss: 2.188314] [G loss: 6.757308] [gradient_penalty: 0.016172]\n",
            "[Epoch 0/200] [Batch 1813/3166] [D loss: 1.957333] [G loss: 6.167637] [gradient_penalty: 0.014625]\n",
            "[Epoch 0/200] [Batch 1814/3166] [D loss: 2.090286] [G loss: 6.888709] [gradient_penalty: 0.010422]\n",
            "[Epoch 0/200] [Batch 1815/3166] [D loss: 2.675783] [G loss: 6.781842] [gradient_penalty: 0.014063]\n",
            "[Epoch 0/200] [Batch 1816/3166] [D loss: 3.070218] [G loss: 7.155399] [gradient_penalty: 0.012745]\n",
            "[Epoch 0/200] [Batch 1817/3166] [D loss: 2.817774] [G loss: 6.707764] [gradient_penalty: 0.015942]\n",
            "[Epoch 0/200] [Batch 1818/3166] [D loss: 2.253416] [G loss: 7.236099] [gradient_penalty: 0.018563]\n",
            "[Epoch 0/200] [Batch 1819/3166] [D loss: 2.298169] [G loss: 7.066478] [gradient_penalty: 0.015591]\n",
            "[Epoch 0/200] [Batch 1820/3166] [D loss: 2.248500] [G loss: 6.799516] [gradient_penalty: 0.011557]\n",
            "[Epoch 0/200] [Batch 1821/3166] [D loss: 2.634173] [G loss: 6.715043] [gradient_penalty: 0.014455]\n",
            "[Epoch 0/200] [Batch 1822/3166] [D loss: 2.511449] [G loss: 6.610839] [gradient_penalty: 0.012976]\n",
            "[Epoch 0/200] [Batch 1823/3166] [D loss: 2.398439] [G loss: 7.224561] [gradient_penalty: 0.015081]\n",
            "[Epoch 0/200] [Batch 1824/3166] [D loss: 2.148719] [G loss: 7.359098] [gradient_penalty: 0.017544]\n",
            "[Epoch 0/200] [Batch 1825/3166] [D loss: 1.866657] [G loss: 7.019049] [gradient_penalty: 0.011878]\n",
            "[Epoch 0/200] [Batch 1826/3166] [D loss: 2.667228] [G loss: 6.794582] [gradient_penalty: 0.012240]\n",
            "[Epoch 0/200] [Batch 1827/3166] [D loss: 1.269869] [G loss: 7.294590] [gradient_penalty: 0.011993]\n",
            "[Epoch 0/200] [Batch 1828/3166] [D loss: 2.773683] [G loss: 6.698795] [gradient_penalty: 0.013262]\n",
            "[Epoch 0/200] [Batch 1829/3166] [D loss: 1.766665] [G loss: 7.121140] [gradient_penalty: 0.014950]\n",
            "[Epoch 0/200] [Batch 1830/3166] [D loss: 2.405566] [G loss: 6.856937] [gradient_penalty: 0.012006]\n",
            "[Epoch 0/200] [Batch 1831/3166] [D loss: 3.023237] [G loss: 7.354239] [gradient_penalty: 0.014612]\n",
            "[Epoch 0/200] [Batch 1832/3166] [D loss: 2.712153] [G loss: 6.775625] [gradient_penalty: 0.015675]\n",
            "[Epoch 0/200] [Batch 1833/3166] [D loss: 2.703609] [G loss: 6.772498] [gradient_penalty: 0.017035]\n",
            "[Epoch 0/200] [Batch 1834/3166] [D loss: 1.906652] [G loss: 7.215112] [gradient_penalty: 0.012405]\n",
            "[Epoch 0/200] [Batch 1835/3166] [D loss: 2.103179] [G loss: 6.929344] [gradient_penalty: 0.012411]\n",
            "[Epoch 0/200] [Batch 1836/3166] [D loss: 2.374355] [G loss: 6.948626] [gradient_penalty: 0.014199]\n",
            "[Epoch 0/200] [Batch 1837/3166] [D loss: 2.081790] [G loss: 6.858219] [gradient_penalty: 0.015305]\n",
            "[Epoch 0/200] [Batch 1838/3166] [D loss: 2.680312] [G loss: 7.452294] [gradient_penalty: 0.010809]\n",
            "[Epoch 0/200] [Batch 1839/3166] [D loss: 2.438339] [G loss: 7.047679] [gradient_penalty: 0.010430]\n",
            "[Epoch 0/200] [Batch 1840/3166] [D loss: 2.018496] [G loss: 6.729910] [gradient_penalty: 0.016243]\n",
            "[Epoch 0/200] [Batch 1841/3166] [D loss: 2.275757] [G loss: 6.633605] [gradient_penalty: 0.014158]\n",
            "[Epoch 0/200] [Batch 1842/3166] [D loss: 1.849490] [G loss: 6.650863] [gradient_penalty: 0.012395]\n",
            "[Epoch 0/200] [Batch 1843/3166] [D loss: 2.111115] [G loss: 7.138033] [gradient_penalty: 0.011516]\n",
            "[Epoch 0/200] [Batch 1844/3166] [D loss: 1.803827] [G loss: 7.305748] [gradient_penalty: 0.014195]\n",
            "[Epoch 0/200] [Batch 1845/3166] [D loss: 2.188415] [G loss: 7.093474] [gradient_penalty: 0.013068]\n",
            "[Epoch 0/200] [Batch 1846/3166] [D loss: 2.510419] [G loss: 7.396034] [gradient_penalty: 0.015167]\n",
            "[Epoch 0/200] [Batch 1847/3166] [D loss: 2.855537] [G loss: 7.421259] [gradient_penalty: 0.014614]\n",
            "[Epoch 0/200] [Batch 1848/3166] [D loss: 3.831328] [G loss: 7.248555] [gradient_penalty: 0.016635]\n",
            "[Epoch 0/200] [Batch 1849/3166] [D loss: 2.669914] [G loss: 7.396678] [gradient_penalty: 0.015175]\n",
            "[Epoch 0/200] [Batch 1850/3166] [D loss: 2.304179] [G loss: 7.004493] [gradient_penalty: 0.015169]\n",
            "[Epoch 0/200] [Batch 1851/3166] [D loss: 3.044715] [G loss: 7.020264] [gradient_penalty: 0.016839]\n",
            "[Epoch 0/200] [Batch 1852/3166] [D loss: 3.091237] [G loss: 7.303253] [gradient_penalty: 0.016767]\n",
            "[Epoch 0/200] [Batch 1853/3166] [D loss: 1.879509] [G loss: 7.125567] [gradient_penalty: 0.012668]\n",
            "[Epoch 0/200] [Batch 1854/3166] [D loss: 2.176541] [G loss: 7.099358] [gradient_penalty: 0.015669]\n",
            "[Epoch 0/200] [Batch 1855/3166] [D loss: 1.528953] [G loss: 7.051072] [gradient_penalty: 0.011865]\n",
            "[Epoch 0/200] [Batch 1856/3166] [D loss: 2.658232] [G loss: 7.495035] [gradient_penalty: 0.014910]\n",
            "[Epoch 0/200] [Batch 1857/3166] [D loss: 2.224016] [G loss: 7.387947] [gradient_penalty: 0.013632]\n",
            "[Epoch 0/200] [Batch 1858/3166] [D loss: 2.493163] [G loss: 7.133810] [gradient_penalty: 0.015167]\n",
            "[Epoch 0/200] [Batch 1859/3166] [D loss: 2.587729] [G loss: 6.986754] [gradient_penalty: 0.015564]\n",
            "[Epoch 0/200] [Batch 1860/3166] [D loss: 2.867979] [G loss: 6.856397] [gradient_penalty: 0.011429]\n",
            "[Epoch 0/200] [Batch 1861/3166] [D loss: 2.439926] [G loss: 7.567587] [gradient_penalty: 0.013023]\n",
            "[Epoch 0/200] [Batch 1862/3166] [D loss: 2.383486] [G loss: 6.776676] [gradient_penalty: 0.011876]\n",
            "[Epoch 0/200] [Batch 1863/3166] [D loss: 2.063603] [G loss: 6.838466] [gradient_penalty: 0.014373]\n",
            "[Epoch 0/200] [Batch 1864/3166] [D loss: 2.079634] [G loss: 7.263988] [gradient_penalty: 0.011679]\n",
            "[Epoch 0/200] [Batch 1865/3166] [D loss: 1.990539] [G loss: 7.051560] [gradient_penalty: 0.011813]\n",
            "[Epoch 0/200] [Batch 1866/3166] [D loss: 2.880596] [G loss: 6.669495] [gradient_penalty: 0.012544]\n",
            "[Epoch 0/200] [Batch 1867/3166] [D loss: 2.501288] [G loss: 6.810513] [gradient_penalty: 0.013898]\n",
            "[Epoch 0/200] [Batch 1868/3166] [D loss: 2.328167] [G loss: 7.092109] [gradient_penalty: 0.015749]\n",
            "[Epoch 0/200] [Batch 1869/3166] [D loss: 2.302176] [G loss: 6.859931] [gradient_penalty: 0.011181]\n",
            "[Epoch 0/200] [Batch 1870/3166] [D loss: 2.516255] [G loss: 7.008724] [gradient_penalty: 0.013048]\n",
            "[Epoch 0/200] [Batch 1871/3166] [D loss: 1.939832] [G loss: 6.810061] [gradient_penalty: 0.014256]\n",
            "[Epoch 0/200] [Batch 1872/3166] [D loss: 2.764587] [G loss: 6.580289] [gradient_penalty: 0.015443]\n",
            "[Epoch 0/200] [Batch 1873/3166] [D loss: 1.867739] [G loss: 7.165094] [gradient_penalty: 0.015511]\n",
            "[Epoch 0/200] [Batch 1874/3166] [D loss: 1.782585] [G loss: 6.882718] [gradient_penalty: 0.013328]\n",
            "[Epoch 0/200] [Batch 1875/3166] [D loss: 2.522777] [G loss: 7.644147] [gradient_penalty: 0.012757]\n",
            "[Epoch 0/200] [Batch 1876/3166] [D loss: 2.968765] [G loss: 6.857153] [gradient_penalty: 0.013964]\n",
            "[Epoch 0/200] [Batch 1877/3166] [D loss: 2.751562] [G loss: 6.713738] [gradient_penalty: 0.016199]\n",
            "[Epoch 0/200] [Batch 1878/3166] [D loss: 2.337809] [G loss: 7.024745] [gradient_penalty: 0.013065]\n",
            "[Epoch 0/200] [Batch 1879/3166] [D loss: 2.756114] [G loss: 6.873342] [gradient_penalty: 0.018174]\n",
            "[Epoch 0/200] [Batch 1880/3166] [D loss: 2.738070] [G loss: 7.373686] [gradient_penalty: 0.012982]\n",
            "[Epoch 0/200] [Batch 1881/3166] [D loss: 2.693141] [G loss: 7.230754] [gradient_penalty: 0.015233]\n",
            "[Epoch 0/200] [Batch 1882/3166] [D loss: 3.018139] [G loss: 6.642783] [gradient_penalty: 0.019698]\n",
            "[Epoch 0/200] [Batch 1883/3166] [D loss: 2.928154] [G loss: 6.845161] [gradient_penalty: 0.017007]\n",
            "[Epoch 0/200] [Batch 1884/3166] [D loss: 2.770208] [G loss: 7.099349] [gradient_penalty: 0.021081]\n",
            "[Epoch 0/200] [Batch 1885/3166] [D loss: 2.227108] [G loss: 7.115433] [gradient_penalty: 0.011108]\n",
            "[Epoch 0/200] [Batch 1886/3166] [D loss: 2.223246] [G loss: 7.778871] [gradient_penalty: 0.013016]\n",
            "[Epoch 0/200] [Batch 1887/3166] [D loss: 2.421485] [G loss: 6.530584] [gradient_penalty: 0.010489]\n",
            "[Epoch 0/200] [Batch 1888/3166] [D loss: 2.366071] [G loss: 6.976174] [gradient_penalty: 0.013602]\n",
            "[Epoch 0/200] [Batch 1889/3166] [D loss: 1.370081] [G loss: 6.880361] [gradient_penalty: 0.009801]\n",
            "[Epoch 0/200] [Batch 1890/3166] [D loss: 2.546336] [G loss: 6.583523] [gradient_penalty: 0.017535]\n",
            "[Epoch 0/200] [Batch 1891/3166] [D loss: 2.689922] [G loss: 6.854889] [gradient_penalty: 0.014857]\n",
            "[Epoch 0/200] [Batch 1892/3166] [D loss: 2.974998] [G loss: 7.031040] [gradient_penalty: 0.016179]\n",
            "[Epoch 0/200] [Batch 1893/3166] [D loss: 2.088818] [G loss: 7.116072] [gradient_penalty: 0.015919]\n",
            "[Epoch 0/200] [Batch 1894/3166] [D loss: 2.654980] [G loss: 7.018674] [gradient_penalty: 0.016535]\n",
            "[Epoch 0/200] [Batch 1895/3166] [D loss: 2.273442] [G loss: 7.123831] [gradient_penalty: 0.012592]\n",
            "[Epoch 0/200] [Batch 1896/3166] [D loss: 2.440647] [G loss: 6.645607] [gradient_penalty: 0.014398]\n",
            "[Epoch 0/200] [Batch 1897/3166] [D loss: 1.754451] [G loss: 6.738110] [gradient_penalty: 0.013275]\n",
            "[Epoch 0/200] [Batch 1898/3166] [D loss: 2.721947] [G loss: 7.150813] [gradient_penalty: 0.014787]\n",
            "[Epoch 0/200] [Batch 1899/3166] [D loss: 2.033998] [G loss: 6.259866] [gradient_penalty: 0.011558]\n",
            "[Epoch 0/200] [Batch 1900/3166] [D loss: 2.728788] [G loss: 6.533733] [gradient_penalty: 0.013932]\n",
            "log\n",
            "[Epoch 0/200] [Batch 1901/3166] [D loss: 2.532037] [G loss: 6.972540] [gradient_penalty: 0.011814]\n",
            "[Epoch 0/200] [Batch 1902/3166] [D loss: 2.416830] [G loss: 7.431260] [gradient_penalty: 0.016187]\n",
            "[Epoch 0/200] [Batch 1903/3166] [D loss: 2.169645] [G loss: 6.505593] [gradient_penalty: 0.010762]\n",
            "[Epoch 0/200] [Batch 1904/3166] [D loss: 2.863822] [G loss: 7.285175] [gradient_penalty: 0.012207]\n",
            "[Epoch 0/200] [Batch 1905/3166] [D loss: 2.455045] [G loss: 7.185249] [gradient_penalty: 0.012720]\n",
            "[Epoch 0/200] [Batch 1906/3166] [D loss: 2.285632] [G loss: 6.807450] [gradient_penalty: 0.015153]\n",
            "[Epoch 0/200] [Batch 1907/3166] [D loss: 2.511253] [G loss: 7.362304] [gradient_penalty: 0.012943]\n",
            "[Epoch 0/200] [Batch 1908/3166] [D loss: 2.937932] [G loss: 7.119617] [gradient_penalty: 0.017856]\n",
            "[Epoch 0/200] [Batch 1909/3166] [D loss: 1.880672] [G loss: 7.046436] [gradient_penalty: 0.015144]\n",
            "[Epoch 0/200] [Batch 1910/3166] [D loss: 2.141715] [G loss: 7.399252] [gradient_penalty: 0.014996]\n",
            "[Epoch 0/200] [Batch 1911/3166] [D loss: 1.891156] [G loss: 7.040067] [gradient_penalty: 0.013542]\n",
            "[Epoch 0/200] [Batch 1912/3166] [D loss: 2.654276] [G loss: 7.360828] [gradient_penalty: 0.012199]\n",
            "[Epoch 0/200] [Batch 1913/3166] [D loss: 2.387070] [G loss: 6.505449] [gradient_penalty: 0.016056]\n",
            "[Epoch 0/200] [Batch 1914/3166] [D loss: 1.868217] [G loss: 7.247531] [gradient_penalty: 0.015642]\n",
            "[Epoch 0/200] [Batch 1915/3166] [D loss: 2.845492] [G loss: 7.183829] [gradient_penalty: 0.015650]\n",
            "[Epoch 0/200] [Batch 1916/3166] [D loss: 2.265542] [G loss: 7.045044] [gradient_penalty: 0.014952]\n",
            "[Epoch 0/200] [Batch 1917/3166] [D loss: 2.206949] [G loss: 7.454322] [gradient_penalty: 0.017175]\n",
            "[Epoch 0/200] [Batch 1918/3166] [D loss: 2.061907] [G loss: 7.089921] [gradient_penalty: 0.012575]\n",
            "[Epoch 0/200] [Batch 1919/3166] [D loss: 2.326458] [G loss: 6.911949] [gradient_penalty: 0.014293]\n",
            "[Epoch 0/200] [Batch 1920/3166] [D loss: 2.235910] [G loss: 7.176600] [gradient_penalty: 0.014356]\n",
            "[Epoch 0/200] [Batch 1921/3166] [D loss: 1.985242] [G loss: 7.413308] [gradient_penalty: 0.015474]\n",
            "[Epoch 0/200] [Batch 1922/3166] [D loss: 2.064687] [G loss: 7.561956] [gradient_penalty: 0.016395]\n",
            "[Epoch 0/200] [Batch 1923/3166] [D loss: 2.716375] [G loss: 7.168114] [gradient_penalty: 0.013422]\n",
            "[Epoch 0/200] [Batch 1924/3166] [D loss: 1.621695] [G loss: 7.308868] [gradient_penalty: 0.010837]\n",
            "[Epoch 0/200] [Batch 1925/3166] [D loss: 2.009476] [G loss: 7.303517] [gradient_penalty: 0.010469]\n",
            "[Epoch 0/200] [Batch 1926/3166] [D loss: 2.280246] [G loss: 7.440732] [gradient_penalty: 0.013657]\n",
            "[Epoch 0/200] [Batch 1927/3166] [D loss: 2.064022] [G loss: 7.532215] [gradient_penalty: 0.012614]\n",
            "[Epoch 0/200] [Batch 1928/3166] [D loss: 2.175545] [G loss: 7.128271] [gradient_penalty: 0.017956]\n",
            "[Epoch 0/200] [Batch 1929/3166] [D loss: 1.719137] [G loss: 7.615644] [gradient_penalty: 0.012250]\n",
            "[Epoch 0/200] [Batch 1930/3166] [D loss: 1.959031] [G loss: 7.681595] [gradient_penalty: 0.015727]\n",
            "[Epoch 0/200] [Batch 1931/3166] [D loss: 1.875855] [G loss: 7.153572] [gradient_penalty: 0.009650]\n",
            "[Epoch 0/200] [Batch 1932/3166] [D loss: 1.406479] [G loss: 7.939678] [gradient_penalty: 0.011787]\n",
            "[Epoch 0/200] [Batch 1933/3166] [D loss: 2.828074] [G loss: 7.594913] [gradient_penalty: 0.018557]\n",
            "[Epoch 0/200] [Batch 1934/3166] [D loss: 1.878191] [G loss: 7.506132] [gradient_penalty: 0.014883]\n",
            "[Epoch 0/200] [Batch 1935/3166] [D loss: 2.357870] [G loss: 7.646952] [gradient_penalty: 0.012983]\n",
            "[Epoch 0/200] [Batch 1936/3166] [D loss: 2.563125] [G loss: 7.351178] [gradient_penalty: 0.012541]\n",
            "[Epoch 0/200] [Batch 1937/3166] [D loss: 2.934771] [G loss: 7.706526] [gradient_penalty: 0.014599]\n",
            "[Epoch 0/200] [Batch 1938/3166] [D loss: 2.229829] [G loss: 7.975664] [gradient_penalty: 0.013369]\n",
            "[Epoch 0/200] [Batch 1939/3166] [D loss: 2.523559] [G loss: 7.552061] [gradient_penalty: 0.015243]\n",
            "[Epoch 0/200] [Batch 1940/3166] [D loss: 2.907820] [G loss: 7.834902] [gradient_penalty: 0.019918]\n",
            "[Epoch 0/200] [Batch 1941/3166] [D loss: 2.764154] [G loss: 7.550200] [gradient_penalty: 0.018193]\n",
            "[Epoch 0/200] [Batch 1942/3166] [D loss: 2.205502] [G loss: 7.465020] [gradient_penalty: 0.011482]\n",
            "[Epoch 0/200] [Batch 1943/3166] [D loss: 2.480144] [G loss: 7.478044] [gradient_penalty: 0.013195]\n",
            "[Epoch 0/200] [Batch 1944/3166] [D loss: 2.659519] [G loss: 7.301063] [gradient_penalty: 0.017523]\n",
            "[Epoch 0/200] [Batch 1945/3166] [D loss: 2.191535] [G loss: 7.540215] [gradient_penalty: 0.012359]\n",
            "[Epoch 0/200] [Batch 1946/3166] [D loss: 2.186299] [G loss: 7.473318] [gradient_penalty: 0.012996]\n",
            "[Epoch 0/200] [Batch 1947/3166] [D loss: 2.773819] [G loss: 7.197482] [gradient_penalty: 0.020328]\n",
            "[Epoch 0/200] [Batch 1948/3166] [D loss: 2.375256] [G loss: 7.431097] [gradient_penalty: 0.014638]\n",
            "[Epoch 0/200] [Batch 1949/3166] [D loss: 2.544375] [G loss: 7.353151] [gradient_penalty: 0.014275]\n",
            "[Epoch 0/200] [Batch 1950/3166] [D loss: 2.821019] [G loss: 7.221544] [gradient_penalty: 0.018923]\n",
            "[Epoch 0/200] [Batch 1951/3166] [D loss: 2.337092] [G loss: 6.945188] [gradient_penalty: 0.016650]\n",
            "[Epoch 0/200] [Batch 1952/3166] [D loss: 2.124526] [G loss: 7.154016] [gradient_penalty: 0.015592]\n",
            "[Epoch 0/200] [Batch 1953/3166] [D loss: 1.845000] [G loss: 7.338284] [gradient_penalty: 0.010997]\n",
            "[Epoch 0/200] [Batch 1954/3166] [D loss: 3.183246] [G loss: 7.398360] [gradient_penalty: 0.018428]\n",
            "[Epoch 0/200] [Batch 1955/3166] [D loss: 2.069173] [G loss: 7.216442] [gradient_penalty: 0.016698]\n",
            "[Epoch 0/200] [Batch 1956/3166] [D loss: 2.513190] [G loss: 7.236658] [gradient_penalty: 0.013061]\n",
            "[Epoch 0/200] [Batch 1957/3166] [D loss: 2.774963] [G loss: 7.011286] [gradient_penalty: 0.015530]\n",
            "[Epoch 0/200] [Batch 1958/3166] [D loss: 2.324967] [G loss: 6.924517] [gradient_penalty: 0.008713]\n",
            "[Epoch 0/200] [Batch 1959/3166] [D loss: 2.804138] [G loss: 7.684360] [gradient_penalty: 0.013352]\n",
            "[Epoch 0/200] [Batch 1960/3166] [D loss: 1.982795] [G loss: 7.170198] [gradient_penalty: 0.016598]\n",
            "[Epoch 0/200] [Batch 1961/3166] [D loss: 2.036905] [G loss: 7.225735] [gradient_penalty: 0.015424]\n",
            "[Epoch 0/200] [Batch 1962/3166] [D loss: 1.438739] [G loss: 7.543127] [gradient_penalty: 0.013331]\n",
            "[Epoch 0/200] [Batch 1963/3166] [D loss: 2.499782] [G loss: 7.674157] [gradient_penalty: 0.019705]\n",
            "[Epoch 0/200] [Batch 1964/3166] [D loss: 2.140031] [G loss: 7.583181] [gradient_penalty: 0.015316]\n",
            "[Epoch 0/200] [Batch 1965/3166] [D loss: 2.470556] [G loss: 7.876455] [gradient_penalty: 0.011572]\n",
            "[Epoch 0/200] [Batch 1966/3166] [D loss: 1.932115] [G loss: 7.524329] [gradient_penalty: 0.011875]\n",
            "[Epoch 0/200] [Batch 1967/3166] [D loss: 3.022349] [G loss: 7.527898] [gradient_penalty: 0.015849]\n",
            "[Epoch 0/200] [Batch 1968/3166] [D loss: 2.336085] [G loss: 7.180609] [gradient_penalty: 0.014333]\n",
            "[Epoch 0/200] [Batch 1969/3166] [D loss: 2.780564] [G loss: 7.275167] [gradient_penalty: 0.014208]\n",
            "[Epoch 0/200] [Batch 1970/3166] [D loss: 2.357701] [G loss: 7.542714] [gradient_penalty: 0.017663]\n",
            "[Epoch 0/200] [Batch 1971/3166] [D loss: 2.124335] [G loss: 7.364471] [gradient_penalty: 0.012023]\n",
            "[Epoch 0/200] [Batch 1972/3166] [D loss: 2.211421] [G loss: 7.390222] [gradient_penalty: 0.011973]\n",
            "[Epoch 0/200] [Batch 1973/3166] [D loss: 2.100892] [G loss: 6.970128] [gradient_penalty: 0.011940]\n",
            "[Epoch 0/200] [Batch 1974/3166] [D loss: 2.611906] [G loss: 7.507189] [gradient_penalty: 0.016865]\n",
            "[Epoch 0/200] [Batch 1975/3166] [D loss: 2.209748] [G loss: 7.412301] [gradient_penalty: 0.014038]\n",
            "[Epoch 0/200] [Batch 1976/3166] [D loss: 1.872072] [G loss: 7.514901] [gradient_penalty: 0.013355]\n",
            "[Epoch 0/200] [Batch 1977/3166] [D loss: 2.454812] [G loss: 7.453563] [gradient_penalty: 0.016518]\n",
            "[Epoch 0/200] [Batch 1978/3166] [D loss: 2.441600] [G loss: 6.976828] [gradient_penalty: 0.012057]\n",
            "[Epoch 0/200] [Batch 1979/3166] [D loss: 1.855893] [G loss: 7.589798] [gradient_penalty: 0.015584]\n",
            "[Epoch 0/200] [Batch 1980/3166] [D loss: 2.661300] [G loss: 6.808683] [gradient_penalty: 0.015504]\n",
            "[Epoch 0/200] [Batch 1981/3166] [D loss: 2.594257] [G loss: 7.027340] [gradient_penalty: 0.013499]\n",
            "[Epoch 0/200] [Batch 1982/3166] [D loss: 2.543918] [G loss: 7.239814] [gradient_penalty: 0.014308]\n",
            "[Epoch 0/200] [Batch 1983/3166] [D loss: 1.878881] [G loss: 7.528794] [gradient_penalty: 0.009190]\n",
            "[Epoch 0/200] [Batch 1984/3166] [D loss: 1.771191] [G loss: 7.518039] [gradient_penalty: 0.013329]\n",
            "[Epoch 0/200] [Batch 1985/3166] [D loss: 2.364266] [G loss: 7.355754] [gradient_penalty: 0.018051]\n",
            "[Epoch 0/200] [Batch 1986/3166] [D loss: 2.106800] [G loss: 7.579123] [gradient_penalty: 0.014005]\n",
            "[Epoch 0/200] [Batch 1987/3166] [D loss: 1.717478] [G loss: 7.603017] [gradient_penalty: 0.011357]\n",
            "[Epoch 0/200] [Batch 1988/3166] [D loss: 2.149293] [G loss: 7.393739] [gradient_penalty: 0.010814]\n",
            "[Epoch 0/200] [Batch 1989/3166] [D loss: 2.011793] [G loss: 7.906151] [gradient_penalty: 0.010744]\n",
            "[Epoch 0/200] [Batch 1990/3166] [D loss: 1.574433] [G loss: 7.888509] [gradient_penalty: 0.014933]\n",
            "[Epoch 0/200] [Batch 1991/3166] [D loss: 2.305506] [G loss: 7.297516] [gradient_penalty: 0.012972]\n",
            "[Epoch 0/200] [Batch 1992/3166] [D loss: 2.194956] [G loss: 7.671921] [gradient_penalty: 0.015509]\n",
            "[Epoch 0/200] [Batch 1993/3166] [D loss: 2.218752] [G loss: 7.341182] [gradient_penalty: 0.011870]\n",
            "[Epoch 0/200] [Batch 1994/3166] [D loss: 2.654638] [G loss: 7.195826] [gradient_penalty: 0.012624]\n",
            "[Epoch 0/200] [Batch 1995/3166] [D loss: 2.461979] [G loss: 7.475369] [gradient_penalty: 0.008905]\n",
            "[Epoch 0/200] [Batch 1996/3166] [D loss: 1.733483] [G loss: 7.198363] [gradient_penalty: 0.013250]\n",
            "[Epoch 0/200] [Batch 1997/3166] [D loss: 1.638526] [G loss: 7.480515] [gradient_penalty: 0.013838]\n",
            "[Epoch 0/200] [Batch 1998/3166] [D loss: 2.098365] [G loss: 7.402442] [gradient_penalty: 0.016473]\n",
            "[Epoch 0/200] [Batch 1999/3166] [D loss: 1.990816] [G loss: 7.428571] [gradient_penalty: 0.013046]\n",
            "[Epoch 0/200] [Batch 2000/3166] [D loss: 2.275003] [G loss: 7.419912] [gradient_penalty: 0.014886]\n",
            "log\n",
            "[Epoch 0/200] [Batch 2001/3166] [D loss: 2.071566] [G loss: 7.501598] [gradient_penalty: 0.014245]\n",
            "[Epoch 0/200] [Batch 2002/3166] [D loss: 3.019429] [G loss: 7.214093] [gradient_penalty: 0.012177]\n",
            "[Epoch 0/200] [Batch 2003/3166] [D loss: 1.721787] [G loss: 7.468236] [gradient_penalty: 0.014996]\n",
            "[Epoch 0/200] [Batch 2004/3166] [D loss: 2.559739] [G loss: 7.474495] [gradient_penalty: 0.013456]\n",
            "[Epoch 0/200] [Batch 2005/3166] [D loss: 2.097517] [G loss: 7.979045] [gradient_penalty: 0.014897]\n",
            "[Epoch 0/200] [Batch 2006/3166] [D loss: 1.902624] [G loss: 7.802215] [gradient_penalty: 0.013509]\n",
            "[Epoch 0/200] [Batch 2007/3166] [D loss: 2.104858] [G loss: 7.845047] [gradient_penalty: 0.010294]\n",
            "[Epoch 0/200] [Batch 2008/3166] [D loss: 2.407168] [G loss: 8.048265] [gradient_penalty: 0.012503]\n",
            "[Epoch 0/200] [Batch 2009/3166] [D loss: 2.246685] [G loss: 7.539150] [gradient_penalty: 0.011630]\n",
            "[Epoch 0/200] [Batch 2010/3166] [D loss: 3.016685] [G loss: 7.678383] [gradient_penalty: 0.012681]\n",
            "[Epoch 0/200] [Batch 2011/3166] [D loss: 1.929647] [G loss: 7.768395] [gradient_penalty: 0.010577]\n",
            "[Epoch 0/200] [Batch 2012/3166] [D loss: 2.476691] [G loss: 7.520003] [gradient_penalty: 0.015275]\n",
            "[Epoch 0/200] [Batch 2013/3166] [D loss: 2.907151] [G loss: 8.232801] [gradient_penalty: 0.013831]\n",
            "[Epoch 0/200] [Batch 2014/3166] [D loss: 2.751346] [G loss: 7.570650] [gradient_penalty: 0.015361]\n",
            "[Epoch 0/200] [Batch 2015/3166] [D loss: 2.589309] [G loss: 8.304888] [gradient_penalty: 0.010429]\n",
            "[Epoch 0/200] [Batch 2016/3166] [D loss: 1.434150] [G loss: 8.084341] [gradient_penalty: 0.010127]\n",
            "[Epoch 0/200] [Batch 2017/3166] [D loss: 2.798710] [G loss: 7.552976] [gradient_penalty: 0.012743]\n",
            "[Epoch 0/200] [Batch 2018/3166] [D loss: 2.769077] [G loss: 7.715961] [gradient_penalty: 0.012255]\n",
            "[Epoch 0/200] [Batch 2019/3166] [D loss: 2.544388] [G loss: 8.065229] [gradient_penalty: 0.012699]\n",
            "[Epoch 0/200] [Batch 2020/3166] [D loss: 2.065140] [G loss: 7.752182] [gradient_penalty: 0.012506]\n",
            "[Epoch 0/200] [Batch 2021/3166] [D loss: 2.604703] [G loss: 7.773738] [gradient_penalty: 0.011494]\n",
            "[Epoch 0/200] [Batch 2022/3166] [D loss: 2.718475] [G loss: 7.910365] [gradient_penalty: 0.013103]\n",
            "[Epoch 0/200] [Batch 2023/3166] [D loss: 2.438744] [G loss: 8.062436] [gradient_penalty: 0.013104]\n",
            "[Epoch 0/200] [Batch 2024/3166] [D loss: 1.806438] [G loss: 7.365734] [gradient_penalty: 0.006613]\n",
            "[Epoch 0/200] [Batch 2025/3166] [D loss: 1.998617] [G loss: 7.616363] [gradient_penalty: 0.010467]\n",
            "[Epoch 0/200] [Batch 2026/3166] [D loss: 1.689087] [G loss: 7.781497] [gradient_penalty: 0.011784]\n",
            "[Epoch 0/200] [Batch 2027/3166] [D loss: 1.823352] [G loss: 8.149844] [gradient_penalty: 0.008923]\n",
            "[Epoch 0/200] [Batch 2028/3166] [D loss: 2.631990] [G loss: 7.854577] [gradient_penalty: 0.013609]\n",
            "[Epoch 0/200] [Batch 2029/3166] [D loss: 1.456945] [G loss: 8.092371] [gradient_penalty: 0.013883]\n",
            "[Epoch 0/200] [Batch 2030/3166] [D loss: 2.521174] [G loss: 8.059358] [gradient_penalty: 0.010511]\n",
            "[Epoch 0/200] [Batch 2031/3166] [D loss: 2.126828] [G loss: 7.978399] [gradient_penalty: 0.009893]\n",
            "[Epoch 0/200] [Batch 2032/3166] [D loss: 2.451392] [G loss: 8.179614] [gradient_penalty: 0.013515]\n",
            "[Epoch 0/200] [Batch 2033/3166] [D loss: 2.308844] [G loss: 8.085194] [gradient_penalty: 0.012150]\n",
            "[Epoch 0/200] [Batch 2034/3166] [D loss: 1.801633] [G loss: 7.672246] [gradient_penalty: 0.010561]\n",
            "[Epoch 0/200] [Batch 2035/3166] [D loss: 1.902480] [G loss: 7.879571] [gradient_penalty: 0.012383]\n",
            "[Epoch 0/200] [Batch 2036/3166] [D loss: 2.685621] [G loss: 8.043985] [gradient_penalty: 0.014268]\n",
            "[Epoch 0/200] [Batch 2037/3166] [D loss: 1.590616] [G loss: 7.642497] [gradient_penalty: 0.012380]\n",
            "[Epoch 0/200] [Batch 2038/3166] [D loss: 2.855010] [G loss: 8.168930] [gradient_penalty: 0.016589]\n",
            "[Epoch 0/200] [Batch 2039/3166] [D loss: 2.274173] [G loss: 7.894886] [gradient_penalty: 0.010771]\n",
            "[Epoch 0/200] [Batch 2040/3166] [D loss: 1.528755] [G loss: 8.139519] [gradient_penalty: 0.012418]\n",
            "[Epoch 0/200] [Batch 2041/3166] [D loss: 2.724553] [G loss: 8.075300] [gradient_penalty: 0.014636]\n",
            "[Epoch 0/200] [Batch 2042/3166] [D loss: 1.823734] [G loss: 8.105306] [gradient_penalty: 0.012585]\n",
            "[Epoch 0/200] [Batch 2043/3166] [D loss: 2.661355] [G loss: 7.971811] [gradient_penalty: 0.015197]\n",
            "[Epoch 0/200] [Batch 2044/3166] [D loss: 2.364051] [G loss: 7.926571] [gradient_penalty: 0.010010]\n",
            "[Epoch 0/200] [Batch 2045/3166] [D loss: 2.228290] [G loss: 7.860802] [gradient_penalty: 0.013454]\n",
            "[Epoch 0/200] [Batch 2046/3166] [D loss: 2.267244] [G loss: 7.663934] [gradient_penalty: 0.013282]\n",
            "[Epoch 0/200] [Batch 2047/3166] [D loss: 1.345880] [G loss: 8.043694] [gradient_penalty: 0.010907]\n",
            "[Epoch 0/200] [Batch 2048/3166] [D loss: 1.872179] [G loss: 7.797272] [gradient_penalty: 0.010568]\n",
            "[Epoch 0/200] [Batch 2049/3166] [D loss: 2.813809] [G loss: 7.691014] [gradient_penalty: 0.014124]\n",
            "[Epoch 0/200] [Batch 2050/3166] [D loss: 2.340546] [G loss: 8.019065] [gradient_penalty: 0.015397]\n",
            "[Epoch 0/200] [Batch 2051/3166] [D loss: 2.820277] [G loss: 7.984791] [gradient_penalty: 0.017187]\n",
            "[Epoch 0/200] [Batch 2052/3166] [D loss: 2.095533] [G loss: 9.018269] [gradient_penalty: 0.008606]\n",
            "[Epoch 0/200] [Batch 2053/3166] [D loss: 2.797032] [G loss: 7.103822] [gradient_penalty: 0.014119]\n",
            "[Epoch 0/200] [Batch 2054/3166] [D loss: 2.436220] [G loss: 7.708854] [gradient_penalty: 0.008810]\n",
            "[Epoch 0/200] [Batch 2055/3166] [D loss: 2.772317] [G loss: 8.073753] [gradient_penalty: 0.015026]\n",
            "[Epoch 0/200] [Batch 2056/3166] [D loss: 2.175929] [G loss: 8.226857] [gradient_penalty: 0.013445]\n",
            "[Epoch 0/200] [Batch 2057/3166] [D loss: 2.410283] [G loss: 8.000116] [gradient_penalty: 0.010234]\n",
            "[Epoch 0/200] [Batch 2058/3166] [D loss: 2.184774] [G loss: 8.412688] [gradient_penalty: 0.009684]\n",
            "[Epoch 0/200] [Batch 2059/3166] [D loss: 1.698265] [G loss: 8.057867] [gradient_penalty: 0.010783]\n",
            "[Epoch 0/200] [Batch 2060/3166] [D loss: 2.224454] [G loss: 8.348845] [gradient_penalty: 0.011916]\n",
            "[Epoch 0/200] [Batch 2061/3166] [D loss: 2.037685] [G loss: 8.218262] [gradient_penalty: 0.016474]\n",
            "[Epoch 0/200] [Batch 2062/3166] [D loss: 2.061965] [G loss: 7.636885] [gradient_penalty: 0.011271]\n",
            "[Epoch 0/200] [Batch 2063/3166] [D loss: 1.527243] [G loss: 8.340340] [gradient_penalty: 0.009771]\n",
            "[Epoch 0/200] [Batch 2064/3166] [D loss: 2.577041] [G loss: 8.337595] [gradient_penalty: 0.012906]\n",
            "[Epoch 0/200] [Batch 2065/3166] [D loss: 2.798467] [G loss: 7.990061] [gradient_penalty: 0.010724]\n",
            "[Epoch 0/200] [Batch 2066/3166] [D loss: 1.873038] [G loss: 7.782792] [gradient_penalty: 0.015261]\n",
            "[Epoch 0/200] [Batch 2067/3166] [D loss: 2.651073] [G loss: 8.195641] [gradient_penalty: 0.010108]\n",
            "[Epoch 0/200] [Batch 2068/3166] [D loss: 2.045586] [G loss: 7.940345] [gradient_penalty: 0.012847]\n",
            "[Epoch 0/200] [Batch 2069/3166] [D loss: 2.573391] [G loss: 8.069759] [gradient_penalty: 0.012486]\n",
            "[Epoch 0/200] [Batch 2070/3166] [D loss: 1.474144] [G loss: 8.224569] [gradient_penalty: 0.009432]\n",
            "[Epoch 0/200] [Batch 2071/3166] [D loss: 2.259977] [G loss: 8.384102] [gradient_penalty: 0.012311]\n",
            "[Epoch 0/200] [Batch 2072/3166] [D loss: 3.448286] [G loss: 8.377376] [gradient_penalty: 0.014725]\n",
            "[Epoch 0/200] [Batch 2073/3166] [D loss: 2.644583] [G loss: 8.186536] [gradient_penalty: 0.013705]\n",
            "[Epoch 0/200] [Batch 2074/3166] [D loss: 2.480386] [G loss: 8.352154] [gradient_penalty: 0.015925]\n",
            "[Epoch 0/200] [Batch 2075/3166] [D loss: 1.677891] [G loss: 8.532187] [gradient_penalty: 0.011039]\n",
            "[Epoch 0/200] [Batch 2076/3166] [D loss: 1.943756] [G loss: 8.512840] [gradient_penalty: 0.012159]\n",
            "[Epoch 0/200] [Batch 2077/3166] [D loss: 2.377333] [G loss: 8.429541] [gradient_penalty: 0.009996]\n",
            "[Epoch 0/200] [Batch 2078/3166] [D loss: 3.002382] [G loss: 8.060028] [gradient_penalty: 0.011437]\n",
            "[Epoch 0/200] [Batch 2079/3166] [D loss: 1.660880] [G loss: 7.964149] [gradient_penalty: 0.011520]\n",
            "[Epoch 0/200] [Batch 2080/3166] [D loss: 1.886105] [G loss: 8.297387] [gradient_penalty: 0.011317]\n",
            "[Epoch 0/200] [Batch 2081/3166] [D loss: 2.885916] [G loss: 8.008698] [gradient_penalty: 0.013677]\n",
            "[Epoch 0/200] [Batch 2082/3166] [D loss: 1.736680] [G loss: 8.376080] [gradient_penalty: 0.008214]\n",
            "[Epoch 0/200] [Batch 2083/3166] [D loss: 2.330850] [G loss: 8.440725] [gradient_penalty: 0.007388]\n",
            "[Epoch 0/200] [Batch 2084/3166] [D loss: 2.284456] [G loss: 8.132256] [gradient_penalty: 0.011598]\n",
            "[Epoch 0/200] [Batch 2085/3166] [D loss: 2.670942] [G loss: 8.235775] [gradient_penalty: 0.014083]\n",
            "[Epoch 0/200] [Batch 2086/3166] [D loss: 2.088601] [G loss: 7.706676] [gradient_penalty: 0.015532]\n",
            "[Epoch 0/200] [Batch 2087/3166] [D loss: 2.562850] [G loss: 7.982189] [gradient_penalty: 0.010538]\n",
            "[Epoch 0/200] [Batch 2088/3166] [D loss: 1.740681] [G loss: 8.279149] [gradient_penalty: 0.013277]\n",
            "[Epoch 0/200] [Batch 2089/3166] [D loss: 2.529151] [G loss: 8.879838] [gradient_penalty: 0.011196]\n",
            "[Epoch 0/200] [Batch 2090/3166] [D loss: 3.040337] [G loss: 8.202553] [gradient_penalty: 0.011616]\n",
            "[Epoch 0/200] [Batch 2091/3166] [D loss: 1.953378] [G loss: 8.173253] [gradient_penalty: 0.019130]\n",
            "[Epoch 0/200] [Batch 2092/3166] [D loss: 2.183271] [G loss: 8.170074] [gradient_penalty: 0.009948]\n",
            "[Epoch 0/200] [Batch 2093/3166] [D loss: 2.809130] [G loss: 8.149142] [gradient_penalty: 0.013568]\n",
            "[Epoch 0/200] [Batch 2094/3166] [D loss: 2.395600] [G loss: 7.773559] [gradient_penalty: 0.015444]\n",
            "[Epoch 0/200] [Batch 2095/3166] [D loss: 1.876021] [G loss: 8.556120] [gradient_penalty: 0.011888]\n",
            "[Epoch 0/200] [Batch 2096/3166] [D loss: 2.034465] [G loss: 8.734253] [gradient_penalty: 0.013368]\n",
            "[Epoch 0/200] [Batch 2097/3166] [D loss: 2.223614] [G loss: 8.435812] [gradient_penalty: 0.011554]\n",
            "[Epoch 0/200] [Batch 2098/3166] [D loss: 1.353350] [G loss: 8.054215] [gradient_penalty: 0.009078]\n",
            "[Epoch 0/200] [Batch 2099/3166] [D loss: 2.260097] [G loss: 8.469097] [gradient_penalty: 0.016000]\n",
            "[Epoch 0/200] [Batch 2100/3166] [D loss: 2.030842] [G loss: 8.615656] [gradient_penalty: 0.013536]\n",
            "log\n",
            "[Epoch 0/200] [Batch 2101/3166] [D loss: 3.070145] [G loss: 8.062470] [gradient_penalty: 0.010216]\n",
            "[Epoch 0/200] [Batch 2102/3166] [D loss: 2.482623] [G loss: 7.951790] [gradient_penalty: 0.013685]\n",
            "[Epoch 0/200] [Batch 2103/3166] [D loss: 1.754703] [G loss: 8.695753] [gradient_penalty: 0.011168]\n",
            "[Epoch 0/200] [Batch 2104/3166] [D loss: 2.455154] [G loss: 7.844781] [gradient_penalty: 0.016287]\n",
            "[Epoch 0/200] [Batch 2105/3166] [D loss: 2.448997] [G loss: 7.936212] [gradient_penalty: 0.009573]\n",
            "[Epoch 0/200] [Batch 2106/3166] [D loss: 2.497588] [G loss: 8.067652] [gradient_penalty: 0.013044]\n",
            "[Epoch 0/200] [Batch 2107/3166] [D loss: 2.613591] [G loss: 8.173392] [gradient_penalty: 0.012453]\n",
            "[Epoch 0/200] [Batch 2108/3166] [D loss: 3.035928] [G loss: 7.744291] [gradient_penalty: 0.010727]\n",
            "[Epoch 0/200] [Batch 2109/3166] [D loss: 1.777265] [G loss: 7.441854] [gradient_penalty: 0.013817]\n",
            "[Epoch 0/200] [Batch 2110/3166] [D loss: 2.258171] [G loss: 8.134571] [gradient_penalty: 0.016754]\n",
            "[Epoch 0/200] [Batch 2111/3166] [D loss: 2.233258] [G loss: 8.162040] [gradient_penalty: 0.011529]\n",
            "[Epoch 0/200] [Batch 2112/3166] [D loss: 2.534244] [G loss: 8.106543] [gradient_penalty: 0.013930]\n",
            "[Epoch 0/200] [Batch 2113/3166] [D loss: 2.201886] [G loss: 8.008600] [gradient_penalty: 0.009540]\n",
            "[Epoch 0/200] [Batch 2114/3166] [D loss: 1.013085] [G loss: 8.314744] [gradient_penalty: 0.006991]\n",
            "[Epoch 0/200] [Batch 2115/3166] [D loss: 1.707340] [G loss: 7.710518] [gradient_penalty: 0.010260]\n",
            "[Epoch 0/200] [Batch 2116/3166] [D loss: 2.118107] [G loss: 8.494113] [gradient_penalty: 0.012074]\n",
            "[Epoch 0/200] [Batch 2117/3166] [D loss: 2.098961] [G loss: 7.671118] [gradient_penalty: 0.010185]\n",
            "[Epoch 0/200] [Batch 2118/3166] [D loss: 1.891810] [G loss: 8.397057] [gradient_penalty: 0.013521]\n",
            "[Epoch 0/200] [Batch 2119/3166] [D loss: 2.629367] [G loss: 8.279680] [gradient_penalty: 0.013105]\n",
            "[Epoch 0/200] [Batch 2120/3166] [D loss: 2.085090] [G loss: 8.560696] [gradient_penalty: 0.010619]\n",
            "[Epoch 0/200] [Batch 2121/3166] [D loss: 2.055528] [G loss: 8.336981] [gradient_penalty: 0.009676]\n",
            "[Epoch 0/200] [Batch 2122/3166] [D loss: 2.169737] [G loss: 8.114769] [gradient_penalty: 0.009531]\n",
            "[Epoch 0/200] [Batch 2123/3166] [D loss: 1.145515] [G loss: 8.379171] [gradient_penalty: 0.008052]\n",
            "[Epoch 0/200] [Batch 2124/3166] [D loss: 2.112530] [G loss: 8.597212] [gradient_penalty: 0.011831]\n",
            "[Epoch 0/200] [Batch 2125/3166] [D loss: 2.482080] [G loss: 8.103863] [gradient_penalty: 0.011867]\n",
            "[Epoch 0/200] [Batch 2126/3166] [D loss: 2.082059] [G loss: 8.162481] [gradient_penalty: 0.014990]\n",
            "[Epoch 0/200] [Batch 2127/3166] [D loss: 1.956759] [G loss: 8.582213] [gradient_penalty: 0.015124]\n",
            "[Epoch 0/200] [Batch 2128/3166] [D loss: 1.605416] [G loss: 8.637559] [gradient_penalty: 0.014096]\n",
            "[Epoch 0/200] [Batch 2129/3166] [D loss: 1.548411] [G loss: 8.286804] [gradient_penalty: 0.011562]\n",
            "[Epoch 0/200] [Batch 2130/3166] [D loss: 1.815790] [G loss: 8.232064] [gradient_penalty: 0.012331]\n",
            "[Epoch 0/200] [Batch 2131/3166] [D loss: 2.030470] [G loss: 8.636496] [gradient_penalty: 0.013002]\n",
            "[Epoch 0/200] [Batch 2132/3166] [D loss: 1.534088] [G loss: 8.168673] [gradient_penalty: 0.011396]\n",
            "[Epoch 0/200] [Batch 2133/3166] [D loss: 2.229053] [G loss: 8.611081] [gradient_penalty: 0.015319]\n",
            "[Epoch 0/200] [Batch 2134/3166] [D loss: 1.532929] [G loss: 8.843926] [gradient_penalty: 0.013102]\n",
            "[Epoch 0/200] [Batch 2135/3166] [D loss: 2.537099] [G loss: 8.569672] [gradient_penalty: 0.015171]\n",
            "[Epoch 0/200] [Batch 2136/3166] [D loss: 2.194419] [G loss: 9.450130] [gradient_penalty: 0.015913]\n",
            "[Epoch 0/200] [Batch 2137/3166] [D loss: 2.228527] [G loss: 8.639618] [gradient_penalty: 0.013499]\n",
            "[Epoch 0/200] [Batch 2138/3166] [D loss: 1.495824] [G loss: 8.472005] [gradient_penalty: 0.010040]\n",
            "[Epoch 0/200] [Batch 2139/3166] [D loss: 1.923627] [G loss: 9.106178] [gradient_penalty: 0.010755]\n",
            "[Epoch 0/200] [Batch 2140/3166] [D loss: 1.973998] [G loss: 8.102434] [gradient_penalty: 0.011322]\n",
            "[Epoch 0/200] [Batch 2141/3166] [D loss: 1.882507] [G loss: 7.971569] [gradient_penalty: 0.013205]\n",
            "[Epoch 0/200] [Batch 2142/3166] [D loss: 1.809814] [G loss: 8.700206] [gradient_penalty: 0.011020]\n",
            "[Epoch 0/200] [Batch 2143/3166] [D loss: 2.872500] [G loss: 8.593842] [gradient_penalty: 0.014158]\n",
            "[Epoch 0/200] [Batch 2144/3166] [D loss: 1.583081] [G loss: 8.090076] [gradient_penalty: 0.014311]\n",
            "[Epoch 0/200] [Batch 2145/3166] [D loss: 2.882343] [G loss: 8.664766] [gradient_penalty: 0.008787]\n",
            "[Epoch 0/200] [Batch 2146/3166] [D loss: 1.162267] [G loss: 8.550445] [gradient_penalty: 0.013868]\n",
            "[Epoch 0/200] [Batch 2147/3166] [D loss: 2.012647] [G loss: 8.891966] [gradient_penalty: 0.014580]\n",
            "[Epoch 0/200] [Batch 2148/3166] [D loss: 2.337592] [G loss: 9.092062] [gradient_penalty: 0.010849]\n",
            "[Epoch 0/200] [Batch 2149/3166] [D loss: 2.420341] [G loss: 9.075723] [gradient_penalty: 0.011261]\n",
            "[Epoch 0/200] [Batch 2150/3166] [D loss: 1.851196] [G loss: 8.430424] [gradient_penalty: 0.012761]\n",
            "[Epoch 0/200] [Batch 2151/3166] [D loss: 1.882458] [G loss: 8.744561] [gradient_penalty: 0.015070]\n",
            "[Epoch 0/200] [Batch 2152/3166] [D loss: 2.269938] [G loss: 9.023718] [gradient_penalty: 0.015865]\n",
            "[Epoch 0/200] [Batch 2153/3166] [D loss: 2.532944] [G loss: 8.540855] [gradient_penalty: 0.012090]\n",
            "[Epoch 0/200] [Batch 2154/3166] [D loss: 2.298509] [G loss: 8.779035] [gradient_penalty: 0.010764]\n",
            "[Epoch 0/200] [Batch 2155/3166] [D loss: 1.873909] [G loss: 9.319290] [gradient_penalty: 0.012947]\n",
            "[Epoch 0/200] [Batch 2156/3166] [D loss: 2.887622] [G loss: 8.649667] [gradient_penalty: 0.014890]\n",
            "[Epoch 0/200] [Batch 2157/3166] [D loss: 2.225229] [G loss: 8.799448] [gradient_penalty: 0.012387]\n",
            "[Epoch 0/200] [Batch 2158/3166] [D loss: 2.087936] [G loss: 8.991124] [gradient_penalty: 0.010126]\n",
            "[Epoch 0/200] [Batch 2159/3166] [D loss: 2.949078] [G loss: 8.897896] [gradient_penalty: 0.015457]\n",
            "[Epoch 0/200] [Batch 2160/3166] [D loss: 2.500008] [G loss: 8.222412] [gradient_penalty: 0.011691]\n",
            "[Epoch 0/200] [Batch 2161/3166] [D loss: 2.217170] [G loss: 8.305803] [gradient_penalty: 0.012841]\n",
            "[Epoch 0/200] [Batch 2162/3166] [D loss: 1.885168] [G loss: 8.762581] [gradient_penalty: 0.012577]\n",
            "[Epoch 0/200] [Batch 2163/3166] [D loss: 2.057597] [G loss: 8.726471] [gradient_penalty: 0.009568]\n",
            "[Epoch 0/200] [Batch 2164/3166] [D loss: 2.298939] [G loss: 8.666953] [gradient_penalty: 0.010025]\n",
            "[Epoch 0/200] [Batch 2165/3166] [D loss: 1.793743] [G loss: 8.432937] [gradient_penalty: 0.017301]\n",
            "[Epoch 0/200] [Batch 2166/3166] [D loss: 1.486125] [G loss: 8.100584] [gradient_penalty: 0.010573]\n",
            "[Epoch 0/200] [Batch 2167/3166] [D loss: 2.209879] [G loss: 8.807592] [gradient_penalty: 0.013608]\n",
            "[Epoch 0/200] [Batch 2168/3166] [D loss: 1.354186] [G loss: 8.923413] [gradient_penalty: 0.010951]\n",
            "[Epoch 0/200] [Batch 2169/3166] [D loss: 2.157421] [G loss: 8.626448] [gradient_penalty: 0.010303]\n",
            "[Epoch 0/200] [Batch 2170/3166] [D loss: 2.562947] [G loss: 8.542612] [gradient_penalty: 0.011897]\n",
            "[Epoch 0/200] [Batch 2171/3166] [D loss: 1.759974] [G loss: 9.281136] [gradient_penalty: 0.014021]\n",
            "[Epoch 0/200] [Batch 2172/3166] [D loss: 1.785035] [G loss: 8.998642] [gradient_penalty: 0.012501]\n",
            "[Epoch 0/200] [Batch 2173/3166] [D loss: 1.345101] [G loss: 9.555490] [gradient_penalty: 0.009554]\n",
            "[Epoch 0/200] [Batch 2174/3166] [D loss: 2.147944] [G loss: 8.969738] [gradient_penalty: 0.013279]\n",
            "[Epoch 0/200] [Batch 2175/3166] [D loss: 2.123195] [G loss: 8.842603] [gradient_penalty: 0.016093]\n",
            "[Epoch 0/200] [Batch 2176/3166] [D loss: 1.688989] [G loss: 8.189959] [gradient_penalty: 0.012866]\n",
            "[Epoch 0/200] [Batch 2177/3166] [D loss: 1.984695] [G loss: 9.247232] [gradient_penalty: 0.010097]\n",
            "[Epoch 0/200] [Batch 2178/3166] [D loss: 1.346610] [G loss: 8.675372] [gradient_penalty: 0.007121]\n",
            "[Epoch 0/200] [Batch 2179/3166] [D loss: 2.162313] [G loss: 8.769064] [gradient_penalty: 0.015924]\n",
            "[Epoch 0/200] [Batch 2180/3166] [D loss: 2.042504] [G loss: 8.910007] [gradient_penalty: 0.011373]\n",
            "[Epoch 0/200] [Batch 2181/3166] [D loss: 1.903557] [G loss: 8.863793] [gradient_penalty: 0.012758]\n",
            "[Epoch 0/200] [Batch 2182/3166] [D loss: 2.317822] [G loss: 8.790930] [gradient_penalty: 0.013801]\n",
            "[Epoch 0/200] [Batch 2183/3166] [D loss: 3.207662] [G loss: 8.453394] [gradient_penalty: 0.010980]\n",
            "[Epoch 0/200] [Batch 2184/3166] [D loss: 2.046584] [G loss: 8.753779] [gradient_penalty: 0.011767]\n",
            "[Epoch 0/200] [Batch 2185/3166] [D loss: 1.866621] [G loss: 8.741282] [gradient_penalty: 0.011355]\n",
            "[Epoch 0/200] [Batch 2186/3166] [D loss: 3.267126] [G loss: 8.646605] [gradient_penalty: 0.013377]\n",
            "[Epoch 0/200] [Batch 2187/3166] [D loss: 2.298259] [G loss: 8.668051] [gradient_penalty: 0.009445]\n",
            "[Epoch 0/200] [Batch 2188/3166] [D loss: 1.990101] [G loss: 8.872320] [gradient_penalty: 0.012129]\n",
            "[Epoch 0/200] [Batch 2189/3166] [D loss: 2.538869] [G loss: 8.205150] [gradient_penalty: 0.017079]\n",
            "[Epoch 0/200] [Batch 2190/3166] [D loss: 1.854434] [G loss: 8.611294] [gradient_penalty: 0.014140]\n",
            "[Epoch 0/200] [Batch 2191/3166] [D loss: 2.019046] [G loss: 8.743126] [gradient_penalty: 0.016874]\n",
            "[Epoch 0/200] [Batch 2192/3166] [D loss: 1.746732] [G loss: 8.283701] [gradient_penalty: 0.008537]\n",
            "[Epoch 0/200] [Batch 2193/3166] [D loss: 1.688015] [G loss: 8.639253] [gradient_penalty: 0.013126]\n",
            "[Epoch 0/200] [Batch 2194/3166] [D loss: 2.256740] [G loss: 8.437449] [gradient_penalty: 0.012290]\n",
            "[Epoch 0/200] [Batch 2195/3166] [D loss: 1.877735] [G loss: 8.081473] [gradient_penalty: 0.010992]\n",
            "[Epoch 0/200] [Batch 2196/3166] [D loss: 2.557402] [G loss: 9.037632] [gradient_penalty: 0.013214]\n",
            "[Epoch 0/200] [Batch 2197/3166] [D loss: 2.189721] [G loss: 7.870070] [gradient_penalty: 0.008575]\n",
            "[Epoch 0/200] [Batch 2198/3166] [D loss: 2.444869] [G loss: 8.126121] [gradient_penalty: 0.014685]\n",
            "[Epoch 0/200] [Batch 2199/3166] [D loss: 2.432539] [G loss: 8.820623] [gradient_penalty: 0.011430]\n",
            "[Epoch 0/200] [Batch 2200/3166] [D loss: 1.852010] [G loss: 8.486036] [gradient_penalty: 0.012645]\n",
            "log\n",
            "[Epoch 0/200] [Batch 2201/3166] [D loss: 1.479937] [G loss: 7.888839] [gradient_penalty: 0.010715]\n",
            "[Epoch 0/200] [Batch 2202/3166] [D loss: 2.945852] [G loss: 8.713665] [gradient_penalty: 0.010912]\n",
            "[Epoch 0/200] [Batch 2203/3166] [D loss: 2.310269] [G loss: 8.825670] [gradient_penalty: 0.011266]\n",
            "[Epoch 0/200] [Batch 2204/3166] [D loss: 1.626133] [G loss: 8.118318] [gradient_penalty: 0.018329]\n",
            "[Epoch 0/200] [Batch 2205/3166] [D loss: 3.008960] [G loss: 8.789253] [gradient_penalty: 0.018259]\n",
            "[Epoch 0/200] [Batch 2206/3166] [D loss: 2.019747] [G loss: 8.792511] [gradient_penalty: 0.017519]\n",
            "[Epoch 0/200] [Batch 2207/3166] [D loss: 1.535889] [G loss: 8.344657] [gradient_penalty: 0.012226]\n",
            "[Epoch 0/200] [Batch 2208/3166] [D loss: 2.478644] [G loss: 8.757293] [gradient_penalty: 0.011298]\n",
            "[Epoch 0/200] [Batch 2209/3166] [D loss: 2.072743] [G loss: 8.923378] [gradient_penalty: 0.012570]\n",
            "[Epoch 0/200] [Batch 2210/3166] [D loss: 1.422132] [G loss: 8.732699] [gradient_penalty: 0.012647]\n",
            "[Epoch 0/200] [Batch 2211/3166] [D loss: 2.183022] [G loss: 8.573627] [gradient_penalty: 0.012607]\n",
            "[Epoch 0/200] [Batch 2212/3166] [D loss: 1.900739] [G loss: 9.106791] [gradient_penalty: 0.011035]\n",
            "[Epoch 0/200] [Batch 2213/3166] [D loss: 1.832050] [G loss: 8.861133] [gradient_penalty: 0.012288]\n",
            "[Epoch 0/200] [Batch 2214/3166] [D loss: 1.240815] [G loss: 8.865436] [gradient_penalty: 0.012031]\n",
            "[Epoch 0/200] [Batch 2215/3166] [D loss: 1.219131] [G loss: 8.917933] [gradient_penalty: 0.012406]\n",
            "[Epoch 0/200] [Batch 2216/3166] [D loss: 2.576893] [G loss: 8.999970] [gradient_penalty: 0.012834]\n",
            "[Epoch 0/200] [Batch 2217/3166] [D loss: 2.178377] [G loss: 8.392065] [gradient_penalty: 0.010936]\n",
            "[Epoch 0/200] [Batch 2218/3166] [D loss: 2.107335] [G loss: 8.771070] [gradient_penalty: 0.011186]\n",
            "[Epoch 0/200] [Batch 2219/3166] [D loss: 1.903774] [G loss: 8.495182] [gradient_penalty: 0.009744]\n",
            "[Epoch 0/200] [Batch 2220/3166] [D loss: 1.731621] [G loss: 9.461104] [gradient_penalty: 0.013994]\n",
            "[Epoch 0/200] [Batch 2221/3166] [D loss: 1.930573] [G loss: 8.284908] [gradient_penalty: 0.013004]\n",
            "[Epoch 0/200] [Batch 2222/3166] [D loss: 1.792636] [G loss: 8.749752] [gradient_penalty: 0.009787]\n",
            "[Epoch 0/200] [Batch 2223/3166] [D loss: 1.848757] [G loss: 8.846508] [gradient_penalty: 0.010797]\n",
            "[Epoch 0/200] [Batch 2224/3166] [D loss: 2.896734] [G loss: 8.891880] [gradient_penalty: 0.014591]\n",
            "[Epoch 0/200] [Batch 2225/3166] [D loss: 2.767059] [G loss: 9.212373] [gradient_penalty: 0.015081]\n",
            "[Epoch 0/200] [Batch 2226/3166] [D loss: 1.517105] [G loss: 9.021136] [gradient_penalty: 0.010280]\n",
            "[Epoch 0/200] [Batch 2227/3166] [D loss: 2.013260] [G loss: 9.306732] [gradient_penalty: 0.010550]\n",
            "[Epoch 0/200] [Batch 2228/3166] [D loss: 1.948779] [G loss: 8.505913] [gradient_penalty: 0.010401]\n",
            "[Epoch 0/200] [Batch 2229/3166] [D loss: 2.235923] [G loss: 8.970222] [gradient_penalty: 0.009282]\n",
            "[Epoch 0/200] [Batch 2230/3166] [D loss: 2.034783] [G loss: 8.661839] [gradient_penalty: 0.011173]\n",
            "[Epoch 0/200] [Batch 2231/3166] [D loss: 1.831628] [G loss: 9.131251] [gradient_penalty: 0.005872]\n",
            "[Epoch 0/200] [Batch 2232/3166] [D loss: 1.353957] [G loss: 8.050392] [gradient_penalty: 0.011083]\n",
            "[Epoch 0/200] [Batch 2233/3166] [D loss: 2.521786] [G loss: 8.623550] [gradient_penalty: 0.008589]\n",
            "[Epoch 0/200] [Batch 2234/3166] [D loss: 2.022479] [G loss: 9.044773] [gradient_penalty: 0.014011]\n",
            "[Epoch 0/200] [Batch 2235/3166] [D loss: 2.144905] [G loss: 8.933611] [gradient_penalty: 0.011245]\n",
            "[Epoch 0/200] [Batch 2236/3166] [D loss: 1.876587] [G loss: 9.390593] [gradient_penalty: 0.008854]\n",
            "[Epoch 0/200] [Batch 2237/3166] [D loss: 2.091542] [G loss: 8.991331] [gradient_penalty: 0.011100]\n",
            "[Epoch 0/200] [Batch 2238/3166] [D loss: 1.969104] [G loss: 8.202332] [gradient_penalty: 0.009312]\n",
            "[Epoch 0/200] [Batch 2239/3166] [D loss: 2.664921] [G loss: 9.160022] [gradient_penalty: 0.008356]\n",
            "[Epoch 0/200] [Batch 2240/3166] [D loss: 2.479433] [G loss: 8.598993] [gradient_penalty: 0.010061]\n",
            "[Epoch 0/200] [Batch 2241/3166] [D loss: 2.333305] [G loss: 8.395704] [gradient_penalty: 0.009427]\n",
            "[Epoch 0/200] [Batch 2242/3166] [D loss: 2.436810] [G loss: 9.324211] [gradient_penalty: 0.009608]\n",
            "[Epoch 0/200] [Batch 2243/3166] [D loss: 1.961966] [G loss: 8.656950] [gradient_penalty: 0.012246]\n",
            "[Epoch 0/200] [Batch 2244/3166] [D loss: 2.004595] [G loss: 8.781081] [gradient_penalty: 0.009574]\n",
            "[Epoch 0/200] [Batch 2245/3166] [D loss: 2.237998] [G loss: 8.883178] [gradient_penalty: 0.017257]\n",
            "[Epoch 0/200] [Batch 2246/3166] [D loss: 1.525441] [G loss: 9.121342] [gradient_penalty: 0.009635]\n",
            "[Epoch 0/200] [Batch 2247/3166] [D loss: 2.307115] [G loss: 8.609406] [gradient_penalty: 0.010389]\n",
            "[Epoch 0/200] [Batch 2248/3166] [D loss: 1.999095] [G loss: 8.463708] [gradient_penalty: 0.009744]\n",
            "[Epoch 0/200] [Batch 2249/3166] [D loss: 2.000287] [G loss: 8.757689] [gradient_penalty: 0.014392]\n",
            "[Epoch 0/200] [Batch 2250/3166] [D loss: 2.223689] [G loss: 8.981108] [gradient_penalty: 0.012640]\n",
            "[Epoch 0/200] [Batch 2251/3166] [D loss: 2.138777] [G loss: 9.044502] [gradient_penalty: 0.007297]\n",
            "[Epoch 0/200] [Batch 2252/3166] [D loss: 2.139787] [G loss: 8.255776] [gradient_penalty: 0.010029]\n",
            "[Epoch 0/200] [Batch 2253/3166] [D loss: 1.472910] [G loss: 8.124479] [gradient_penalty: 0.008432]\n",
            "[Epoch 0/200] [Batch 2254/3166] [D loss: 2.266134] [G loss: 8.353081] [gradient_penalty: 0.011140]\n",
            "[Epoch 0/200] [Batch 2255/3166] [D loss: 1.682727] [G loss: 8.180413] [gradient_penalty: 0.009358]\n",
            "[Epoch 0/200] [Batch 2256/3166] [D loss: 1.923643] [G loss: 9.203367] [gradient_penalty: 0.013804]\n",
            "[Epoch 0/200] [Batch 2257/3166] [D loss: 2.062789] [G loss: 8.782331] [gradient_penalty: 0.011244]\n",
            "[Epoch 0/200] [Batch 2258/3166] [D loss: 1.751496] [G loss: 8.248482] [gradient_penalty: 0.013625]\n",
            "[Epoch 0/200] [Batch 2259/3166] [D loss: 1.882250] [G loss: 8.628685] [gradient_penalty: 0.009636]\n",
            "[Epoch 0/200] [Batch 2260/3166] [D loss: 2.106853] [G loss: 8.404593] [gradient_penalty: 0.010850]\n",
            "[Epoch 0/200] [Batch 2261/3166] [D loss: 2.332570] [G loss: 8.469288] [gradient_penalty: 0.010710]\n",
            "[Epoch 0/200] [Batch 2262/3166] [D loss: 1.868066] [G loss: 8.534053] [gradient_penalty: 0.010314]\n",
            "[Epoch 0/200] [Batch 2263/3166] [D loss: 1.608190] [G loss: 8.389294] [gradient_penalty: 0.013196]\n",
            "[Epoch 0/200] [Batch 2264/3166] [D loss: 2.470640] [G loss: 8.640902] [gradient_penalty: 0.011492]\n",
            "[Epoch 0/200] [Batch 2265/3166] [D loss: 2.017299] [G loss: 8.668693] [gradient_penalty: 0.009629]\n",
            "[Epoch 0/200] [Batch 2266/3166] [D loss: 1.855214] [G loss: 8.408802] [gradient_penalty: 0.010004]\n",
            "[Epoch 0/200] [Batch 2267/3166] [D loss: 2.195002] [G loss: 8.401001] [gradient_penalty: 0.011128]\n",
            "[Epoch 0/200] [Batch 2268/3166] [D loss: 1.943120] [G loss: 8.685352] [gradient_penalty: 0.010175]\n",
            "[Epoch 0/200] [Batch 2269/3166] [D loss: 1.777240] [G loss: 8.828947] [gradient_penalty: 0.009992]\n",
            "[Epoch 0/200] [Batch 2270/3166] [D loss: 2.028607] [G loss: 8.269216] [gradient_penalty: 0.010149]\n",
            "[Epoch 0/200] [Batch 2271/3166] [D loss: 0.993549] [G loss: 8.521873] [gradient_penalty: 0.007551]\n",
            "[Epoch 0/200] [Batch 2272/3166] [D loss: 1.990008] [G loss: 8.311799] [gradient_penalty: 0.012364]\n",
            "[Epoch 0/200] [Batch 2273/3166] [D loss: 1.638409] [G loss: 8.979095] [gradient_penalty: 0.010343]\n",
            "[Epoch 0/200] [Batch 2274/3166] [D loss: 1.656109] [G loss: 8.566362] [gradient_penalty: 0.009704]\n",
            "[Epoch 0/200] [Batch 2275/3166] [D loss: 1.435282] [G loss: 8.099607] [gradient_penalty: 0.008113]\n",
            "[Epoch 0/200] [Batch 2276/3166] [D loss: 2.207279] [G loss: 8.451267] [gradient_penalty: 0.012320]\n",
            "[Epoch 0/200] [Batch 2277/3166] [D loss: 1.948774] [G loss: 8.091461] [gradient_penalty: 0.013276]\n",
            "[Epoch 0/200] [Batch 2278/3166] [D loss: 1.879046] [G loss: 8.121519] [gradient_penalty: 0.010138]\n",
            "[Epoch 0/200] [Batch 2279/3166] [D loss: 2.182600] [G loss: 8.699210] [gradient_penalty: 0.011157]\n",
            "[Epoch 0/200] [Batch 2280/3166] [D loss: 2.467496] [G loss: 8.289972] [gradient_penalty: 0.012535]\n",
            "[Epoch 0/200] [Batch 2281/3166] [D loss: 2.030828] [G loss: 8.849529] [gradient_penalty: 0.010231]\n",
            "[Epoch 0/200] [Batch 2282/3166] [D loss: 1.839475] [G loss: 8.210932] [gradient_penalty: 0.008282]\n",
            "[Epoch 0/200] [Batch 2283/3166] [D loss: 2.382099] [G loss: 8.754232] [gradient_penalty: 0.013403]\n",
            "[Epoch 0/200] [Batch 2284/3166] [D loss: 1.562013] [G loss: 8.595263] [gradient_penalty: 0.010491]\n",
            "[Epoch 0/200] [Batch 2285/3166] [D loss: 2.543048] [G loss: 8.965927] [gradient_penalty: 0.012087]\n",
            "[Epoch 0/200] [Batch 2286/3166] [D loss: 1.623856] [G loss: 8.682621] [gradient_penalty: 0.009581]\n",
            "[Epoch 0/200] [Batch 2287/3166] [D loss: 2.295132] [G loss: 8.706492] [gradient_penalty: 0.011794]\n",
            "[Epoch 0/200] [Batch 2288/3166] [D loss: 2.378381] [G loss: 8.697289] [gradient_penalty: 0.013044]\n",
            "[Epoch 0/200] [Batch 2289/3166] [D loss: 2.190986] [G loss: 8.239206] [gradient_penalty: 0.008748]\n",
            "[Epoch 0/200] [Batch 2290/3166] [D loss: 2.021623] [G loss: 9.179245] [gradient_penalty: 0.010938]\n",
            "[Epoch 0/200] [Batch 2291/3166] [D loss: 2.801059] [G loss: 9.274622] [gradient_penalty: 0.010935]\n",
            "[Epoch 0/200] [Batch 2292/3166] [D loss: 1.848938] [G loss: 8.936662] [gradient_penalty: 0.013803]\n",
            "[Epoch 0/200] [Batch 2293/3166] [D loss: 2.486860] [G loss: 9.177290] [gradient_penalty: 0.013562]\n",
            "[Epoch 0/200] [Batch 2294/3166] [D loss: 1.913277] [G loss: 8.470529] [gradient_penalty: 0.010889]\n",
            "[Epoch 0/200] [Batch 2295/3166] [D loss: 2.145053] [G loss: 8.591001] [gradient_penalty: 0.011432]\n",
            "[Epoch 0/200] [Batch 2296/3166] [D loss: 2.393794] [G loss: 8.305832] [gradient_penalty: 0.012824]\n",
            "[Epoch 0/200] [Batch 2297/3166] [D loss: 1.491197] [G loss: 8.294443] [gradient_penalty: 0.010696]\n",
            "[Epoch 0/200] [Batch 2298/3166] [D loss: 1.516862] [G loss: 8.716890] [gradient_penalty: 0.011265]\n",
            "[Epoch 0/200] [Batch 2299/3166] [D loss: 1.560916] [G loss: 8.974954] [gradient_penalty: 0.009394]\n",
            "[Epoch 0/200] [Batch 2300/3166] [D loss: 1.931893] [G loss: 8.639812] [gradient_penalty: 0.009455]\n",
            "log\n",
            "[Epoch 0/200] [Batch 2301/3166] [D loss: 1.919581] [G loss: 9.311242] [gradient_penalty: 0.011045]\n",
            "[Epoch 0/200] [Batch 2302/3166] [D loss: 2.373501] [G loss: 8.315146] [gradient_penalty: 0.010517]\n",
            "[Epoch 0/200] [Batch 2303/3166] [D loss: 1.817731] [G loss: 8.696016] [gradient_penalty: 0.012022]\n",
            "[Epoch 0/200] [Batch 2304/3166] [D loss: 1.779149] [G loss: 9.004910] [gradient_penalty: 0.010456]\n",
            "[Epoch 0/200] [Batch 2305/3166] [D loss: 1.914692] [G loss: 8.503778] [gradient_penalty: 0.009136]\n",
            "[Epoch 0/200] [Batch 2306/3166] [D loss: 2.512441] [G loss: 8.962110] [gradient_penalty: 0.013483]\n",
            "[Epoch 0/200] [Batch 2307/3166] [D loss: 1.372468] [G loss: 8.759430] [gradient_penalty: 0.009492]\n",
            "[Epoch 0/200] [Batch 2308/3166] [D loss: 2.127218] [G loss: 8.872017] [gradient_penalty: 0.009819]\n",
            "[Epoch 0/200] [Batch 2309/3166] [D loss: 2.194530] [G loss: 8.648258] [gradient_penalty: 0.013130]\n",
            "[Epoch 0/200] [Batch 2310/3166] [D loss: 1.446644] [G loss: 8.633196] [gradient_penalty: 0.008611]\n",
            "[Epoch 0/200] [Batch 2311/3166] [D loss: 1.832066] [G loss: 8.801050] [gradient_penalty: 0.009779]\n",
            "[Epoch 0/200] [Batch 2312/3166] [D loss: 2.737797] [G loss: 8.653079] [gradient_penalty: 0.013418]\n",
            "[Epoch 0/200] [Batch 2313/3166] [D loss: 1.498780] [G loss: 8.998178] [gradient_penalty: 0.009521]\n",
            "[Epoch 0/200] [Batch 2314/3166] [D loss: 1.273190] [G loss: 9.130704] [gradient_penalty: 0.011485]\n",
            "[Epoch 0/200] [Batch 2315/3166] [D loss: 2.785048] [G loss: 8.409790] [gradient_penalty: 0.010325]\n",
            "[Epoch 0/200] [Batch 2316/3166] [D loss: 1.633008] [G loss: 8.721962] [gradient_penalty: 0.012238]\n",
            "[Epoch 0/200] [Batch 2317/3166] [D loss: 1.829585] [G loss: 8.703983] [gradient_penalty: 0.011584]\n",
            "[Epoch 0/200] [Batch 2318/3166] [D loss: 1.952874] [G loss: 8.823603] [gradient_penalty: 0.009336]\n",
            "[Epoch 0/200] [Batch 2319/3166] [D loss: 2.199837] [G loss: 9.065717] [gradient_penalty: 0.013538]\n",
            "[Epoch 0/200] [Batch 2320/3166] [D loss: 1.209910] [G loss: 9.122742] [gradient_penalty: 0.009664]\n",
            "[Epoch 0/200] [Batch 2321/3166] [D loss: 1.315594] [G loss: 8.567667] [gradient_penalty: 0.008331]\n",
            "[Epoch 0/200] [Batch 2322/3166] [D loss: 1.755195] [G loss: 8.987554] [gradient_penalty: 0.010590]\n",
            "[Epoch 0/200] [Batch 2323/3166] [D loss: 1.702729] [G loss: 8.876719] [gradient_penalty: 0.015005]\n",
            "[Epoch 0/200] [Batch 2324/3166] [D loss: 1.917733] [G loss: 8.768038] [gradient_penalty: 0.012639]\n",
            "[Epoch 0/200] [Batch 2325/3166] [D loss: 2.170162] [G loss: 8.908465] [gradient_penalty: 0.013517]\n",
            "[Epoch 0/200] [Batch 2326/3166] [D loss: 1.730547] [G loss: 9.033992] [gradient_penalty: 0.012861]\n",
            "[Epoch 0/200] [Batch 2327/3166] [D loss: 2.717310] [G loss: 9.057411] [gradient_penalty: 0.013949]\n",
            "[Epoch 0/200] [Batch 2328/3166] [D loss: 2.021244] [G loss: 9.006783] [gradient_penalty: 0.010436]\n",
            "[Epoch 0/200] [Batch 2329/3166] [D loss: 2.709811] [G loss: 8.744802] [gradient_penalty: 0.010267]\n",
            "[Epoch 0/200] [Batch 2330/3166] [D loss: 2.561564] [G loss: 8.576406] [gradient_penalty: 0.010236]\n",
            "[Epoch 0/200] [Batch 2331/3166] [D loss: 1.892801] [G loss: 8.949960] [gradient_penalty: 0.009765]\n",
            "[Epoch 0/200] [Batch 2332/3166] [D loss: 1.565069] [G loss: 8.848078] [gradient_penalty: 0.010460]\n",
            "[Epoch 0/200] [Batch 2333/3166] [D loss: 1.386940] [G loss: 8.981682] [gradient_penalty: 0.012895]\n",
            "[Epoch 0/200] [Batch 2334/3166] [D loss: 2.008629] [G loss: 9.251510] [gradient_penalty: 0.008098]\n",
            "[Epoch 0/200] [Batch 2335/3166] [D loss: 2.352226] [G loss: 8.700245] [gradient_penalty: 0.012401]\n",
            "[Epoch 0/200] [Batch 2336/3166] [D loss: 1.987381] [G loss: 9.111910] [gradient_penalty: 0.010407]\n",
            "[Epoch 0/200] [Batch 2337/3166] [D loss: 1.811162] [G loss: 9.499116] [gradient_penalty: 0.012447]\n",
            "[Epoch 0/200] [Batch 2338/3166] [D loss: 2.311736] [G loss: 8.767929] [gradient_penalty: 0.013187]\n",
            "[Epoch 0/200] [Batch 2339/3166] [D loss: 1.668428] [G loss: 8.799238] [gradient_penalty: 0.010771]\n",
            "[Epoch 0/200] [Batch 2340/3166] [D loss: 1.837815] [G loss: 8.925544] [gradient_penalty: 0.012657]\n",
            "[Epoch 0/200] [Batch 2341/3166] [D loss: 2.239609] [G loss: 8.492162] [gradient_penalty: 0.010781]\n",
            "[Epoch 0/200] [Batch 2342/3166] [D loss: 2.254751] [G loss: 8.034125] [gradient_penalty: 0.010252]\n",
            "[Epoch 0/200] [Batch 2343/3166] [D loss: 1.916299] [G loss: 8.616177] [gradient_penalty: 0.014183]\n",
            "[Epoch 0/200] [Batch 2344/3166] [D loss: 1.777148] [G loss: 8.625026] [gradient_penalty: 0.010818]\n",
            "[Epoch 0/200] [Batch 2345/3166] [D loss: 1.475509] [G loss: 8.326440] [gradient_penalty: 0.007994]\n",
            "[Epoch 0/200] [Batch 2346/3166] [D loss: 1.877609] [G loss: 8.591396] [gradient_penalty: 0.009971]\n",
            "[Epoch 0/200] [Batch 2347/3166] [D loss: 1.197163] [G loss: 9.089367] [gradient_penalty: 0.008812]\n",
            "[Epoch 0/200] [Batch 2348/3166] [D loss: 2.540102] [G loss: 8.644863] [gradient_penalty: 0.011194]\n",
            "[Epoch 0/200] [Batch 2349/3166] [D loss: 2.176245] [G loss: 8.764811] [gradient_penalty: 0.013142]\n",
            "[Epoch 0/200] [Batch 2350/3166] [D loss: 1.676665] [G loss: 8.752621] [gradient_penalty: 0.014030]\n",
            "[Epoch 0/200] [Batch 2351/3166] [D loss: 2.374470] [G loss: 9.133872] [gradient_penalty: 0.009294]\n",
            "[Epoch 0/200] [Batch 2352/3166] [D loss: 2.594143] [G loss: 9.032845] [gradient_penalty: 0.011248]\n",
            "[Epoch 0/200] [Batch 2353/3166] [D loss: 1.805168] [G loss: 8.618555] [gradient_penalty: 0.009995]\n",
            "[Epoch 0/200] [Batch 2354/3166] [D loss: 2.551446] [G loss: 8.130394] [gradient_penalty: 0.010749]\n",
            "[Epoch 0/200] [Batch 2355/3166] [D loss: 0.918221] [G loss: 8.680023] [gradient_penalty: 0.010678]\n",
            "[Epoch 0/200] [Batch 2356/3166] [D loss: 1.936100] [G loss: 8.602100] [gradient_penalty: 0.010832]\n",
            "[Epoch 0/200] [Batch 2357/3166] [D loss: 1.925959] [G loss: 8.775025] [gradient_penalty: 0.010543]\n",
            "[Epoch 0/200] [Batch 2358/3166] [D loss: 2.030714] [G loss: 8.551539] [gradient_penalty: 0.012153]\n",
            "[Epoch 0/200] [Batch 2359/3166] [D loss: 1.337433] [G loss: 8.801472] [gradient_penalty: 0.008607]\n",
            "[Epoch 0/200] [Batch 2360/3166] [D loss: 1.923731] [G loss: 9.187677] [gradient_penalty: 0.012134]\n",
            "[Epoch 0/200] [Batch 2361/3166] [D loss: 1.126085] [G loss: 8.431453] [gradient_penalty: 0.012278]\n",
            "[Epoch 0/200] [Batch 2362/3166] [D loss: 1.953885] [G loss: 8.945263] [gradient_penalty: 0.009964]\n",
            "[Epoch 0/200] [Batch 2363/3166] [D loss: 2.030906] [G loss: 8.467260] [gradient_penalty: 0.009677]\n",
            "[Epoch 0/200] [Batch 2364/3166] [D loss: 2.179803] [G loss: 8.982986] [gradient_penalty: 0.011576]\n",
            "[Epoch 0/200] [Batch 2365/3166] [D loss: 3.216034] [G loss: 7.941688] [gradient_penalty: 0.010297]\n",
            "[Epoch 0/200] [Batch 2366/3166] [D loss: 1.727783] [G loss: 8.678404] [gradient_penalty: 0.015633]\n",
            "[Epoch 0/200] [Batch 2367/3166] [D loss: 1.886702] [G loss: 8.536067] [gradient_penalty: 0.010052]\n",
            "[Epoch 0/200] [Batch 2368/3166] [D loss: 1.855794] [G loss: 8.412645] [gradient_penalty: 0.009493]\n",
            "[Epoch 0/200] [Batch 2369/3166] [D loss: 1.831471] [G loss: 8.709776] [gradient_penalty: 0.013291]\n",
            "[Epoch 0/200] [Batch 2370/3166] [D loss: 1.863117] [G loss: 8.519356] [gradient_penalty: 0.009472]\n",
            "[Epoch 0/200] [Batch 2371/3166] [D loss: 1.306041] [G loss: 8.344336] [gradient_penalty: 0.009498]\n",
            "[Epoch 0/200] [Batch 2372/3166] [D loss: 2.432044] [G loss: 8.787487] [gradient_penalty: 0.009758]\n",
            "[Epoch 0/200] [Batch 2373/3166] [D loss: 2.665503] [G loss: 8.758722] [gradient_penalty: 0.008963]\n",
            "[Epoch 0/200] [Batch 2374/3166] [D loss: 2.157736] [G loss: 8.697569] [gradient_penalty: 0.012950]\n",
            "[Epoch 0/200] [Batch 2375/3166] [D loss: 2.082091] [G loss: 8.963366] [gradient_penalty: 0.011503]\n",
            "[Epoch 0/200] [Batch 2376/3166] [D loss: 1.949430] [G loss: 8.690770] [gradient_penalty: 0.009677]\n",
            "[Epoch 0/200] [Batch 2377/3166] [D loss: 1.807167] [G loss: 8.455521] [gradient_penalty: 0.015071]\n",
            "[Epoch 0/200] [Batch 2378/3166] [D loss: 2.408370] [G loss: 8.458415] [gradient_penalty: 0.009511]\n",
            "[Epoch 0/200] [Batch 2379/3166] [D loss: 2.220217] [G loss: 8.889877] [gradient_penalty: 0.011268]\n",
            "[Epoch 0/200] [Batch 2380/3166] [D loss: 1.639264] [G loss: 8.836136] [gradient_penalty: 0.009891]\n",
            "[Epoch 0/200] [Batch 2381/3166] [D loss: 2.215426] [G loss: 8.376163] [gradient_penalty: 0.009053]\n",
            "[Epoch 0/200] [Batch 2382/3166] [D loss: 1.795981] [G loss: 8.683649] [gradient_penalty: 0.012095]\n",
            "[Epoch 0/200] [Batch 2383/3166] [D loss: 2.232443] [G loss: 8.051061] [gradient_penalty: 0.010904]\n",
            "[Epoch 0/200] [Batch 2384/3166] [D loss: 1.280774] [G loss: 8.141741] [gradient_penalty: 0.008066]\n",
            "[Epoch 0/200] [Batch 2385/3166] [D loss: 1.769503] [G loss: 8.506165] [gradient_penalty: 0.012980]\n",
            "[Epoch 0/200] [Batch 2386/3166] [D loss: 1.976065] [G loss: 8.620840] [gradient_penalty: 0.009310]\n",
            "[Epoch 0/200] [Batch 2387/3166] [D loss: 1.247270] [G loss: 8.602940] [gradient_penalty: 0.009864]\n",
            "[Epoch 0/200] [Batch 2388/3166] [D loss: 2.432270] [G loss: 8.848125] [gradient_penalty: 0.015022]\n",
            "[Epoch 0/200] [Batch 2389/3166] [D loss: 1.751305] [G loss: 9.056083] [gradient_penalty: 0.013139]\n",
            "[Epoch 0/200] [Batch 2390/3166] [D loss: 1.951967] [G loss: 8.424116] [gradient_penalty: 0.011282]\n",
            "[Epoch 0/200] [Batch 2391/3166] [D loss: 1.528218] [G loss: 8.958205] [gradient_penalty: 0.011923]\n",
            "[Epoch 0/200] [Batch 2392/3166] [D loss: 1.880761] [G loss: 8.608258] [gradient_penalty: 0.014906]\n",
            "[Epoch 0/200] [Batch 2393/3166] [D loss: 1.622934] [G loss: 8.867895] [gradient_penalty: 0.011182]\n",
            "[Epoch 0/200] [Batch 2394/3166] [D loss: 1.779976] [G loss: 8.311774] [gradient_penalty: 0.010826]\n",
            "[Epoch 0/200] [Batch 2395/3166] [D loss: 1.255502] [G loss: 8.643915] [gradient_penalty: 0.009104]\n",
            "[Epoch 0/200] [Batch 2396/3166] [D loss: 1.860860] [G loss: 8.653647] [gradient_penalty: 0.011666]\n",
            "[Epoch 0/200] [Batch 2397/3166] [D loss: 1.961714] [G loss: 8.348331] [gradient_penalty: 0.011628]\n",
            "[Epoch 0/200] [Batch 2398/3166] [D loss: 1.754482] [G loss: 8.360094] [gradient_penalty: 0.008724]\n",
            "[Epoch 0/200] [Batch 2399/3166] [D loss: 2.207652] [G loss: 8.719448] [gradient_penalty: 0.009954]\n",
            "[Epoch 0/200] [Batch 2400/3166] [D loss: 1.236457] [G loss: 8.971514] [gradient_penalty: 0.010936]\n",
            "log\n",
            "[Epoch 0/200] [Batch 2401/3166] [D loss: 1.626036] [G loss: 8.484438] [gradient_penalty: 0.007660]\n",
            "[Epoch 0/200] [Batch 2402/3166] [D loss: 1.994129] [G loss: 7.970685] [gradient_penalty: 0.013551]\n",
            "[Epoch 0/200] [Batch 2403/3166] [D loss: 2.010655] [G loss: 8.878168] [gradient_penalty: 0.013074]\n",
            "[Epoch 0/200] [Batch 2404/3166] [D loss: 1.638031] [G loss: 8.430250] [gradient_penalty: 0.009236]\n",
            "[Epoch 0/200] [Batch 2405/3166] [D loss: 2.219652] [G loss: 8.511875] [gradient_penalty: 0.014984]\n",
            "[Epoch 0/200] [Batch 2406/3166] [D loss: 1.778357] [G loss: 8.713646] [gradient_penalty: 0.007752]\n",
            "[Epoch 0/200] [Batch 2407/3166] [D loss: 1.644572] [G loss: 8.749736] [gradient_penalty: 0.008250]\n",
            "[Epoch 0/200] [Batch 2408/3166] [D loss: 2.303277] [G loss: 8.384410] [gradient_penalty: 0.009169]\n",
            "[Epoch 0/200] [Batch 2409/3166] [D loss: 1.819244] [G loss: 8.836363] [gradient_penalty: 0.014469]\n",
            "[Epoch 0/200] [Batch 2410/3166] [D loss: 2.266689] [G loss: 8.961040] [gradient_penalty: 0.013337]\n",
            "[Epoch 0/200] [Batch 2411/3166] [D loss: 0.824132] [G loss: 8.750805] [gradient_penalty: 0.006652]\n",
            "[Epoch 0/200] [Batch 2412/3166] [D loss: 2.442810] [G loss: 9.259863] [gradient_penalty: 0.007776]\n",
            "[Epoch 0/200] [Batch 2413/3166] [D loss: 1.261091] [G loss: 8.922837] [gradient_penalty: 0.012058]\n",
            "[Epoch 0/200] [Batch 2414/3166] [D loss: 2.594573] [G loss: 8.802467] [gradient_penalty: 0.011389]\n",
            "[Epoch 0/200] [Batch 2415/3166] [D loss: 2.709370] [G loss: 8.757343] [gradient_penalty: 0.011823]\n",
            "[Epoch 0/200] [Batch 2416/3166] [D loss: 1.666737] [G loss: 8.525539] [gradient_penalty: 0.008227]\n",
            "[Epoch 0/200] [Batch 2417/3166] [D loss: 1.374832] [G loss: 8.805355] [gradient_penalty: 0.011166]\n",
            "[Epoch 0/200] [Batch 2418/3166] [D loss: 1.448958] [G loss: 9.288759] [gradient_penalty: 0.008092]\n",
            "[Epoch 0/200] [Batch 2419/3166] [D loss: 2.203984] [G loss: 8.855158] [gradient_penalty: 0.012455]\n",
            "[Epoch 0/200] [Batch 2420/3166] [D loss: 2.341348] [G loss: 8.867246] [gradient_penalty: 0.011024]\n",
            "[Epoch 0/200] [Batch 2421/3166] [D loss: 1.775169] [G loss: 8.345856] [gradient_penalty: 0.011996]\n",
            "[Epoch 0/200] [Batch 2422/3166] [D loss: 2.345550] [G loss: 9.054651] [gradient_penalty: 0.011032]\n",
            "[Epoch 0/200] [Batch 2423/3166] [D loss: 1.547031] [G loss: 8.755783] [gradient_penalty: 0.009176]\n",
            "[Epoch 0/200] [Batch 2424/3166] [D loss: 2.485495] [G loss: 9.074347] [gradient_penalty: 0.013179]\n",
            "[Epoch 0/200] [Batch 2425/3166] [D loss: 1.186989] [G loss: 9.388727] [gradient_penalty: 0.011384]\n",
            "[Epoch 0/200] [Batch 2426/3166] [D loss: 1.841888] [G loss: 8.976179] [gradient_penalty: 0.013131]\n",
            "[Epoch 0/200] [Batch 2427/3166] [D loss: 2.223948] [G loss: 8.772953] [gradient_penalty: 0.007630]\n",
            "[Epoch 0/200] [Batch 2428/3166] [D loss: 2.081066] [G loss: 8.209661] [gradient_penalty: 0.009162]\n",
            "[Epoch 0/200] [Batch 2429/3166] [D loss: 1.744436] [G loss: 8.314981] [gradient_penalty: 0.012305]\n",
            "[Epoch 0/200] [Batch 2430/3166] [D loss: 2.524424] [G loss: 8.568203] [gradient_penalty: 0.009827]\n",
            "[Epoch 0/200] [Batch 2431/3166] [D loss: 2.215417] [G loss: 8.800200] [gradient_penalty: 0.013183]\n",
            "[Epoch 0/200] [Batch 2432/3166] [D loss: 1.695626] [G loss: 8.641058] [gradient_penalty: 0.010064]\n",
            "[Epoch 0/200] [Batch 2433/3166] [D loss: 1.838794] [G loss: 8.947313] [gradient_penalty: 0.007002]\n",
            "[Epoch 0/200] [Batch 2434/3166] [D loss: 1.069816] [G loss: 8.631550] [gradient_penalty: 0.010800]\n",
            "[Epoch 0/200] [Batch 2435/3166] [D loss: 2.327088] [G loss: 8.898744] [gradient_penalty: 0.010320]\n",
            "[Epoch 0/200] [Batch 2436/3166] [D loss: 1.498106] [G loss: 8.726954] [gradient_penalty: 0.009607]\n",
            "[Epoch 0/200] [Batch 2437/3166] [D loss: 1.856815] [G loss: 8.620689] [gradient_penalty: 0.010159]\n",
            "[Epoch 0/200] [Batch 2438/3166] [D loss: 1.759786] [G loss: 8.367012] [gradient_penalty: 0.009430]\n",
            "[Epoch 0/200] [Batch 2439/3166] [D loss: 1.701145] [G loss: 8.566652] [gradient_penalty: 0.011448]\n",
            "[Epoch 0/200] [Batch 2440/3166] [D loss: 2.459513] [G loss: 8.994947] [gradient_penalty: 0.009988]\n",
            "[Epoch 0/200] [Batch 2441/3166] [D loss: 1.597998] [G loss: 8.558386] [gradient_penalty: 0.010705]\n",
            "[Epoch 0/200] [Batch 2442/3166] [D loss: 2.332839] [G loss: 9.090399] [gradient_penalty: 0.008044]\n",
            "[Epoch 0/200] [Batch 2443/3166] [D loss: 1.737144] [G loss: 8.732977] [gradient_penalty: 0.010457]\n",
            "[Epoch 0/200] [Batch 2444/3166] [D loss: 1.345930] [G loss: 8.456182] [gradient_penalty: 0.011119]\n",
            "[Epoch 0/200] [Batch 2445/3166] [D loss: 2.110368] [G loss: 9.039831] [gradient_penalty: 0.008614]\n",
            "[Epoch 0/200] [Batch 2446/3166] [D loss: 1.583206] [G loss: 9.010694] [gradient_penalty: 0.010792]\n",
            "[Epoch 0/200] [Batch 2447/3166] [D loss: 1.819710] [G loss: 8.778797] [gradient_penalty: 0.015373]\n",
            "[Epoch 0/200] [Batch 2448/3166] [D loss: 2.161391] [G loss: 8.792151] [gradient_penalty: 0.010458]\n",
            "[Epoch 0/200] [Batch 2449/3166] [D loss: 0.092895] [G loss: 9.444481] [gradient_penalty: 0.007737]\n",
            "[Epoch 0/200] [Batch 2450/3166] [D loss: 2.379553] [G loss: 9.173310] [gradient_penalty: 0.012956]\n",
            "[Epoch 0/200] [Batch 2451/3166] [D loss: 2.305196] [G loss: 9.207783] [gradient_penalty: 0.009111]\n",
            "[Epoch 0/200] [Batch 2452/3166] [D loss: 2.013275] [G loss: 8.759270] [gradient_penalty: 0.007829]\n",
            "[Epoch 0/200] [Batch 2453/3166] [D loss: 1.667595] [G loss: 8.649611] [gradient_penalty: 0.014096]\n",
            "[Epoch 0/200] [Batch 2454/3166] [D loss: 1.079062] [G loss: 9.021788] [gradient_penalty: 0.010786]\n",
            "[Epoch 0/200] [Batch 2455/3166] [D loss: 1.734777] [G loss: 8.833405] [gradient_penalty: 0.014298]\n",
            "[Epoch 0/200] [Batch 2456/3166] [D loss: 2.723394] [G loss: 9.123737] [gradient_penalty: 0.011725]\n",
            "[Epoch 0/200] [Batch 2457/3166] [D loss: 2.106218] [G loss: 9.288596] [gradient_penalty: 0.010867]\n",
            "[Epoch 0/200] [Batch 2458/3166] [D loss: 1.921844] [G loss: 9.250744] [gradient_penalty: 0.010191]\n",
            "[Epoch 0/200] [Batch 2459/3166] [D loss: 2.912670] [G loss: 9.065250] [gradient_penalty: 0.010225]\n",
            "[Epoch 0/200] [Batch 2460/3166] [D loss: 1.921295] [G loss: 9.049091] [gradient_penalty: 0.009605]\n",
            "[Epoch 0/200] [Batch 2461/3166] [D loss: 1.263760] [G loss: 8.707867] [gradient_penalty: 0.012133]\n",
            "[Epoch 0/200] [Batch 2462/3166] [D loss: 1.810379] [G loss: 8.608812] [gradient_penalty: 0.008989]\n",
            "[Epoch 0/200] [Batch 2463/3166] [D loss: 2.049979] [G loss: 8.923581] [gradient_penalty: 0.008665]\n",
            "[Epoch 0/200] [Batch 2464/3166] [D loss: 2.199812] [G loss: 8.706232] [gradient_penalty: 0.008677]\n",
            "[Epoch 0/200] [Batch 2465/3166] [D loss: 2.636532] [G loss: 8.358504] [gradient_penalty: 0.008084]\n",
            "[Epoch 0/200] [Batch 2466/3166] [D loss: 1.008010] [G loss: 8.785372] [gradient_penalty: 0.007947]\n",
            "[Epoch 0/200] [Batch 2467/3166] [D loss: 1.612841] [G loss: 8.410444] [gradient_penalty: 0.007906]\n",
            "[Epoch 0/200] [Batch 2468/3166] [D loss: 2.117054] [G loss: 8.948356] [gradient_penalty: 0.014208]\n",
            "[Epoch 0/200] [Batch 2469/3166] [D loss: 1.594075] [G loss: 8.866227] [gradient_penalty: 0.012074]\n",
            "[Epoch 0/200] [Batch 2470/3166] [D loss: 1.428694] [G loss: 8.550879] [gradient_penalty: 0.012537]\n",
            "[Epoch 0/200] [Batch 2471/3166] [D loss: 1.730479] [G loss: 8.986077] [gradient_penalty: 0.008741]\n",
            "[Epoch 0/200] [Batch 2472/3166] [D loss: 2.529574] [G loss: 8.740921] [gradient_penalty: 0.012917]\n",
            "[Epoch 0/200] [Batch 2473/3166] [D loss: 1.877776] [G loss: 9.153704] [gradient_penalty: 0.010856]\n",
            "[Epoch 0/200] [Batch 2474/3166] [D loss: 2.854557] [G loss: 8.761437] [gradient_penalty: 0.012298]\n",
            "[Epoch 0/200] [Batch 2475/3166] [D loss: 2.118295] [G loss: 8.960060] [gradient_penalty: 0.008220]\n",
            "[Epoch 0/200] [Batch 2476/3166] [D loss: 2.350097] [G loss: 8.492563] [gradient_penalty: 0.013786]\n",
            "[Epoch 0/200] [Batch 2477/3166] [D loss: 2.285002] [G loss: 8.842566] [gradient_penalty: 0.011676]\n",
            "[Epoch 0/200] [Batch 2478/3166] [D loss: 1.939173] [G loss: 8.906368] [gradient_penalty: 0.010801]\n",
            "[Epoch 0/200] [Batch 2479/3166] [D loss: 1.827863] [G loss: 9.057762] [gradient_penalty: 0.008953]\n",
            "[Epoch 0/200] [Batch 2480/3166] [D loss: 1.845261] [G loss: 8.645126] [gradient_penalty: 0.012460]\n",
            "[Epoch 0/200] [Batch 2481/3166] [D loss: 1.145409] [G loss: 8.201082] [gradient_penalty: 0.008290]\n",
            "[Epoch 0/200] [Batch 2482/3166] [D loss: 2.365299] [G loss: 9.112798] [gradient_penalty: 0.011791]\n",
            "[Epoch 0/200] [Batch 2483/3166] [D loss: 1.858942] [G loss: 8.292521] [gradient_penalty: 0.014576]\n",
            "[Epoch 0/200] [Batch 2484/3166] [D loss: 1.207031] [G loss: 8.875002] [gradient_penalty: 0.010323]\n",
            "[Epoch 0/200] [Batch 2485/3166] [D loss: 2.334310] [G loss: 8.775263] [gradient_penalty: 0.011904]\n",
            "[Epoch 0/200] [Batch 2486/3166] [D loss: 1.546176] [G loss: 9.014221] [gradient_penalty: 0.010024]\n",
            "[Epoch 0/200] [Batch 2487/3166] [D loss: 1.798884] [G loss: 8.957158] [gradient_penalty: 0.012086]\n",
            "[Epoch 0/200] [Batch 2488/3166] [D loss: 2.195135] [G loss: 8.850746] [gradient_penalty: 0.008794]\n",
            "[Epoch 0/200] [Batch 2489/3166] [D loss: 1.607074] [G loss: 8.574120] [gradient_penalty: 0.012819]\n",
            "[Epoch 0/200] [Batch 2490/3166] [D loss: 1.587796] [G loss: 9.028828] [gradient_penalty: 0.010870]\n",
            "[Epoch 0/200] [Batch 2491/3166] [D loss: 1.939641] [G loss: 8.077139] [gradient_penalty: 0.012058]\n",
            "[Epoch 0/200] [Batch 2492/3166] [D loss: 2.126033] [G loss: 8.811241] [gradient_penalty: 0.011644]\n",
            "[Epoch 0/200] [Batch 2493/3166] [D loss: 1.970370] [G loss: 8.792592] [gradient_penalty: 0.009034]\n",
            "[Epoch 0/200] [Batch 2494/3166] [D loss: 1.447204] [G loss: 9.373852] [gradient_penalty: 0.009177]\n",
            "[Epoch 0/200] [Batch 2495/3166] [D loss: 2.532943] [G loss: 8.812708] [gradient_penalty: 0.009042]\n",
            "[Epoch 0/200] [Batch 2496/3166] [D loss: 1.956671] [G loss: 8.829184] [gradient_penalty: 0.010863]\n",
            "[Epoch 0/200] [Batch 2497/3166] [D loss: 1.460315] [G loss: 8.736016] [gradient_penalty: 0.006860]\n",
            "[Epoch 0/200] [Batch 2498/3166] [D loss: 2.549169] [G loss: 8.732159] [gradient_penalty: 0.010690]\n",
            "[Epoch 0/200] [Batch 2499/3166] [D loss: 2.593353] [G loss: 8.919129] [gradient_penalty: 0.009766]\n",
            "[Epoch 0/200] [Batch 2500/3166] [D loss: 3.350302] [G loss: 8.529451] [gradient_penalty: 0.008222]\n",
            "log\n",
            "[Epoch 0/200] [Batch 2501/3166] [D loss: 1.465665] [G loss: 8.503752] [gradient_penalty: 0.013285]\n",
            "[Epoch 0/200] [Batch 2502/3166] [D loss: 1.542037] [G loss: 8.364895] [gradient_penalty: 0.011836]\n",
            "[Epoch 0/200] [Batch 2503/3166] [D loss: 0.984120] [G loss: 8.692268] [gradient_penalty: 0.010918]\n",
            "[Epoch 0/200] [Batch 2504/3166] [D loss: 2.206925] [G loss: 8.764433] [gradient_penalty: 0.011039]\n",
            "[Epoch 0/200] [Batch 2505/3166] [D loss: 1.557957] [G loss: 9.602884] [gradient_penalty: 0.008278]\n",
            "[Epoch 0/200] [Batch 2506/3166] [D loss: 1.847237] [G loss: 9.083488] [gradient_penalty: 0.011214]\n",
            "[Epoch 0/200] [Batch 2507/3166] [D loss: 1.119989] [G loss: 8.936965] [gradient_penalty: 0.007362]\n",
            "[Epoch 0/200] [Batch 2508/3166] [D loss: 2.297156] [G loss: 9.520143] [gradient_penalty: 0.009020]\n",
            "[Epoch 0/200] [Batch 2509/3166] [D loss: 1.570218] [G loss: 8.961332] [gradient_penalty: 0.011441]\n",
            "[Epoch 0/200] [Batch 2510/3166] [D loss: 1.283259] [G loss: 9.169001] [gradient_penalty: 0.012100]\n",
            "[Epoch 0/200] [Batch 2511/3166] [D loss: 1.200087] [G loss: 9.333366] [gradient_penalty: 0.010582]\n",
            "[Epoch 0/200] [Batch 2512/3166] [D loss: 1.624474] [G loss: 9.477442] [gradient_penalty: 0.008452]\n",
            "[Epoch 0/200] [Batch 2513/3166] [D loss: 1.921278] [G loss: 9.538759] [gradient_penalty: 0.011910]\n",
            "[Epoch 0/200] [Batch 2514/3166] [D loss: 1.379807] [G loss: 9.552698] [gradient_penalty: 0.012483]\n",
            "[Epoch 0/200] [Batch 2515/3166] [D loss: 1.898377] [G loss: 8.914249] [gradient_penalty: 0.011189]\n",
            "[Epoch 0/200] [Batch 2516/3166] [D loss: 1.614212] [G loss: 9.176224] [gradient_penalty: 0.009569]\n",
            "[Epoch 0/200] [Batch 2517/3166] [D loss: 2.125675] [G loss: 8.731603] [gradient_penalty: 0.013053]\n",
            "[Epoch 0/200] [Batch 2518/3166] [D loss: 1.269181] [G loss: 8.733021] [gradient_penalty: 0.009413]\n",
            "[Epoch 0/200] [Batch 2519/3166] [D loss: 1.672136] [G loss: 9.531550] [gradient_penalty: 0.009044]\n",
            "[Epoch 0/200] [Batch 2520/3166] [D loss: 2.051708] [G loss: 9.728497] [gradient_penalty: 0.010073]\n",
            "[Epoch 0/200] [Batch 2521/3166] [D loss: 1.435203] [G loss: 9.715421] [gradient_penalty: 0.007813]\n",
            "[Epoch 0/200] [Batch 2522/3166] [D loss: 1.385674] [G loss: 9.106552] [gradient_penalty: 0.009604]\n",
            "[Epoch 0/200] [Batch 2523/3166] [D loss: 1.926816] [G loss: 9.417060] [gradient_penalty: 0.012666]\n",
            "[Epoch 0/200] [Batch 2524/3166] [D loss: 1.590303] [G loss: 8.911520] [gradient_penalty: 0.008100]\n",
            "[Epoch 0/200] [Batch 2525/3166] [D loss: 1.213658] [G loss: 8.992256] [gradient_penalty: 0.006302]\n",
            "[Epoch 0/200] [Batch 2526/3166] [D loss: 1.982641] [G loss: 9.294795] [gradient_penalty: 0.010799]\n",
            "[Epoch 0/200] [Batch 2527/3166] [D loss: 2.349632] [G loss: 9.346974] [gradient_penalty: 0.012070]\n",
            "[Epoch 0/200] [Batch 2528/3166] [D loss: 2.261606] [G loss: 9.230412] [gradient_penalty: 0.010956]\n",
            "[Epoch 0/200] [Batch 2529/3166] [D loss: 1.375415] [G loss: 9.385018] [gradient_penalty: 0.010005]\n",
            "[Epoch 0/200] [Batch 2530/3166] [D loss: 2.015723] [G loss: 9.510349] [gradient_penalty: 0.012996]\n",
            "[Epoch 0/200] [Batch 2531/3166] [D loss: 1.955268] [G loss: 9.731302] [gradient_penalty: 0.006749]\n",
            "[Epoch 0/200] [Batch 2532/3166] [D loss: 1.722822] [G loss: 9.448549] [gradient_penalty: 0.009712]\n",
            "[Epoch 0/200] [Batch 2533/3166] [D loss: 1.617269] [G loss: 9.292387] [gradient_penalty: 0.009362]\n",
            "[Epoch 0/200] [Batch 2534/3166] [D loss: 1.790867] [G loss: 9.283440] [gradient_penalty: 0.013968]\n",
            "[Epoch 0/200] [Batch 2535/3166] [D loss: 2.236006] [G loss: 9.612824] [gradient_penalty: 0.013909]\n",
            "[Epoch 0/200] [Batch 2536/3166] [D loss: 1.119482] [G loss: 9.435360] [gradient_penalty: 0.010136]\n",
            "[Epoch 0/200] [Batch 2537/3166] [D loss: 2.066460] [G loss: 9.254283] [gradient_penalty: 0.009507]\n",
            "[Epoch 0/200] [Batch 2538/3166] [D loss: 2.919433] [G loss: 9.807798] [gradient_penalty: 0.011435]\n",
            "[Epoch 0/200] [Batch 2539/3166] [D loss: 1.609189] [G loss: 9.241810] [gradient_penalty: 0.008237]\n",
            "[Epoch 0/200] [Batch 2540/3166] [D loss: 2.641051] [G loss: 9.685256] [gradient_penalty: 0.011421]\n",
            "[Epoch 0/200] [Batch 2541/3166] [D loss: 1.076199] [G loss: 9.521614] [gradient_penalty: 0.007155]\n",
            "[Epoch 0/200] [Batch 2542/3166] [D loss: 1.465786] [G loss: 9.345700] [gradient_penalty: 0.010136]\n",
            "[Epoch 0/200] [Batch 2543/3166] [D loss: 2.331909] [G loss: 8.974789] [gradient_penalty: 0.009824]\n",
            "[Epoch 0/200] [Batch 2544/3166] [D loss: 1.542188] [G loss: 9.705702] [gradient_penalty: 0.011393]\n",
            "[Epoch 0/200] [Batch 2545/3166] [D loss: 2.380248] [G loss: 9.682062] [gradient_penalty: 0.011607]\n",
            "[Epoch 0/200] [Batch 2546/3166] [D loss: 1.735297] [G loss: 9.181980] [gradient_penalty: 0.009704]\n",
            "[Epoch 0/200] [Batch 2547/3166] [D loss: 1.821518] [G loss: 9.041234] [gradient_penalty: 0.009623]\n",
            "[Epoch 0/200] [Batch 2548/3166] [D loss: 2.018398] [G loss: 9.062113] [gradient_penalty: 0.009664]\n",
            "[Epoch 0/200] [Batch 2549/3166] [D loss: 2.128396] [G loss: 9.170109] [gradient_penalty: 0.012433]\n",
            "[Epoch 0/200] [Batch 2550/3166] [D loss: 1.693653] [G loss: 9.679459] [gradient_penalty: 0.011070]\n",
            "[Epoch 0/200] [Batch 2551/3166] [D loss: 2.167114] [G loss: 9.243961] [gradient_penalty: 0.010231]\n",
            "[Epoch 0/200] [Batch 2552/3166] [D loss: 2.042196] [G loss: 9.723597] [gradient_penalty: 0.009639]\n",
            "[Epoch 0/200] [Batch 2553/3166] [D loss: 1.278585] [G loss: 9.540572] [gradient_penalty: 0.008583]\n",
            "[Epoch 0/200] [Batch 2554/3166] [D loss: 1.877923] [G loss: 9.584566] [gradient_penalty: 0.008341]\n",
            "[Epoch 0/200] [Batch 2555/3166] [D loss: 2.550176] [G loss: 9.204752] [gradient_penalty: 0.011993]\n",
            "[Epoch 0/200] [Batch 2556/3166] [D loss: 1.859071] [G loss: 9.106808] [gradient_penalty: 0.008814]\n",
            "[Epoch 0/200] [Batch 2557/3166] [D loss: 2.356904] [G loss: 9.569721] [gradient_penalty: 0.013540]\n",
            "[Epoch 0/200] [Batch 2558/3166] [D loss: 1.991694] [G loss: 9.772154] [gradient_penalty: 0.009529]\n",
            "[Epoch 0/200] [Batch 2559/3166] [D loss: 1.047119] [G loss: 9.752975] [gradient_penalty: 0.007298]\n",
            "[Epoch 0/200] [Batch 2560/3166] [D loss: 1.128469] [G loss: 9.906506] [gradient_penalty: 0.008663]\n",
            "[Epoch 0/200] [Batch 2561/3166] [D loss: 1.827945] [G loss: 10.333240] [gradient_penalty: 0.012037]\n",
            "[Epoch 0/200] [Batch 2562/3166] [D loss: 1.743284] [G loss: 10.222304] [gradient_penalty: 0.009285]\n",
            "[Epoch 0/200] [Batch 2563/3166] [D loss: 1.756157] [G loss: 9.710970] [gradient_penalty: 0.010109]\n",
            "[Epoch 0/200] [Batch 2564/3166] [D loss: 1.431678] [G loss: 9.684804] [gradient_penalty: 0.007513]\n",
            "[Epoch 0/200] [Batch 2565/3166] [D loss: 1.970338] [G loss: 9.851078] [gradient_penalty: 0.008915]\n",
            "[Epoch 0/200] [Batch 2566/3166] [D loss: 2.041369] [G loss: 9.807825] [gradient_penalty: 0.013031]\n",
            "[Epoch 0/200] [Batch 2567/3166] [D loss: 2.325713] [G loss: 9.585783] [gradient_penalty: 0.010823]\n",
            "[Epoch 0/200] [Batch 2568/3166] [D loss: 1.870891] [G loss: 9.158291] [gradient_penalty: 0.012932]\n",
            "[Epoch 0/200] [Batch 2569/3166] [D loss: 1.474486] [G loss: 9.905525] [gradient_penalty: 0.010714]\n",
            "[Epoch 0/200] [Batch 2570/3166] [D loss: 2.195491] [G loss: 10.591257] [gradient_penalty: 0.009731]\n",
            "[Epoch 0/200] [Batch 2571/3166] [D loss: 1.864326] [G loss: 9.641142] [gradient_penalty: 0.011786]\n",
            "[Epoch 0/200] [Batch 2572/3166] [D loss: 1.922824] [G loss: 9.810043] [gradient_penalty: 0.009574]\n",
            "[Epoch 0/200] [Batch 2573/3166] [D loss: 1.639578] [G loss: 9.947283] [gradient_penalty: 0.013072]\n",
            "[Epoch 0/200] [Batch 2574/3166] [D loss: 1.795472] [G loss: 9.835438] [gradient_penalty: 0.009934]\n",
            "[Epoch 0/200] [Batch 2575/3166] [D loss: 1.873948] [G loss: 9.387901] [gradient_penalty: 0.010703]\n",
            "[Epoch 0/200] [Batch 2576/3166] [D loss: 1.576420] [G loss: 10.502629] [gradient_penalty: 0.012029]\n",
            "[Epoch 0/200] [Batch 2577/3166] [D loss: 2.307194] [G loss: 10.277225] [gradient_penalty: 0.008059]\n",
            "[Epoch 0/200] [Batch 2578/3166] [D loss: 1.384099] [G loss: 9.818298] [gradient_penalty: 0.005437]\n",
            "[Epoch 0/200] [Batch 2579/3166] [D loss: 1.438396] [G loss: 9.770445] [gradient_penalty: 0.009801]\n",
            "[Epoch 0/200] [Batch 2580/3166] [D loss: 2.049887] [G loss: 10.523212] [gradient_penalty: 0.012534]\n",
            "[Epoch 0/200] [Batch 2581/3166] [D loss: 1.794811] [G loss: 10.035002] [gradient_penalty: 0.009310]\n",
            "[Epoch 0/200] [Batch 2582/3166] [D loss: 1.890994] [G loss: 9.988869] [gradient_penalty: 0.009290]\n",
            "[Epoch 0/200] [Batch 2583/3166] [D loss: 1.863662] [G loss: 10.078680] [gradient_penalty: 0.009875]\n",
            "[Epoch 0/200] [Batch 2584/3166] [D loss: 1.464652] [G loss: 9.979176] [gradient_penalty: 0.009638]\n",
            "[Epoch 0/200] [Batch 2585/3166] [D loss: 1.728670] [G loss: 9.837630] [gradient_penalty: 0.008859]\n",
            "[Epoch 0/200] [Batch 2586/3166] [D loss: 1.806222] [G loss: 10.295464] [gradient_penalty: 0.007373]\n",
            "[Epoch 0/200] [Batch 2587/3166] [D loss: 2.575665] [G loss: 9.784625] [gradient_penalty: 0.013850]\n",
            "[Epoch 0/200] [Batch 2588/3166] [D loss: 1.302578] [G loss: 10.107322] [gradient_penalty: 0.011177]\n",
            "[Epoch 0/200] [Batch 2589/3166] [D loss: 2.376534] [G loss: 9.814728] [gradient_penalty: 0.014327]\n",
            "[Epoch 0/200] [Batch 2590/3166] [D loss: 1.842342] [G loss: 10.484068] [gradient_penalty: 0.010413]\n",
            "[Epoch 0/200] [Batch 2591/3166] [D loss: 2.461809] [G loss: 9.740192] [gradient_penalty: 0.010272]\n",
            "[Epoch 0/200] [Batch 2592/3166] [D loss: 2.058083] [G loss: 9.803150] [gradient_penalty: 0.009900]\n",
            "[Epoch 0/200] [Batch 2593/3166] [D loss: 1.561886] [G loss: 10.776546] [gradient_penalty: 0.013385]\n",
            "[Epoch 0/200] [Batch 2594/3166] [D loss: 2.375546] [G loss: 10.411785] [gradient_penalty: 0.009181]\n",
            "[Epoch 0/200] [Batch 2595/3166] [D loss: 2.200000] [G loss: 10.730896] [gradient_penalty: 0.012608]\n",
            "[Epoch 0/200] [Batch 2596/3166] [D loss: 1.353041] [G loss: 9.933537] [gradient_penalty: 0.010436]\n",
            "[Epoch 0/200] [Batch 2597/3166] [D loss: 2.131083] [G loss: 9.965570] [gradient_penalty: 0.012212]\n",
            "[Epoch 0/200] [Batch 2598/3166] [D loss: 2.117406] [G loss: 10.573467] [gradient_penalty: 0.009135]\n",
            "[Epoch 0/200] [Batch 2599/3166] [D loss: 2.152143] [G loss: 10.436121] [gradient_penalty: 0.012228]\n",
            "[Epoch 0/200] [Batch 2600/3166] [D loss: 2.804524] [G loss: 10.225149] [gradient_penalty: 0.012210]\n",
            "log\n",
            "[Epoch 0/200] [Batch 2601/3166] [D loss: 2.568976] [G loss: 10.465033] [gradient_penalty: 0.011965]\n",
            "[Epoch 0/200] [Batch 2602/3166] [D loss: 1.361985] [G loss: 10.504512] [gradient_penalty: 0.007216]\n",
            "[Epoch 0/200] [Batch 2603/3166] [D loss: 1.777346] [G loss: 9.736119] [gradient_penalty: 0.008796]\n",
            "[Epoch 0/200] [Batch 2604/3166] [D loss: 2.378702] [G loss: 10.396564] [gradient_penalty: 0.012964]\n",
            "[Epoch 0/200] [Batch 2605/3166] [D loss: 2.117435] [G loss: 10.710150] [gradient_penalty: 0.010205]\n",
            "[Epoch 0/200] [Batch 2606/3166] [D loss: 2.056772] [G loss: 10.792261] [gradient_penalty: 0.011398]\n",
            "[Epoch 0/200] [Batch 2607/3166] [D loss: 1.415552] [G loss: 10.387364] [gradient_penalty: 0.012827]\n",
            "[Epoch 0/200] [Batch 2608/3166] [D loss: 1.498127] [G loss: 10.566488] [gradient_penalty: 0.013276]\n",
            "[Epoch 0/200] [Batch 2609/3166] [D loss: 0.779727] [G loss: 10.951464] [gradient_penalty: 0.010522]\n",
            "[Epoch 0/200] [Batch 2610/3166] [D loss: 2.520233] [G loss: 10.388158] [gradient_penalty: 0.012657]\n",
            "[Epoch 0/200] [Batch 2611/3166] [D loss: 1.733837] [G loss: 11.062233] [gradient_penalty: 0.007857]\n",
            "[Epoch 0/200] [Batch 2612/3166] [D loss: 1.969175] [G loss: 10.479205] [gradient_penalty: 0.014143]\n",
            "[Epoch 0/200] [Batch 2613/3166] [D loss: 2.492037] [G loss: 10.764510] [gradient_penalty: 0.009097]\n",
            "[Epoch 0/200] [Batch 2614/3166] [D loss: 1.347250] [G loss: 10.758169] [gradient_penalty: 0.009500]\n",
            "[Epoch 0/200] [Batch 2615/3166] [D loss: 2.524671] [G loss: 10.873717] [gradient_penalty: 0.010865]\n",
            "[Epoch 0/200] [Batch 2616/3166] [D loss: 1.773034] [G loss: 10.564846] [gradient_penalty: 0.011745]\n",
            "[Epoch 0/200] [Batch 2617/3166] [D loss: 2.520590] [G loss: 10.970669] [gradient_penalty: 0.011369]\n",
            "[Epoch 0/200] [Batch 2618/3166] [D loss: 1.902986] [G loss: 10.789040] [gradient_penalty: 0.011183]\n",
            "[Epoch 0/200] [Batch 2619/3166] [D loss: 1.940432] [G loss: 10.567013] [gradient_penalty: 0.011163]\n",
            "[Epoch 0/200] [Batch 2620/3166] [D loss: 1.508959] [G loss: 10.291693] [gradient_penalty: 0.011591]\n",
            "[Epoch 0/200] [Batch 2621/3166] [D loss: 1.657598] [G loss: 10.645594] [gradient_penalty: 0.011954]\n",
            "[Epoch 0/200] [Batch 2622/3166] [D loss: 1.477174] [G loss: 11.261003] [gradient_penalty: 0.011613]\n",
            "[Epoch 0/200] [Batch 2623/3166] [D loss: 2.161554] [G loss: 10.702860] [gradient_penalty: 0.009553]\n",
            "[Epoch 0/200] [Batch 2624/3166] [D loss: 3.067214] [G loss: 10.707821] [gradient_penalty: 0.016359]\n",
            "[Epoch 0/200] [Batch 2625/3166] [D loss: 1.662573] [G loss: 10.526468] [gradient_penalty: 0.016618]\n",
            "[Epoch 0/200] [Batch 2626/3166] [D loss: 1.651912] [G loss: 10.598537] [gradient_penalty: 0.012592]\n",
            "[Epoch 0/200] [Batch 2627/3166] [D loss: 2.540910] [G loss: 10.522076] [gradient_penalty: 0.010794]\n",
            "[Epoch 0/200] [Batch 2628/3166] [D loss: 1.075566] [G loss: 10.960317] [gradient_penalty: 0.010424]\n",
            "[Epoch 0/200] [Batch 2629/3166] [D loss: 2.503430] [G loss: 10.732157] [gradient_penalty: 0.014235]\n",
            "[Epoch 0/200] [Batch 2630/3166] [D loss: 1.445610] [G loss: 11.025385] [gradient_penalty: 0.008732]\n",
            "[Epoch 0/200] [Batch 2631/3166] [D loss: 0.854499] [G loss: 10.483414] [gradient_penalty: 0.013125]\n",
            "[Epoch 0/200] [Batch 2632/3166] [D loss: 1.579753] [G loss: 10.409040] [gradient_penalty: 0.010380]\n",
            "[Epoch 0/200] [Batch 2633/3166] [D loss: 1.819265] [G loss: 11.322932] [gradient_penalty: 0.011193]\n",
            "[Epoch 0/200] [Batch 2634/3166] [D loss: 1.179143] [G loss: 10.238095] [gradient_penalty: 0.011874]\n",
            "[Epoch 0/200] [Batch 2635/3166] [D loss: 2.006788] [G loss: 10.605844] [gradient_penalty: 0.008544]\n",
            "[Epoch 0/200] [Batch 2636/3166] [D loss: 2.095760] [G loss: 11.212295] [gradient_penalty: 0.011596]\n",
            "[Epoch 0/200] [Batch 2637/3166] [D loss: 1.173279] [G loss: 10.656260] [gradient_penalty: 0.010335]\n",
            "[Epoch 0/200] [Batch 2638/3166] [D loss: 1.551681] [G loss: 10.920419] [gradient_penalty: 0.010201]\n",
            "[Epoch 0/200] [Batch 2639/3166] [D loss: 1.527566] [G loss: 10.517412] [gradient_penalty: 0.007984]\n",
            "[Epoch 0/200] [Batch 2640/3166] [D loss: 1.717155] [G loss: 10.881697] [gradient_penalty: 0.012307]\n",
            "[Epoch 0/200] [Batch 2641/3166] [D loss: 2.271414] [G loss: 10.501675] [gradient_penalty: 0.008862]\n",
            "[Epoch 0/200] [Batch 2642/3166] [D loss: 1.720538] [G loss: 10.984998] [gradient_penalty: 0.011537]\n",
            "[Epoch 0/200] [Batch 2643/3166] [D loss: 1.836420] [G loss: 10.981876] [gradient_penalty: 0.013259]\n",
            "[Epoch 0/200] [Batch 2644/3166] [D loss: 2.092200] [G loss: 10.710102] [gradient_penalty: 0.010586]\n",
            "[Epoch 0/200] [Batch 2645/3166] [D loss: 1.815784] [G loss: 10.640152] [gradient_penalty: 0.008400]\n",
            "[Epoch 0/200] [Batch 2646/3166] [D loss: 2.019312] [G loss: 10.354156] [gradient_penalty: 0.007609]\n",
            "[Epoch 0/200] [Batch 2647/3166] [D loss: 1.315862] [G loss: 10.688221] [gradient_penalty: 0.009622]\n",
            "[Epoch 0/200] [Batch 2648/3166] [D loss: 2.848481] [G loss: 10.949563] [gradient_penalty: 0.013973]\n",
            "[Epoch 0/200] [Batch 2649/3166] [D loss: 2.871744] [G loss: 10.211227] [gradient_penalty: 0.015539]\n",
            "[Epoch 0/200] [Batch 2650/3166] [D loss: 2.179550] [G loss: 10.667554] [gradient_penalty: 0.009623]\n",
            "[Epoch 0/200] [Batch 2651/3166] [D loss: 2.710327] [G loss: 10.284131] [gradient_penalty: 0.007049]\n",
            "[Epoch 0/200] [Batch 2652/3166] [D loss: 2.701604] [G loss: 10.825604] [gradient_penalty: 0.012183]\n",
            "[Epoch 0/200] [Batch 2653/3166] [D loss: 2.183862] [G loss: 10.681657] [gradient_penalty: 0.011526]\n",
            "[Epoch 0/200] [Batch 2654/3166] [D loss: 1.813998] [G loss: 10.413108] [gradient_penalty: 0.010913]\n",
            "[Epoch 0/200] [Batch 2655/3166] [D loss: 2.132637] [G loss: 10.517731] [gradient_penalty: 0.012541]\n",
            "[Epoch 0/200] [Batch 2656/3166] [D loss: 1.932641] [G loss: 10.165016] [gradient_penalty: 0.013357]\n",
            "[Epoch 0/200] [Batch 2657/3166] [D loss: 1.862387] [G loss: 10.540730] [gradient_penalty: 0.010449]\n",
            "[Epoch 0/200] [Batch 2658/3166] [D loss: 1.971551] [G loss: 10.547543] [gradient_penalty: 0.012448]\n",
            "[Epoch 0/200] [Batch 2659/3166] [D loss: 1.738740] [G loss: 10.156353] [gradient_penalty: 0.008735]\n",
            "[Epoch 0/200] [Batch 2660/3166] [D loss: 1.429793] [G loss: 10.796822] [gradient_penalty: 0.010061]\n",
            "[Epoch 0/200] [Batch 2661/3166] [D loss: 2.154730] [G loss: 10.171468] [gradient_penalty: 0.011032]\n",
            "[Epoch 0/200] [Batch 2662/3166] [D loss: 2.423826] [G loss: 11.053500] [gradient_penalty: 0.012204]\n",
            "[Epoch 0/200] [Batch 2663/3166] [D loss: 1.616480] [G loss: 10.607653] [gradient_penalty: 0.010592]\n",
            "[Epoch 0/200] [Batch 2664/3166] [D loss: 1.968358] [G loss: 10.557265] [gradient_penalty: 0.011438]\n",
            "[Epoch 0/200] [Batch 2665/3166] [D loss: 1.497151] [G loss: 10.864677] [gradient_penalty: 0.009173]\n",
            "[Epoch 0/200] [Batch 2666/3166] [D loss: 1.620936] [G loss: 9.997129] [gradient_penalty: 0.011128]\n",
            "[Epoch 0/200] [Batch 2667/3166] [D loss: 2.939056] [G loss: 10.391991] [gradient_penalty: 0.009129]\n",
            "[Epoch 0/200] [Batch 2668/3166] [D loss: 2.212339] [G loss: 10.105820] [gradient_penalty: 0.014418]\n",
            "[Epoch 0/200] [Batch 2669/3166] [D loss: 2.000887] [G loss: 10.328772] [gradient_penalty: 0.016888]\n",
            "[Epoch 0/200] [Batch 2670/3166] [D loss: 1.302372] [G loss: 10.133606] [gradient_penalty: 0.007158]\n",
            "[Epoch 0/200] [Batch 2671/3166] [D loss: 2.255400] [G loss: 10.485273] [gradient_penalty: 0.011266]\n",
            "[Epoch 0/200] [Batch 2672/3166] [D loss: 1.814173] [G loss: 10.227692] [gradient_penalty: 0.010420]\n",
            "[Epoch 0/200] [Batch 2673/3166] [D loss: 1.417834] [G loss: 10.919169] [gradient_penalty: 0.008733]\n",
            "[Epoch 0/200] [Batch 2674/3166] [D loss: 1.851828] [G loss: 10.473964] [gradient_penalty: 0.012827]\n",
            "[Epoch 0/200] [Batch 2675/3166] [D loss: 1.379448] [G loss: 10.393655] [gradient_penalty: 0.012318]\n",
            "[Epoch 0/200] [Batch 2676/3166] [D loss: 2.006847] [G loss: 10.245892] [gradient_penalty: 0.010142]\n",
            "[Epoch 0/200] [Batch 2677/3166] [D loss: 0.701966] [G loss: 10.616736] [gradient_penalty: 0.009230]\n",
            "[Epoch 0/200] [Batch 2678/3166] [D loss: 1.699221] [G loss: 10.389677] [gradient_penalty: 0.008148]\n",
            "[Epoch 0/200] [Batch 2679/3166] [D loss: 1.546997] [G loss: 9.834835] [gradient_penalty: 0.013851]\n",
            "[Epoch 0/200] [Batch 2680/3166] [D loss: 1.110271] [G loss: 10.222509] [gradient_penalty: 0.012831]\n",
            "[Epoch 0/200] [Batch 2681/3166] [D loss: 2.360460] [G loss: 10.676662] [gradient_penalty: 0.013148]\n",
            "[Epoch 0/200] [Batch 2682/3166] [D loss: 1.936998] [G loss: 10.726072] [gradient_penalty: 0.010385]\n",
            "[Epoch 0/200] [Batch 2683/3166] [D loss: 1.685867] [G loss: 10.406702] [gradient_penalty: 0.008895]\n",
            "[Epoch 0/200] [Batch 2684/3166] [D loss: 0.847515] [G loss: 10.232784] [gradient_penalty: 0.006637]\n",
            "[Epoch 0/200] [Batch 2685/3166] [D loss: 2.139767] [G loss: 11.046492] [gradient_penalty: 0.008953]\n",
            "[Epoch 0/200] [Batch 2686/3166] [D loss: 2.062553] [G loss: 10.001633] [gradient_penalty: 0.007784]\n",
            "[Epoch 0/200] [Batch 2687/3166] [D loss: 2.172059] [G loss: 10.202383] [gradient_penalty: 0.009672]\n",
            "[Epoch 0/200] [Batch 2688/3166] [D loss: 2.149659] [G loss: 10.289094] [gradient_penalty: 0.012004]\n",
            "[Epoch 0/200] [Batch 2689/3166] [D loss: 1.936131] [G loss: 10.262403] [gradient_penalty: 0.012631]\n",
            "[Epoch 0/200] [Batch 2690/3166] [D loss: 1.659916] [G loss: 10.385051] [gradient_penalty: 0.008080]\n",
            "[Epoch 0/200] [Batch 2691/3166] [D loss: 1.548120] [G loss: 10.292617] [gradient_penalty: 0.013215]\n",
            "[Epoch 0/200] [Batch 2692/3166] [D loss: 2.745359] [G loss: 10.217890] [gradient_penalty: 0.011207]\n",
            "[Epoch 0/200] [Batch 2693/3166] [D loss: 2.030932] [G loss: 10.157692] [gradient_penalty: 0.008073]\n",
            "[Epoch 0/200] [Batch 2694/3166] [D loss: 2.229434] [G loss: 10.756773] [gradient_penalty: 0.010831]\n",
            "[Epoch 0/200] [Batch 2695/3166] [D loss: 1.191982] [G loss: 10.107821] [gradient_penalty: 0.009373]\n",
            "[Epoch 0/200] [Batch 2696/3166] [D loss: 1.740950] [G loss: 10.068983] [gradient_penalty: 0.008889]\n",
            "[Epoch 0/200] [Batch 2697/3166] [D loss: 2.068961] [G loss: 10.069077] [gradient_penalty: 0.011717]\n",
            "[Epoch 0/200] [Batch 2698/3166] [D loss: 2.530243] [G loss: 10.061286] [gradient_penalty: 0.008574]\n",
            "[Epoch 0/200] [Batch 2699/3166] [D loss: 1.609812] [G loss: 10.377588] [gradient_penalty: 0.009167]\n",
            "[Epoch 0/200] [Batch 2700/3166] [D loss: 1.519143] [G loss: 9.883816] [gradient_penalty: 0.008120]\n",
            "log\n",
            "[Epoch 0/200] [Batch 2701/3166] [D loss: 2.111192] [G loss: 10.072962] [gradient_penalty: 0.007695]\n",
            "[Epoch 0/200] [Batch 2702/3166] [D loss: 2.287390] [G loss: 9.958412] [gradient_penalty: 0.011192]\n",
            "[Epoch 0/200] [Batch 2703/3166] [D loss: 1.645905] [G loss: 9.780116] [gradient_penalty: 0.009472]\n",
            "[Epoch 0/200] [Batch 2704/3166] [D loss: 2.038752] [G loss: 10.390560] [gradient_penalty: 0.007046]\n",
            "[Epoch 0/200] [Batch 2705/3166] [D loss: 1.543239] [G loss: 9.432879] [gradient_penalty: 0.010310]\n",
            "[Epoch 0/200] [Batch 2706/3166] [D loss: 2.760329] [G loss: 9.928608] [gradient_penalty: 0.010548]\n",
            "[Epoch 0/200] [Batch 2707/3166] [D loss: 2.174051] [G loss: 10.256038] [gradient_penalty: 0.010500]\n",
            "[Epoch 0/200] [Batch 2708/3166] [D loss: 2.879326] [G loss: 9.387283] [gradient_penalty: 0.012591]\n",
            "[Epoch 0/200] [Batch 2709/3166] [D loss: 2.299439] [G loss: 9.481642] [gradient_penalty: 0.008163]\n",
            "[Epoch 0/200] [Batch 2710/3166] [D loss: 1.862535] [G loss: 9.429349] [gradient_penalty: 0.012974]\n",
            "[Epoch 0/200] [Batch 2711/3166] [D loss: 1.691387] [G loss: 9.756104] [gradient_penalty: 0.010694]\n",
            "[Epoch 0/200] [Batch 2712/3166] [D loss: 1.536725] [G loss: 10.280466] [gradient_penalty: 0.010855]\n",
            "[Epoch 0/200] [Batch 2713/3166] [D loss: 2.234013] [G loss: 9.574944] [gradient_penalty: 0.012282]\n",
            "[Epoch 0/200] [Batch 2714/3166] [D loss: 1.431185] [G loss: 10.209255] [gradient_penalty: 0.011635]\n",
            "[Epoch 0/200] [Batch 2715/3166] [D loss: 2.001428] [G loss: 10.059567] [gradient_penalty: 0.009090]\n",
            "[Epoch 0/200] [Batch 2716/3166] [D loss: 1.972484] [G loss: 10.002274] [gradient_penalty: 0.008767]\n",
            "[Epoch 0/200] [Batch 2717/3166] [D loss: 2.308969] [G loss: 9.660397] [gradient_penalty: 0.012040]\n",
            "[Epoch 0/200] [Batch 2718/3166] [D loss: 1.250599] [G loss: 9.521404] [gradient_penalty: 0.010023]\n",
            "[Epoch 0/200] [Batch 2719/3166] [D loss: 1.583023] [G loss: 9.756811] [gradient_penalty: 0.011508]\n",
            "[Epoch 0/200] [Batch 2720/3166] [D loss: 1.891582] [G loss: 9.642552] [gradient_penalty: 0.009313]\n",
            "[Epoch 0/200] [Batch 2721/3166] [D loss: 1.738333] [G loss: 9.685913] [gradient_penalty: 0.006017]\n",
            "[Epoch 0/200] [Batch 2722/3166] [D loss: 0.280809] [G loss: 9.695927] [gradient_penalty: 0.007191]\n",
            "[Epoch 0/200] [Batch 2723/3166] [D loss: 1.445338] [G loss: 9.097539] [gradient_penalty: 0.007699]\n",
            "[Epoch 0/200] [Batch 2724/3166] [D loss: 1.485896] [G loss: 9.482142] [gradient_penalty: 0.007241]\n",
            "[Epoch 0/200] [Batch 2725/3166] [D loss: 1.922181] [G loss: 9.746488] [gradient_penalty: 0.012417]\n",
            "[Epoch 0/200] [Batch 2726/3166] [D loss: 1.627176] [G loss: 9.225616] [gradient_penalty: 0.007795]\n",
            "[Epoch 0/200] [Batch 2727/3166] [D loss: 1.645904] [G loss: 9.666916] [gradient_penalty: 0.012463]\n",
            "[Epoch 0/200] [Batch 2728/3166] [D loss: 2.081120] [G loss: 9.608149] [gradient_penalty: 0.010057]\n",
            "[Epoch 0/200] [Batch 2729/3166] [D loss: 1.127845] [G loss: 9.527573] [gradient_penalty: 0.010031]\n",
            "[Epoch 0/200] [Batch 2730/3166] [D loss: 1.908616] [G loss: 9.820089] [gradient_penalty: 0.010558]\n",
            "[Epoch 0/200] [Batch 2731/3166] [D loss: 1.755481] [G loss: 9.602994] [gradient_penalty: 0.008520]\n",
            "[Epoch 0/200] [Batch 2732/3166] [D loss: 2.025180] [G loss: 10.027664] [gradient_penalty: 0.008538]\n",
            "[Epoch 0/200] [Batch 2733/3166] [D loss: 2.602989] [G loss: 9.932838] [gradient_penalty: 0.016479]\n",
            "[Epoch 0/200] [Batch 2734/3166] [D loss: 1.960945] [G loss: 9.916788] [gradient_penalty: 0.010237]\n",
            "[Epoch 0/200] [Batch 2735/3166] [D loss: 1.342752] [G loss: 10.164666] [gradient_penalty: 0.009423]\n",
            "[Epoch 0/200] [Batch 2736/3166] [D loss: 1.294592] [G loss: 10.471706] [gradient_penalty: 0.010772]\n",
            "[Epoch 0/200] [Batch 2737/3166] [D loss: 1.631368] [G loss: 10.207947] [gradient_penalty: 0.010603]\n",
            "[Epoch 0/200] [Batch 2738/3166] [D loss: 1.445999] [G loss: 9.990507] [gradient_penalty: 0.009289]\n",
            "[Epoch 0/200] [Batch 2739/3166] [D loss: 1.353943] [G loss: 9.662495] [gradient_penalty: 0.008634]\n",
            "[Epoch 0/200] [Batch 2740/3166] [D loss: 1.571725] [G loss: 10.034756] [gradient_penalty: 0.012528]\n",
            "[Epoch 0/200] [Batch 2741/3166] [D loss: 2.246912] [G loss: 10.197104] [gradient_penalty: 0.008387]\n",
            "[Epoch 0/200] [Batch 2742/3166] [D loss: 1.235094] [G loss: 9.525253] [gradient_penalty: 0.008540]\n",
            "[Epoch 0/200] [Batch 2743/3166] [D loss: 1.567067] [G loss: 9.823254] [gradient_penalty: 0.011488]\n",
            "[Epoch 0/200] [Batch 2744/3166] [D loss: 1.965065] [G loss: 9.415775] [gradient_penalty: 0.010368]\n",
            "[Epoch 0/200] [Batch 2745/3166] [D loss: 1.821915] [G loss: 10.012442] [gradient_penalty: 0.011375]\n",
            "[Epoch 0/200] [Batch 2746/3166] [D loss: 1.233465] [G loss: 9.265218] [gradient_penalty: 0.008145]\n",
            "[Epoch 0/200] [Batch 2747/3166] [D loss: 2.028769] [G loss: 9.743227] [gradient_penalty: 0.009353]\n",
            "[Epoch 0/200] [Batch 2748/3166] [D loss: 1.193679] [G loss: 10.024148] [gradient_penalty: 0.007824]\n",
            "[Epoch 0/200] [Batch 2749/3166] [D loss: 1.297770] [G loss: 9.529465] [gradient_penalty: 0.010122]\n",
            "[Epoch 0/200] [Batch 2750/3166] [D loss: 1.314618] [G loss: 9.948769] [gradient_penalty: 0.009733]\n",
            "[Epoch 0/200] [Batch 2751/3166] [D loss: 1.857778] [G loss: 9.741316] [gradient_penalty: 0.008699]\n",
            "[Epoch 0/200] [Batch 2752/3166] [D loss: 2.517531] [G loss: 9.790104] [gradient_penalty: 0.010436]\n",
            "[Epoch 0/200] [Batch 2753/3166] [D loss: 1.566172] [G loss: 10.015165] [gradient_penalty: 0.010568]\n",
            "[Epoch 0/200] [Batch 2754/3166] [D loss: 1.456242] [G loss: 10.103752] [gradient_penalty: 0.009460]\n",
            "[Epoch 0/200] [Batch 2755/3166] [D loss: 1.309938] [G loss: 10.445226] [gradient_penalty: 0.009598]\n",
            "[Epoch 0/200] [Batch 2756/3166] [D loss: 2.520598] [G loss: 9.814631] [gradient_penalty: 0.011382]\n",
            "[Epoch 0/200] [Batch 2757/3166] [D loss: 1.705760] [G loss: 10.105253] [gradient_penalty: 0.008155]\n",
            "[Epoch 0/200] [Batch 2758/3166] [D loss: 2.049921] [G loss: 9.642252] [gradient_penalty: 0.009681]\n",
            "[Epoch 0/200] [Batch 2759/3166] [D loss: 1.155299] [G loss: 9.904142] [gradient_penalty: 0.009995]\n",
            "[Epoch 0/200] [Batch 2760/3166] [D loss: 1.638134] [G loss: 10.306026] [gradient_penalty: 0.009278]\n",
            "[Epoch 0/200] [Batch 2761/3166] [D loss: 2.533203] [G loss: 10.315712] [gradient_penalty: 0.009700]\n",
            "[Epoch 0/200] [Batch 2762/3166] [D loss: 1.836196] [G loss: 10.575689] [gradient_penalty: 0.006969]\n",
            "[Epoch 0/200] [Batch 2763/3166] [D loss: 2.198824] [G loss: 10.313480] [gradient_penalty: 0.009911]\n",
            "[Epoch 0/200] [Batch 2764/3166] [D loss: 2.338810] [G loss: 10.725557] [gradient_penalty: 0.012470]\n",
            "[Epoch 0/200] [Batch 2765/3166] [D loss: 1.550743] [G loss: 9.354459] [gradient_penalty: 0.008003]\n",
            "[Epoch 0/200] [Batch 2766/3166] [D loss: 1.298042] [G loss: 10.728298] [gradient_penalty: 0.011565]\n",
            "[Epoch 0/200] [Batch 2767/3166] [D loss: 1.581821] [G loss: 10.538486] [gradient_penalty: 0.012124]\n",
            "[Epoch 0/200] [Batch 2768/3166] [D loss: 0.995004] [G loss: 10.333824] [gradient_penalty: 0.010150]\n",
            "[Epoch 0/200] [Batch 2769/3166] [D loss: 2.006551] [G loss: 10.602899] [gradient_penalty: 0.009693]\n",
            "[Epoch 0/200] [Batch 2770/3166] [D loss: 2.099297] [G loss: 10.663468] [gradient_penalty: 0.009469]\n",
            "[Epoch 0/200] [Batch 2771/3166] [D loss: 1.647000] [G loss: 10.969616] [gradient_penalty: 0.012719]\n",
            "[Epoch 0/200] [Batch 2772/3166] [D loss: 1.928241] [G loss: 10.359125] [gradient_penalty: 0.008463]\n",
            "[Epoch 0/200] [Batch 2773/3166] [D loss: 2.211440] [G loss: 10.448585] [gradient_penalty: 0.008565]\n",
            "[Epoch 0/200] [Batch 2774/3166] [D loss: 1.325524] [G loss: 9.979850] [gradient_penalty: 0.010976]\n",
            "[Epoch 0/200] [Batch 2775/3166] [D loss: 2.499755] [G loss: 10.265665] [gradient_penalty: 0.012496]\n",
            "[Epoch 0/200] [Batch 2776/3166] [D loss: 0.847201] [G loss: 11.133228] [gradient_penalty: 0.009083]\n",
            "[Epoch 0/200] [Batch 2777/3166] [D loss: 2.101185] [G loss: 10.921445] [gradient_penalty: 0.009026]\n",
            "[Epoch 0/200] [Batch 2778/3166] [D loss: 1.337621] [G loss: 11.449371] [gradient_penalty: 0.007606]\n",
            "[Epoch 0/200] [Batch 2779/3166] [D loss: 1.734776] [G loss: 11.177020] [gradient_penalty: 0.008780]\n",
            "[Epoch 0/200] [Batch 2780/3166] [D loss: 0.999961] [G loss: 11.421784] [gradient_penalty: 0.009589]\n",
            "[Epoch 0/200] [Batch 2781/3166] [D loss: 1.012331] [G loss: 11.094769] [gradient_penalty: 0.009876]\n",
            "[Epoch 0/200] [Batch 2782/3166] [D loss: 1.502066] [G loss: 10.687428] [gradient_penalty: 0.009314]\n",
            "[Epoch 0/200] [Batch 2783/3166] [D loss: 1.369349] [G loss: 10.809164] [gradient_penalty: 0.008519]\n",
            "[Epoch 0/200] [Batch 2784/3166] [D loss: 1.912773] [G loss: 10.287526] [gradient_penalty: 0.010495]\n",
            "[Epoch 0/200] [Batch 2785/3166] [D loss: 2.192624] [G loss: 11.104570] [gradient_penalty: 0.008536]\n",
            "[Epoch 0/200] [Batch 2786/3166] [D loss: 2.178776] [G loss: 11.487556] [gradient_penalty: 0.011618]\n",
            "[Epoch 0/200] [Batch 2787/3166] [D loss: 0.478105] [G loss: 11.397553] [gradient_penalty: 0.006926]\n",
            "[Epoch 0/200] [Batch 2788/3166] [D loss: 1.035264] [G loss: 10.823527] [gradient_penalty: 0.010921]\n",
            "[Epoch 0/200] [Batch 2789/3166] [D loss: 0.889074] [G loss: 11.429180] [gradient_penalty: 0.013178]\n",
            "[Epoch 0/200] [Batch 2790/3166] [D loss: 1.843741] [G loss: 11.217211] [gradient_penalty: 0.011971]\n",
            "[Epoch 0/200] [Batch 2791/3166] [D loss: 1.659546] [G loss: 10.930052] [gradient_penalty: 0.010457]\n",
            "[Epoch 0/200] [Batch 2792/3166] [D loss: 1.538056] [G loss: 11.729441] [gradient_penalty: 0.013240]\n",
            "[Epoch 0/200] [Batch 2793/3166] [D loss: 1.676821] [G loss: 11.886215] [gradient_penalty: 0.011634]\n",
            "[Epoch 0/200] [Batch 2794/3166] [D loss: 1.946914] [G loss: 11.236257] [gradient_penalty: 0.013226]\n",
            "[Epoch 0/200] [Batch 2795/3166] [D loss: 2.465590] [G loss: 11.847922] [gradient_penalty: 0.010365]\n",
            "[Epoch 0/200] [Batch 2796/3166] [D loss: 2.944196] [G loss: 11.485525] [gradient_penalty: 0.008543]\n",
            "[Epoch 0/200] [Batch 2797/3166] [D loss: 1.348058] [G loss: 12.367335] [gradient_penalty: 0.012628]\n",
            "[Epoch 0/200] [Batch 2798/3166] [D loss: 2.502103] [G loss: 11.184353] [gradient_penalty: 0.013176]\n",
            "[Epoch 0/200] [Batch 2799/3166] [D loss: 2.033817] [G loss: 11.488907] [gradient_penalty: 0.010405]\n",
            "[Epoch 0/200] [Batch 2800/3166] [D loss: 1.233854] [G loss: 11.529394] [gradient_penalty: 0.008487]\n",
            "log\n",
            "[Epoch 0/200] [Batch 2801/3166] [D loss: 1.280454] [G loss: 11.516424] [gradient_penalty: 0.007243]\n",
            "[Epoch 0/200] [Batch 2802/3166] [D loss: 0.899542] [G loss: 11.716095] [gradient_penalty: 0.010301]\n",
            "[Epoch 0/200] [Batch 2803/3166] [D loss: 1.742590] [G loss: 10.902489] [gradient_penalty: 0.007857]\n",
            "[Epoch 0/200] [Batch 2804/3166] [D loss: 1.061581] [G loss: 10.836283] [gradient_penalty: 0.007709]\n",
            "[Epoch 0/200] [Batch 2805/3166] [D loss: 2.004564] [G loss: 11.534369] [gradient_penalty: 0.005101]\n",
            "[Epoch 0/200] [Batch 2806/3166] [D loss: 2.094944] [G loss: 11.295990] [gradient_penalty: 0.008280]\n",
            "[Epoch 0/200] [Batch 2807/3166] [D loss: 1.085615] [G loss: 11.762623] [gradient_penalty: 0.011305]\n",
            "[Epoch 0/200] [Batch 2808/3166] [D loss: 1.008426] [G loss: 12.342803] [gradient_penalty: 0.008159]\n",
            "[Epoch 0/200] [Batch 2809/3166] [D loss: 1.354872] [G loss: 12.182576] [gradient_penalty: 0.011999]\n",
            "[Epoch 0/200] [Batch 2810/3166] [D loss: 1.220413] [G loss: 12.541071] [gradient_penalty: 0.009889]\n",
            "[Epoch 0/200] [Batch 2811/3166] [D loss: 1.350106] [G loss: 12.594671] [gradient_penalty: 0.013918]\n",
            "[Epoch 0/200] [Batch 2812/3166] [D loss: 2.813336] [G loss: 12.676769] [gradient_penalty: 0.011094]\n",
            "[Epoch 0/200] [Batch 2813/3166] [D loss: 1.755025] [G loss: 12.419020] [gradient_penalty: 0.011983]\n",
            "[Epoch 0/200] [Batch 2814/3166] [D loss: 1.723555] [G loss: 12.566938] [gradient_penalty: 0.012753]\n",
            "[Epoch 0/200] [Batch 2815/3166] [D loss: 1.017623] [G loss: 11.914817] [gradient_penalty: 0.013573]\n",
            "[Epoch 0/200] [Batch 2816/3166] [D loss: 1.998651] [G loss: 12.104538] [gradient_penalty: 0.013044]\n",
            "[Epoch 0/200] [Batch 2817/3166] [D loss: 0.919808] [G loss: 12.172479] [gradient_penalty: 0.007527]\n",
            "[Epoch 0/200] [Batch 2818/3166] [D loss: 1.225618] [G loss: 11.889120] [gradient_penalty: 0.009333]\n",
            "[Epoch 0/200] [Batch 2819/3166] [D loss: 1.192425] [G loss: 11.848276] [gradient_penalty: 0.006778]\n",
            "[Epoch 0/200] [Batch 2820/3166] [D loss: 1.798726] [G loss: 11.830298] [gradient_penalty: 0.008096]\n",
            "[Epoch 0/200] [Batch 2821/3166] [D loss: 0.530493] [G loss: 12.629761] [gradient_penalty: 0.006409]\n",
            "[Epoch 0/200] [Batch 2822/3166] [D loss: 0.933858] [G loss: 12.239508] [gradient_penalty: 0.007494]\n",
            "[Epoch 0/200] [Batch 2823/3166] [D loss: 0.989647] [G loss: 11.925467] [gradient_penalty: 0.011080]\n",
            "[Epoch 0/200] [Batch 2824/3166] [D loss: 2.303206] [G loss: 12.035839] [gradient_penalty: 0.009447]\n",
            "[Epoch 0/200] [Batch 2825/3166] [D loss: 1.396953] [G loss: 11.365823] [gradient_penalty: 0.009210]\n",
            "[Epoch 0/200] [Batch 2826/3166] [D loss: 1.070825] [G loss: 12.265560] [gradient_penalty: 0.009636]\n",
            "[Epoch 0/200] [Batch 2827/3166] [D loss: 1.772952] [G loss: 11.706476] [gradient_penalty: 0.008695]\n",
            "[Epoch 0/200] [Batch 2828/3166] [D loss: 1.904621] [G loss: 12.760927] [gradient_penalty: 0.008759]\n",
            "[Epoch 0/200] [Batch 2829/3166] [D loss: 1.620590] [G loss: 12.209732] [gradient_penalty: 0.009251]\n",
            "[Epoch 0/200] [Batch 2830/3166] [D loss: 0.834677] [G loss: 12.488637] [gradient_penalty: 0.011437]\n",
            "[Epoch 0/200] [Batch 2831/3166] [D loss: 1.945176] [G loss: 12.716499] [gradient_penalty: 0.009431]\n",
            "[Epoch 0/200] [Batch 2832/3166] [D loss: 1.142386] [G loss: 12.162338] [gradient_penalty: 0.010116]\n",
            "[Epoch 0/200] [Batch 2833/3166] [D loss: 0.804561] [G loss: 12.199031] [gradient_penalty: 0.008010]\n",
            "[Epoch 0/200] [Batch 2834/3166] [D loss: 2.108891] [G loss: 12.573376] [gradient_penalty: 0.007868]\n",
            "[Epoch 0/200] [Batch 2835/3166] [D loss: 1.134620] [G loss: 12.279980] [gradient_penalty: 0.011370]\n",
            "[Epoch 0/200] [Batch 2836/3166] [D loss: 1.930880] [G loss: 12.608335] [gradient_penalty: 0.008428]\n",
            "[Epoch 0/200] [Batch 2837/3166] [D loss: 1.675401] [G loss: 13.392305] [gradient_penalty: 0.012879]\n",
            "[Epoch 0/200] [Batch 2838/3166] [D loss: 0.858775] [G loss: 12.267603] [gradient_penalty: 0.009632]\n",
            "[Epoch 0/200] [Batch 2839/3166] [D loss: 1.386480] [G loss: 11.882860] [gradient_penalty: 0.007551]\n",
            "[Epoch 0/200] [Batch 2840/3166] [D loss: 1.885043] [G loss: 12.344140] [gradient_penalty: 0.010588]\n",
            "[Epoch 0/200] [Batch 2841/3166] [D loss: 2.254201] [G loss: 12.549047] [gradient_penalty: 0.010014]\n",
            "[Epoch 0/200] [Batch 2842/3166] [D loss: 2.888333] [G loss: 12.893389] [gradient_penalty: 0.011502]\n",
            "[Epoch 0/200] [Batch 2843/3166] [D loss: 1.915849] [G loss: 12.369781] [gradient_penalty: 0.008838]\n",
            "[Epoch 0/200] [Batch 2844/3166] [D loss: 0.965544] [G loss: 12.023703] [gradient_penalty: 0.009639]\n",
            "[Epoch 0/200] [Batch 2845/3166] [D loss: 0.272878] [G loss: 12.375402] [gradient_penalty: 0.009421]\n",
            "[Epoch 0/200] [Batch 2846/3166] [D loss: 1.315633] [G loss: 12.210044] [gradient_penalty: 0.008497]\n",
            "[Epoch 0/200] [Batch 2847/3166] [D loss: 1.873284] [G loss: 12.246275] [gradient_penalty: 0.010673]\n",
            "[Epoch 0/200] [Batch 2848/3166] [D loss: 2.084212] [G loss: 12.283269] [gradient_penalty: 0.010503]\n",
            "[Epoch 0/200] [Batch 2849/3166] [D loss: 1.743987] [G loss: 12.822954] [gradient_penalty: 0.009683]\n",
            "[Epoch 0/200] [Batch 2850/3166] [D loss: 1.032532] [G loss: 12.081753] [gradient_penalty: 0.006847]\n",
            "[Epoch 0/200] [Batch 2851/3166] [D loss: 0.996988] [G loss: 12.738609] [gradient_penalty: 0.008504]\n",
            "[Epoch 0/200] [Batch 2852/3166] [D loss: 1.342748] [G loss: 12.605829] [gradient_penalty: 0.009931]\n",
            "[Epoch 0/200] [Batch 2853/3166] [D loss: 0.877848] [G loss: 12.633489] [gradient_penalty: 0.009353]\n",
            "[Epoch 0/200] [Batch 2854/3166] [D loss: 1.743035] [G loss: 12.556351] [gradient_penalty: 0.007829]\n",
            "[Epoch 0/200] [Batch 2855/3166] [D loss: 1.805453] [G loss: 12.871536] [gradient_penalty: 0.008173]\n",
            "[Epoch 0/200] [Batch 2856/3166] [D loss: 1.966857] [G loss: 12.650688] [gradient_penalty: 0.012180]\n",
            "[Epoch 0/200] [Batch 2857/3166] [D loss: 1.717241] [G loss: 12.747497] [gradient_penalty: 0.011328]\n",
            "[Epoch 0/200] [Batch 2858/3166] [D loss: 1.017582] [G loss: 12.763803] [gradient_penalty: 0.012317]\n",
            "[Epoch 0/200] [Batch 2859/3166] [D loss: 1.230111] [G loss: 11.913939] [gradient_penalty: 0.010259]\n",
            "[Epoch 0/200] [Batch 2860/3166] [D loss: 0.707695] [G loss: 12.848822] [gradient_penalty: 0.010537]\n",
            "[Epoch 0/200] [Batch 2861/3166] [D loss: 1.830999] [G loss: 12.013968] [gradient_penalty: 0.009832]\n",
            "[Epoch 0/200] [Batch 2862/3166] [D loss: 1.621238] [G loss: 12.030020] [gradient_penalty: 0.012879]\n",
            "[Epoch 0/200] [Batch 2863/3166] [D loss: 1.708628] [G loss: 12.249556] [gradient_penalty: 0.007126]\n",
            "[Epoch 0/200] [Batch 2864/3166] [D loss: 1.586924] [G loss: 12.810767] [gradient_penalty: 0.010470]\n",
            "[Epoch 0/200] [Batch 2865/3166] [D loss: 1.745384] [G loss: 12.603699] [gradient_penalty: 0.007926]\n",
            "[Epoch 0/200] [Batch 2866/3166] [D loss: 0.908613] [G loss: 11.925013] [gradient_penalty: 0.006547]\n",
            "[Epoch 0/200] [Batch 2867/3166] [D loss: 0.862422] [G loss: 12.704098] [gradient_penalty: 0.010867]\n",
            "[Epoch 0/200] [Batch 2868/3166] [D loss: 1.496225] [G loss: 12.506733] [gradient_penalty: 0.009105]\n",
            "[Epoch 0/200] [Batch 2869/3166] [D loss: 1.505482] [G loss: 12.304590] [gradient_penalty: 0.012603]\n",
            "[Epoch 0/200] [Batch 2870/3166] [D loss: 1.429142] [G loss: 12.766164] [gradient_penalty: 0.010299]\n",
            "[Epoch 0/200] [Batch 2871/3166] [D loss: 0.753753] [G loss: 11.901245] [gradient_penalty: 0.008136]\n",
            "[Epoch 0/200] [Batch 2872/3166] [D loss: 1.613687] [G loss: 12.448164] [gradient_penalty: 0.011656]\n",
            "[Epoch 0/200] [Batch 2873/3166] [D loss: 1.717815] [G loss: 12.546457] [gradient_penalty: 0.006382]\n",
            "[Epoch 0/200] [Batch 2874/3166] [D loss: 1.802745] [G loss: 12.389002] [gradient_penalty: 0.010033]\n",
            "[Epoch 0/200] [Batch 2875/3166] [D loss: 2.215277] [G loss: 12.750319] [gradient_penalty: 0.010077]\n",
            "[Epoch 0/200] [Batch 2876/3166] [D loss: 1.713002] [G loss: 12.681740] [gradient_penalty: 0.009739]\n",
            "[Epoch 0/200] [Batch 2877/3166] [D loss: 1.792054] [G loss: 11.634235] [gradient_penalty: 0.010904]\n",
            "[Epoch 0/200] [Batch 2878/3166] [D loss: 0.932364] [G loss: 12.703547] [gradient_penalty: 0.009647]\n",
            "[Epoch 0/200] [Batch 2879/3166] [D loss: 1.917344] [G loss: 12.040136] [gradient_penalty: 0.011003]\n",
            "[Epoch 0/200] [Batch 2880/3166] [D loss: 1.509858] [G loss: 13.044223] [gradient_penalty: 0.009420]\n",
            "[Epoch 0/200] [Batch 2881/3166] [D loss: 1.328092] [G loss: 12.669847] [gradient_penalty: 0.010221]\n",
            "[Epoch 0/200] [Batch 2882/3166] [D loss: 1.856818] [G loss: 12.191710] [gradient_penalty: 0.012091]\n",
            "[Epoch 0/200] [Batch 2883/3166] [D loss: 1.747445] [G loss: 12.793276] [gradient_penalty: 0.009475]\n",
            "[Epoch 0/200] [Batch 2884/3166] [D loss: 1.219741] [G loss: 11.729107] [gradient_penalty: 0.007601]\n",
            "[Epoch 0/200] [Batch 2885/3166] [D loss: 1.144115] [G loss: 12.172680] [gradient_penalty: 0.011486]\n",
            "[Epoch 0/200] [Batch 2886/3166] [D loss: 0.964176] [G loss: 11.945309] [gradient_penalty: 0.009054]\n",
            "[Epoch 0/200] [Batch 2887/3166] [D loss: 1.412278] [G loss: 12.464824] [gradient_penalty: 0.006814]\n",
            "[Epoch 0/200] [Batch 2888/3166] [D loss: 1.447823] [G loss: 12.117065] [gradient_penalty: 0.009422]\n",
            "[Epoch 0/200] [Batch 2889/3166] [D loss: 1.582181] [G loss: 13.099487] [gradient_penalty: 0.009509]\n",
            "[Epoch 0/200] [Batch 2890/3166] [D loss: 1.092571] [G loss: 12.669729] [gradient_penalty: 0.009639]\n",
            "[Epoch 0/200] [Batch 2891/3166] [D loss: 1.927007] [G loss: 12.522532] [gradient_penalty: 0.010010]\n",
            "[Epoch 0/200] [Batch 2892/3166] [D loss: 1.673014] [G loss: 12.610394] [gradient_penalty: 0.012170]\n",
            "[Epoch 0/200] [Batch 2893/3166] [D loss: 2.550725] [G loss: 12.461317] [gradient_penalty: 0.012721]\n",
            "[Epoch 0/200] [Batch 2894/3166] [D loss: 2.198832] [G loss: 12.235153] [gradient_penalty: 0.009611]\n",
            "[Epoch 0/200] [Batch 2895/3166] [D loss: 0.765763] [G loss: 12.060833] [gradient_penalty: 0.009516]\n",
            "[Epoch 0/200] [Batch 2896/3166] [D loss: 1.246650] [G loss: 12.134799] [gradient_penalty: 0.017432]\n",
            "[Epoch 0/200] [Batch 2897/3166] [D loss: 1.443397] [G loss: 12.822430] [gradient_penalty: 0.007118]\n",
            "[Epoch 0/200] [Batch 2898/3166] [D loss: 2.056750] [G loss: 12.403530] [gradient_penalty: 0.007001]\n",
            "[Epoch 0/200] [Batch 2899/3166] [D loss: 2.089682] [G loss: 11.779474] [gradient_penalty: 0.008987]\n",
            "[Epoch 0/200] [Batch 2900/3166] [D loss: 1.012465] [G loss: 12.506541] [gradient_penalty: 0.008219]\n",
            "log\n",
            "[Epoch 0/200] [Batch 2901/3166] [D loss: 1.040541] [G loss: 12.042430] [gradient_penalty: 0.009347]\n",
            "[Epoch 0/200] [Batch 2902/3166] [D loss: 0.882395] [G loss: 12.856035] [gradient_penalty: 0.009834]\n",
            "[Epoch 0/200] [Batch 2903/3166] [D loss: 1.330099] [G loss: 12.537296] [gradient_penalty: 0.008732]\n",
            "[Epoch 0/200] [Batch 2904/3166] [D loss: 1.115625] [G loss: 11.754284] [gradient_penalty: 0.009691]\n",
            "[Epoch 0/200] [Batch 2905/3166] [D loss: 2.412174] [G loss: 11.959904] [gradient_penalty: 0.022346]\n",
            "[Epoch 0/200] [Batch 2906/3166] [D loss: 1.676222] [G loss: 12.146653] [gradient_penalty: 0.010785]\n",
            "[Epoch 0/200] [Batch 2907/3166] [D loss: 1.805423] [G loss: 12.234436] [gradient_penalty: 0.007276]\n",
            "[Epoch 0/200] [Batch 2908/3166] [D loss: 1.540256] [G loss: 12.707933] [gradient_penalty: 0.007394]\n",
            "[Epoch 0/200] [Batch 2909/3166] [D loss: 2.877699] [G loss: 12.136925] [gradient_penalty: 0.009998]\n",
            "[Epoch 0/200] [Batch 2910/3166] [D loss: 1.663181] [G loss: 11.689445] [gradient_penalty: 0.009978]\n",
            "[Epoch 0/200] [Batch 2911/3166] [D loss: 2.573907] [G loss: 12.146687] [gradient_penalty: 0.011031]\n",
            "[Epoch 0/200] [Batch 2912/3166] [D loss: 1.264268] [G loss: 11.948204] [gradient_penalty: 0.007417]\n",
            "[Epoch 0/200] [Batch 2913/3166] [D loss: 1.999999] [G loss: 12.349052] [gradient_penalty: 0.010925]\n",
            "[Epoch 0/200] [Batch 2914/3166] [D loss: 1.558778] [G loss: 12.455736] [gradient_penalty: 0.006489]\n",
            "[Epoch 0/200] [Batch 2915/3166] [D loss: 1.633203] [G loss: 12.059806] [gradient_penalty: 0.006746]\n",
            "[Epoch 0/200] [Batch 2916/3166] [D loss: 1.813720] [G loss: 11.875563] [gradient_penalty: 0.007745]\n",
            "[Epoch 0/200] [Batch 2917/3166] [D loss: 1.246613] [G loss: 11.933329] [gradient_penalty: 0.007989]\n",
            "[Epoch 0/200] [Batch 2918/3166] [D loss: 1.105196] [G loss: 11.932703] [gradient_penalty: 0.007845]\n",
            "[Epoch 0/200] [Batch 2919/3166] [D loss: 1.664605] [G loss: 11.812563] [gradient_penalty: 0.008106]\n",
            "[Epoch 0/200] [Batch 2920/3166] [D loss: 1.804349] [G loss: 12.126559] [gradient_penalty: 0.011005]\n",
            "[Epoch 0/200] [Batch 2921/3166] [D loss: 1.771147] [G loss: 12.206047] [gradient_penalty: 0.006765]\n",
            "[Epoch 0/200] [Batch 2922/3166] [D loss: 2.111650] [G loss: 12.025909] [gradient_penalty: 0.007493]\n",
            "[Epoch 0/200] [Batch 2923/3166] [D loss: 2.116650] [G loss: 12.872924] [gradient_penalty: 0.009634]\n",
            "[Epoch 0/200] [Batch 2924/3166] [D loss: 0.794107] [G loss: 12.495440] [gradient_penalty: 0.008940]\n",
            "[Epoch 0/200] [Batch 2925/3166] [D loss: 1.350672] [G loss: 11.699463] [gradient_penalty: 0.010340]\n",
            "[Epoch 0/200] [Batch 2926/3166] [D loss: 1.813683] [G loss: 11.797190] [gradient_penalty: 0.008146]\n",
            "[Epoch 0/200] [Batch 2927/3166] [D loss: 0.879641] [G loss: 12.875253] [gradient_penalty: 0.009215]\n",
            "[Epoch 0/200] [Batch 2928/3166] [D loss: 1.651682] [G loss: 12.194098] [gradient_penalty: 0.008159]\n",
            "[Epoch 0/200] [Batch 2929/3166] [D loss: 1.045867] [G loss: 12.073860] [gradient_penalty: 0.008380]\n",
            "[Epoch 0/200] [Batch 2930/3166] [D loss: 1.679277] [G loss: 11.954541] [gradient_penalty: 0.009667]\n",
            "[Epoch 0/200] [Batch 2931/3166] [D loss: 3.025085] [G loss: 12.173296] [gradient_penalty: 0.010388]\n",
            "[Epoch 0/200] [Batch 2932/3166] [D loss: 2.204861] [G loss: 11.915619] [gradient_penalty: 0.007574]\n",
            "[Epoch 0/200] [Batch 2933/3166] [D loss: 1.754341] [G loss: 11.884466] [gradient_penalty: 0.007824]\n",
            "[Epoch 0/200] [Batch 2934/3166] [D loss: 1.902688] [G loss: 11.244827] [gradient_penalty: 0.007187]\n",
            "[Epoch 0/200] [Batch 2935/3166] [D loss: 2.119656] [G loss: 12.048220] [gradient_penalty: 0.009562]\n",
            "[Epoch 0/200] [Batch 2936/3166] [D loss: 1.844839] [G loss: 11.975413] [gradient_penalty: 0.008061]\n",
            "[Epoch 0/200] [Batch 2937/3166] [D loss: 2.504223] [G loss: 11.132845] [gradient_penalty: 0.008270]\n",
            "[Epoch 0/200] [Batch 2938/3166] [D loss: 1.302582] [G loss: 11.419220] [gradient_penalty: 0.009620]\n",
            "[Epoch 0/200] [Batch 2939/3166] [D loss: 2.318418] [G loss: 11.962763] [gradient_penalty: 0.010283]\n",
            "[Epoch 0/200] [Batch 2940/3166] [D loss: 1.735880] [G loss: 12.036160] [gradient_penalty: 0.011133]\n",
            "[Epoch 0/200] [Batch 2941/3166] [D loss: 1.252061] [G loss: 11.693153] [gradient_penalty: 0.010508]\n",
            "[Epoch 0/200] [Batch 2942/3166] [D loss: 0.844451] [G loss: 11.853463] [gradient_penalty: 0.010349]\n",
            "[Epoch 0/200] [Batch 2943/3166] [D loss: 1.961494] [G loss: 11.565970] [gradient_penalty: 0.008086]\n",
            "[Epoch 0/200] [Batch 2944/3166] [D loss: 1.751355] [G loss: 12.065313] [gradient_penalty: 0.008967]\n",
            "[Epoch 0/200] [Batch 2945/3166] [D loss: 2.311267] [G loss: 11.508184] [gradient_penalty: 0.009021]\n",
            "[Epoch 0/200] [Batch 2946/3166] [D loss: 0.613563] [G loss: 12.540426] [gradient_penalty: 0.008322]\n",
            "[Epoch 0/200] [Batch 2947/3166] [D loss: 1.287660] [G loss: 12.189817] [gradient_penalty: 0.009957]\n",
            "[Epoch 0/200] [Batch 2948/3166] [D loss: 2.070282] [G loss: 11.480587] [gradient_penalty: 0.007624]\n",
            "[Epoch 0/200] [Batch 2949/3166] [D loss: 1.826867] [G loss: 11.365190] [gradient_penalty: 0.005936]\n",
            "[Epoch 0/200] [Batch 2950/3166] [D loss: 1.527320] [G loss: 11.814404] [gradient_penalty: 0.010464]\n",
            "[Epoch 0/200] [Batch 2951/3166] [D loss: 0.990887] [G loss: 12.156639] [gradient_penalty: 0.011361]\n",
            "[Epoch 0/200] [Batch 2952/3166] [D loss: 1.653335] [G loss: 11.671490] [gradient_penalty: 0.011894]\n",
            "[Epoch 0/200] [Batch 2953/3166] [D loss: 1.343998] [G loss: 11.587457] [gradient_penalty: 0.009211]\n",
            "[Epoch 0/200] [Batch 2954/3166] [D loss: 1.230155] [G loss: 12.419039] [gradient_penalty: 0.008837]\n",
            "[Epoch 0/200] [Batch 2955/3166] [D loss: 1.378950] [G loss: 11.180610] [gradient_penalty: 0.009202]\n",
            "[Epoch 0/200] [Batch 2956/3166] [D loss: 2.274245] [G loss: 11.357402] [gradient_penalty: 0.007653]\n",
            "[Epoch 0/200] [Batch 2957/3166] [D loss: 1.279573] [G loss: 11.175710] [gradient_penalty: 0.011144]\n",
            "[Epoch 0/200] [Batch 2958/3166] [D loss: 2.125831] [G loss: 12.045769] [gradient_penalty: 0.008763]\n",
            "[Epoch 0/200] [Batch 2959/3166] [D loss: 1.565213] [G loss: 11.521740] [gradient_penalty: 0.012758]\n",
            "[Epoch 0/200] [Batch 2960/3166] [D loss: 1.960848] [G loss: 11.683931] [gradient_penalty: 0.009992]\n",
            "[Epoch 0/200] [Batch 2961/3166] [D loss: 0.945996] [G loss: 11.929772] [gradient_penalty: 0.011835]\n",
            "[Epoch 0/200] [Batch 2962/3166] [D loss: 2.542497] [G loss: 12.113487] [gradient_penalty: 0.014941]\n",
            "[Epoch 0/200] [Batch 2963/3166] [D loss: 1.893889] [G loss: 10.908416] [gradient_penalty: 0.010244]\n",
            "[Epoch 0/200] [Batch 2964/3166] [D loss: 1.038437] [G loss: 11.082573] [gradient_penalty: 0.012499]\n",
            "[Epoch 0/200] [Batch 2965/3166] [D loss: 2.444285] [G loss: 11.531244] [gradient_penalty: 0.010148]\n",
            "[Epoch 0/200] [Batch 2966/3166] [D loss: 1.996143] [G loss: 11.725048] [gradient_penalty: 0.011460]\n",
            "[Epoch 0/200] [Batch 2967/3166] [D loss: 1.234698] [G loss: 11.338256] [gradient_penalty: 0.010582]\n",
            "[Epoch 0/200] [Batch 2968/3166] [D loss: 2.532285] [G loss: 11.264620] [gradient_penalty: 0.009291]\n",
            "[Epoch 0/200] [Batch 2969/3166] [D loss: 2.086897] [G loss: 10.929359] [gradient_penalty: 0.008738]\n",
            "[Epoch 0/200] [Batch 2970/3166] [D loss: 1.048449] [G loss: 12.021461] [gradient_penalty: 0.011926]\n",
            "[Epoch 0/200] [Batch 2971/3166] [D loss: 1.772380] [G loss: 10.605882] [gradient_penalty: 0.009440]\n",
            "[Epoch 0/200] [Batch 2972/3166] [D loss: 2.481323] [G loss: 11.855751] [gradient_penalty: 0.009170]\n",
            "[Epoch 0/200] [Batch 2973/3166] [D loss: 2.114884] [G loss: 11.351021] [gradient_penalty: 0.015660]\n",
            "[Epoch 0/200] [Batch 2974/3166] [D loss: 1.152250] [G loss: 11.464319] [gradient_penalty: 0.009375]\n",
            "[Epoch 0/200] [Batch 2975/3166] [D loss: 1.294698] [G loss: 11.872433] [gradient_penalty: 0.008679]\n",
            "[Epoch 0/200] [Batch 2976/3166] [D loss: 2.093409] [G loss: 11.775693] [gradient_penalty: 0.015647]\n",
            "[Epoch 0/200] [Batch 2977/3166] [D loss: 1.989832] [G loss: 11.785957] [gradient_penalty: 0.005973]\n",
            "[Epoch 0/200] [Batch 2978/3166] [D loss: 1.386930] [G loss: 11.737141] [gradient_penalty: 0.009873]\n",
            "[Epoch 0/200] [Batch 2979/3166] [D loss: 1.775950] [G loss: 12.834793] [gradient_penalty: 0.009728]\n",
            "[Epoch 0/200] [Batch 2980/3166] [D loss: 1.919596] [G loss: 11.954998] [gradient_penalty: 0.010663]\n",
            "[Epoch 0/200] [Batch 2981/3166] [D loss: 1.450677] [G loss: 11.513041] [gradient_penalty: 0.009023]\n",
            "[Epoch 0/200] [Batch 2982/3166] [D loss: 2.027013] [G loss: 11.897399] [gradient_penalty: 0.012623]\n",
            "[Epoch 0/200] [Batch 2983/3166] [D loss: 1.380424] [G loss: 11.990418] [gradient_penalty: 0.009124]\n",
            "[Epoch 0/200] [Batch 2984/3166] [D loss: 2.548259] [G loss: 10.998772] [gradient_penalty: 0.010361]\n",
            "[Epoch 0/200] [Batch 2985/3166] [D loss: 1.461383] [G loss: 11.632360] [gradient_penalty: 0.009008]\n",
            "[Epoch 0/200] [Batch 2986/3166] [D loss: 1.320588] [G loss: 11.569017] [gradient_penalty: 0.007431]\n",
            "[Epoch 0/200] [Batch 2987/3166] [D loss: 1.597447] [G loss: 11.877186] [gradient_penalty: 0.010347]\n",
            "[Epoch 0/200] [Batch 2988/3166] [D loss: 1.296023] [G loss: 11.352231] [gradient_penalty: 0.010294]\n",
            "[Epoch 0/200] [Batch 2989/3166] [D loss: 1.769424] [G loss: 12.509873] [gradient_penalty: 0.009008]\n",
            "[Epoch 0/200] [Batch 2990/3166] [D loss: 1.496743] [G loss: 11.348088] [gradient_penalty: 0.007246]\n",
            "[Epoch 0/200] [Batch 2991/3166] [D loss: 2.040361] [G loss: 11.754629] [gradient_penalty: 0.008579]\n",
            "[Epoch 0/200] [Batch 2992/3166] [D loss: 1.977173] [G loss: 11.678799] [gradient_penalty: 0.008406]\n",
            "[Epoch 0/200] [Batch 2993/3166] [D loss: 2.460011] [G loss: 11.219347] [gradient_penalty: 0.014141]\n",
            "[Epoch 0/200] [Batch 2994/3166] [D loss: 1.890555] [G loss: 11.181482] [gradient_penalty: 0.007916]\n",
            "[Epoch 0/200] [Batch 2995/3166] [D loss: 1.934611] [G loss: 11.741104] [gradient_penalty: 0.008226]\n",
            "[Epoch 0/200] [Batch 2996/3166] [D loss: 1.897465] [G loss: 11.432440] [gradient_penalty: 0.006218]\n",
            "[Epoch 0/200] [Batch 2997/3166] [D loss: 1.675756] [G loss: 11.860725] [gradient_penalty: 0.009024]\n",
            "[Epoch 0/200] [Batch 2998/3166] [D loss: 1.877537] [G loss: 11.982687] [gradient_penalty: 0.010828]\n",
            "[Epoch 0/200] [Batch 2999/3166] [D loss: 1.564015] [G loss: 11.912418] [gradient_penalty: 0.007419]\n",
            "[Epoch 0/200] [Batch 3000/3166] [D loss: 1.705711] [G loss: 11.452706] [gradient_penalty: 0.009121]\n",
            "log\n",
            "[Epoch 0/200] [Batch 3001/3166] [D loss: 1.083779] [G loss: 11.830125] [gradient_penalty: 0.010460]\n",
            "[Epoch 0/200] [Batch 3002/3166] [D loss: 1.072255] [G loss: 11.948084] [gradient_penalty: 0.007317]\n",
            "[Epoch 0/200] [Batch 3003/3166] [D loss: 1.495033] [G loss: 11.998160] [gradient_penalty: 0.007301]\n",
            "[Epoch 0/200] [Batch 3004/3166] [D loss: 1.979007] [G loss: 11.661089] [gradient_penalty: 0.007571]\n",
            "[Epoch 0/200] [Batch 3005/3166] [D loss: 1.527193] [G loss: 11.624695] [gradient_penalty: 0.009301]\n",
            "[Epoch 0/200] [Batch 3006/3166] [D loss: 1.432273] [G loss: 11.393181] [gradient_penalty: 0.011118]\n",
            "[Epoch 0/200] [Batch 3007/3166] [D loss: 2.624487] [G loss: 11.499413] [gradient_penalty: 0.011680]\n",
            "[Epoch 0/200] [Batch 3008/3166] [D loss: 1.266774] [G loss: 11.370624] [gradient_penalty: 0.009971]\n",
            "[Epoch 0/200] [Batch 3009/3166] [D loss: 0.711871] [G loss: 11.675987] [gradient_penalty: 0.014865]\n",
            "[Epoch 0/200] [Batch 3010/3166] [D loss: 1.443871] [G loss: 11.628029] [gradient_penalty: 0.010760]\n",
            "[Epoch 0/200] [Batch 3011/3166] [D loss: 2.922360] [G loss: 11.131092] [gradient_penalty: 0.011169]\n",
            "[Epoch 0/200] [Batch 3012/3166] [D loss: 1.546099] [G loss: 11.483300] [gradient_penalty: 0.009557]\n",
            "[Epoch 0/200] [Batch 3013/3166] [D loss: 1.166829] [G loss: 11.497803] [gradient_penalty: 0.007752]\n",
            "[Epoch 0/200] [Batch 3014/3166] [D loss: 1.938353] [G loss: 11.291664] [gradient_penalty: 0.011004]\n",
            "[Epoch 0/200] [Batch 3015/3166] [D loss: 2.080604] [G loss: 11.937542] [gradient_penalty: 0.005621]\n",
            "[Epoch 0/200] [Batch 3016/3166] [D loss: 1.512620] [G loss: 11.689966] [gradient_penalty: 0.008814]\n",
            "[Epoch 0/200] [Batch 3017/3166] [D loss: 1.222724] [G loss: 11.608427] [gradient_penalty: 0.007253]\n",
            "[Epoch 0/200] [Batch 3018/3166] [D loss: 1.517203] [G loss: 11.231121] [gradient_penalty: 0.011861]\n",
            "[Epoch 0/200] [Batch 3019/3166] [D loss: 1.547775] [G loss: 12.222456] [gradient_penalty: 0.008948]\n",
            "[Epoch 0/200] [Batch 3020/3166] [D loss: 1.357437] [G loss: 11.819351] [gradient_penalty: 0.010759]\n",
            "[Epoch 0/200] [Batch 3021/3166] [D loss: 1.838447] [G loss: 11.626912] [gradient_penalty: 0.009051]\n",
            "[Epoch 0/200] [Batch 3022/3166] [D loss: 2.337774] [G loss: 11.919748] [gradient_penalty: 0.010794]\n",
            "[Epoch 0/200] [Batch 3023/3166] [D loss: 2.307454] [G loss: 11.251740] [gradient_penalty: 0.008917]\n",
            "[Epoch 0/200] [Batch 3024/3166] [D loss: 1.313236] [G loss: 11.381597] [gradient_penalty: 0.009756]\n",
            "[Epoch 0/200] [Batch 3025/3166] [D loss: 1.917097] [G loss: 11.218805] [gradient_penalty: 0.009576]\n",
            "[Epoch 0/200] [Batch 3026/3166] [D loss: 1.454285] [G loss: 11.331463] [gradient_penalty: 0.008954]\n",
            "[Epoch 0/200] [Batch 3027/3166] [D loss: 2.363028] [G loss: 11.593660] [gradient_penalty: 0.012386]\n",
            "[Epoch 0/200] [Batch 3028/3166] [D loss: 1.826374] [G loss: 11.550068] [gradient_penalty: 0.009831]\n",
            "[Epoch 0/200] [Batch 3029/3166] [D loss: 1.802912] [G loss: 11.247716] [gradient_penalty: 0.008658]\n",
            "[Epoch 0/200] [Batch 3030/3166] [D loss: 1.072861] [G loss: 11.641958] [gradient_penalty: 0.007896]\n",
            "[Epoch 0/200] [Batch 3031/3166] [D loss: 0.738589] [G loss: 10.858716] [gradient_penalty: 0.004695]\n",
            "[Epoch 0/200] [Batch 3032/3166] [D loss: 1.484447] [G loss: 11.327303] [gradient_penalty: 0.006722]\n",
            "[Epoch 0/200] [Batch 3033/3166] [D loss: 1.632424] [G loss: 11.194236] [gradient_penalty: 0.006722]\n",
            "[Epoch 0/200] [Batch 3034/3166] [D loss: 1.304933] [G loss: 11.664287] [gradient_penalty: 0.008421]\n",
            "[Epoch 0/200] [Batch 3035/3166] [D loss: 0.924075] [G loss: 11.865963] [gradient_penalty: 0.010566]\n",
            "[Epoch 0/200] [Batch 3036/3166] [D loss: 1.619621] [G loss: 12.335590] [gradient_penalty: 0.009071]\n",
            "[Epoch 0/200] [Batch 3037/3166] [D loss: 1.419587] [G loss: 11.675843] [gradient_penalty: 0.011926]\n",
            "[Epoch 0/200] [Batch 3038/3166] [D loss: 1.062654] [G loss: 11.784124] [gradient_penalty: 0.006049]\n",
            "[Epoch 0/200] [Batch 3039/3166] [D loss: 2.224136] [G loss: 11.590470] [gradient_penalty: 0.011010]\n",
            "[Epoch 0/200] [Batch 3040/3166] [D loss: 1.086039] [G loss: 11.438334] [gradient_penalty: 0.010707]\n",
            "[Epoch 0/200] [Batch 3041/3166] [D loss: 1.654815] [G loss: 11.040382] [gradient_penalty: 0.010734]\n",
            "[Epoch 0/200] [Batch 3042/3166] [D loss: 1.403958] [G loss: 11.586992] [gradient_penalty: 0.006855]\n",
            "[Epoch 0/200] [Batch 3043/3166] [D loss: 1.299190] [G loss: 11.449276] [gradient_penalty: 0.009447]\n",
            "[Epoch 0/200] [Batch 3044/3166] [D loss: 2.250246] [G loss: 12.106242] [gradient_penalty: 0.007196]\n",
            "[Epoch 0/200] [Batch 3045/3166] [D loss: 1.157598] [G loss: 11.566916] [gradient_penalty: 0.008979]\n",
            "[Epoch 0/200] [Batch 3046/3166] [D loss: 1.728502] [G loss: 12.159103] [gradient_penalty: 0.012025]\n",
            "[Epoch 0/200] [Batch 3047/3166] [D loss: 0.505739] [G loss: 11.790546] [gradient_penalty: 0.010814]\n",
            "[Epoch 0/200] [Batch 3048/3166] [D loss: 1.935465] [G loss: 11.039774] [gradient_penalty: 0.009797]\n",
            "[Epoch 0/200] [Batch 3049/3166] [D loss: 0.880492] [G loss: 11.509514] [gradient_penalty: 0.007860]\n",
            "[Epoch 0/200] [Batch 3050/3166] [D loss: 1.475294] [G loss: 11.515740] [gradient_penalty: 0.010162]\n",
            "[Epoch 0/200] [Batch 3051/3166] [D loss: 2.098233] [G loss: 11.451740] [gradient_penalty: 0.007619]\n",
            "[Epoch 0/200] [Batch 3052/3166] [D loss: 1.173264] [G loss: 11.690619] [gradient_penalty: 0.007673]\n",
            "[Epoch 0/200] [Batch 3053/3166] [D loss: 1.383793] [G loss: 11.334205] [gradient_penalty: 0.005868]\n",
            "[Epoch 0/200] [Batch 3054/3166] [D loss: 2.227100] [G loss: 11.875978] [gradient_penalty: 0.009751]\n",
            "[Epoch 0/200] [Batch 3055/3166] [D loss: 1.634964] [G loss: 11.485680] [gradient_penalty: 0.010511]\n",
            "[Epoch 0/200] [Batch 3056/3166] [D loss: 1.977027] [G loss: 12.207756] [gradient_penalty: 0.007830]\n",
            "[Epoch 0/200] [Batch 3057/3166] [D loss: 1.263399] [G loss: 11.634691] [gradient_penalty: 0.010869]\n",
            "[Epoch 0/200] [Batch 3058/3166] [D loss: 1.521606] [G loss: 11.529202] [gradient_penalty: 0.012735]\n",
            "[Epoch 0/200] [Batch 3059/3166] [D loss: 1.178760] [G loss: 11.366442] [gradient_penalty: 0.010494]\n",
            "[Epoch 0/200] [Batch 3060/3166] [D loss: 1.264049] [G loss: 11.999806] [gradient_penalty: 0.011122]\n",
            "[Epoch 0/200] [Batch 3061/3166] [D loss: 1.061210] [G loss: 11.836811] [gradient_penalty: 0.012129]\n",
            "[Epoch 0/200] [Batch 3062/3166] [D loss: 1.558458] [G loss: 11.992302] [gradient_penalty: 0.012354]\n",
            "[Epoch 0/200] [Batch 3063/3166] [D loss: 2.432053] [G loss: 12.106827] [gradient_penalty: 0.010192]\n",
            "[Epoch 0/200] [Batch 3064/3166] [D loss: 0.398874] [G loss: 11.495125] [gradient_penalty: 0.009628]\n",
            "[Epoch 0/200] [Batch 3065/3166] [D loss: 1.936699] [G loss: 12.282371] [gradient_penalty: 0.007673]\n",
            "[Epoch 0/200] [Batch 3066/3166] [D loss: 2.339343] [G loss: 12.028934] [gradient_penalty: 0.008514]\n",
            "[Epoch 0/200] [Batch 3067/3166] [D loss: 1.785211] [G loss: 12.133808] [gradient_penalty: 0.009884]\n",
            "[Epoch 0/200] [Batch 3068/3166] [D loss: 1.819081] [G loss: 11.638338] [gradient_penalty: 0.006812]\n",
            "[Epoch 0/200] [Batch 3069/3166] [D loss: 1.934926] [G loss: 11.560461] [gradient_penalty: 0.007854]\n",
            "[Epoch 0/200] [Batch 3070/3166] [D loss: 0.906733] [G loss: 12.030924] [gradient_penalty: 0.008370]\n",
            "[Epoch 0/200] [Batch 3071/3166] [D loss: 0.892051] [G loss: 11.488258] [gradient_penalty: 0.008346]\n",
            "[Epoch 0/200] [Batch 3072/3166] [D loss: 1.655706] [G loss: 11.947685] [gradient_penalty: 0.008451]\n",
            "[Epoch 0/200] [Batch 3073/3166] [D loss: 1.750309] [G loss: 12.153285] [gradient_penalty: 0.007870]\n",
            "[Epoch 0/200] [Batch 3074/3166] [D loss: 1.989382] [G loss: 11.823179] [gradient_penalty: 0.007139]\n",
            "[Epoch 0/200] [Batch 3075/3166] [D loss: 1.449328] [G loss: 11.436132] [gradient_penalty: 0.007014]\n",
            "[Epoch 0/200] [Batch 3076/3166] [D loss: 0.697586] [G loss: 12.806772] [gradient_penalty: 0.007403]\n",
            "[Epoch 0/200] [Batch 3077/3166] [D loss: 2.340102] [G loss: 12.649705] [gradient_penalty: 0.012061]\n",
            "[Epoch 0/200] [Batch 3078/3166] [D loss: 2.444427] [G loss: 11.829550] [gradient_penalty: 0.009598]\n",
            "[Epoch 0/200] [Batch 3079/3166] [D loss: 1.952996] [G loss: 12.070488] [gradient_penalty: 0.012649]\n",
            "[Epoch 0/200] [Batch 3080/3166] [D loss: 2.158113] [G loss: 11.568529] [gradient_penalty: 0.010365]\n",
            "[Epoch 0/200] [Batch 3081/3166] [D loss: 1.747316] [G loss: 11.205818] [gradient_penalty: 0.012740]\n",
            "[Epoch 0/200] [Batch 3082/3166] [D loss: 1.018321] [G loss: 12.576582] [gradient_penalty: 0.009968]\n",
            "[Epoch 0/200] [Batch 3083/3166] [D loss: 1.253033] [G loss: 11.984955] [gradient_penalty: 0.007557]\n",
            "[Epoch 0/200] [Batch 3084/3166] [D loss: 1.739289] [G loss: 11.646065] [gradient_penalty: 0.008608]\n",
            "[Epoch 0/200] [Batch 3085/3166] [D loss: 2.157295] [G loss: 11.660944] [gradient_penalty: 0.009906]\n",
            "[Epoch 0/200] [Batch 3086/3166] [D loss: 0.780341] [G loss: 11.812683] [gradient_penalty: 0.008181]\n",
            "[Epoch 0/200] [Batch 3087/3166] [D loss: 1.644283] [G loss: 12.357475] [gradient_penalty: 0.007373]\n",
            "[Epoch 0/200] [Batch 3088/3166] [D loss: 0.717141] [G loss: 12.714456] [gradient_penalty: 0.008259]\n",
            "[Epoch 0/200] [Batch 3089/3166] [D loss: 1.063927] [G loss: 12.080427] [gradient_penalty: 0.010086]\n",
            "[Epoch 0/200] [Batch 3090/3166] [D loss: 1.103018] [G loss: 12.040303] [gradient_penalty: 0.008947]\n",
            "[Epoch 0/200] [Batch 3091/3166] [D loss: 1.732978] [G loss: 12.127271] [gradient_penalty: 0.009422]\n",
            "[Epoch 0/200] [Batch 3092/3166] [D loss: 2.485791] [G loss: 11.577415] [gradient_penalty: 0.010526]\n",
            "[Epoch 0/200] [Batch 3093/3166] [D loss: 1.545418] [G loss: 12.181021] [gradient_penalty: 0.009049]\n",
            "[Epoch 0/200] [Batch 3094/3166] [D loss: 1.025714] [G loss: 11.572876] [gradient_penalty: 0.009491]\n",
            "[Epoch 0/200] [Batch 3095/3166] [D loss: 0.726585] [G loss: 12.493079] [gradient_penalty: 0.007656]\n",
            "[Epoch 0/200] [Batch 3096/3166] [D loss: 1.296394] [G loss: 11.813916] [gradient_penalty: 0.007609]\n",
            "[Epoch 0/200] [Batch 3097/3166] [D loss: 1.308300] [G loss: 12.212153] [gradient_penalty: 0.010552]\n",
            "[Epoch 0/200] [Batch 3098/3166] [D loss: 1.149989] [G loss: 12.491380] [gradient_penalty: 0.011551]\n",
            "[Epoch 0/200] [Batch 3099/3166] [D loss: 2.389850] [G loss: 12.117527] [gradient_penalty: 0.012173]\n",
            "[Epoch 0/200] [Batch 3100/3166] [D loss: 1.202226] [G loss: 12.104980] [gradient_penalty: 0.008793]\n",
            "log\n",
            "[Epoch 0/200] [Batch 3101/3166] [D loss: 2.111979] [G loss: 12.979213] [gradient_penalty: 0.027136]\n",
            "[Epoch 0/200] [Batch 3102/3166] [D loss: 1.302850] [G loss: 13.198647] [gradient_penalty: 0.008844]\n",
            "[Epoch 0/200] [Batch 3103/3166] [D loss: 1.646046] [G loss: 12.745794] [gradient_penalty: 0.012325]\n",
            "[Epoch 0/200] [Batch 3104/3166] [D loss: 1.572962] [G loss: 11.494843] [gradient_penalty: 0.009949]\n",
            "[Epoch 0/200] [Batch 3105/3166] [D loss: 1.376538] [G loss: 12.377577] [gradient_penalty: 0.008943]\n",
            "[Epoch 0/200] [Batch 3106/3166] [D loss: 1.603144] [G loss: 12.692712] [gradient_penalty: 0.009723]\n",
            "[Epoch 0/200] [Batch 3107/3166] [D loss: 1.568603] [G loss: 12.279688] [gradient_penalty: 0.007744]\n",
            "[Epoch 0/200] [Batch 3108/3166] [D loss: 1.792908] [G loss: 12.691284] [gradient_penalty: 0.009975]\n",
            "[Epoch 0/200] [Batch 3109/3166] [D loss: 0.654305] [G loss: 12.306877] [gradient_penalty: 0.007299]\n",
            "[Epoch 0/200] [Batch 3110/3166] [D loss: 1.505545] [G loss: 13.206594] [gradient_penalty: 0.006019]\n",
            "[Epoch 0/200] [Batch 3111/3166] [D loss: 1.665148] [G loss: 12.133280] [gradient_penalty: 0.006576]\n",
            "[Epoch 0/200] [Batch 3112/3166] [D loss: 1.955007] [G loss: 12.647333] [gradient_penalty: 0.012676]\n",
            "[Epoch 0/200] [Batch 3113/3166] [D loss: 1.639298] [G loss: 12.429403] [gradient_penalty: 0.009179]\n",
            "[Epoch 0/200] [Batch 3114/3166] [D loss: 1.020894] [G loss: 12.498553] [gradient_penalty: 0.009947]\n",
            "[Epoch 0/200] [Batch 3115/3166] [D loss: 1.225058] [G loss: 12.114532] [gradient_penalty: 0.010377]\n",
            "[Epoch 0/200] [Batch 3116/3166] [D loss: 1.521258] [G loss: 13.075802] [gradient_penalty: 0.006478]\n",
            "[Epoch 0/200] [Batch 3117/3166] [D loss: 1.118125] [G loss: 12.917965] [gradient_penalty: 0.007915]\n",
            "[Epoch 0/200] [Batch 3118/3166] [D loss: 1.932819] [G loss: 12.510651] [gradient_penalty: 0.008974]\n",
            "[Epoch 0/200] [Batch 3119/3166] [D loss: 0.326194] [G loss: 13.173412] [gradient_penalty: 0.010113]\n",
            "[Epoch 0/200] [Batch 3120/3166] [D loss: 1.107138] [G loss: 12.960291] [gradient_penalty: 0.012105]\n",
            "[Epoch 0/200] [Batch 3121/3166] [D loss: 1.699182] [G loss: 12.689542] [gradient_penalty: 0.010106]\n",
            "[Epoch 0/200] [Batch 3122/3166] [D loss: 1.590563] [G loss: 12.810886] [gradient_penalty: 0.008534]\n",
            "[Epoch 0/200] [Batch 3123/3166] [D loss: 1.716066] [G loss: 12.310488] [gradient_penalty: 0.012394]\n",
            "[Epoch 0/200] [Batch 3124/3166] [D loss: 2.028166] [G loss: 13.561899] [gradient_penalty: 0.007393]\n",
            "[Epoch 0/200] [Batch 3125/3166] [D loss: 1.743760] [G loss: 12.850005] [gradient_penalty: 0.011336]\n",
            "[Epoch 0/200] [Batch 3126/3166] [D loss: 1.494259] [G loss: 12.056714] [gradient_penalty: 0.009029]\n",
            "[Epoch 0/200] [Batch 3127/3166] [D loss: 1.411506] [G loss: 12.315526] [gradient_penalty: 0.010429]\n",
            "[Epoch 0/200] [Batch 3128/3166] [D loss: 2.375908] [G loss: 12.476524] [gradient_penalty: 0.011422]\n",
            "[Epoch 0/200] [Batch 3129/3166] [D loss: 1.201978] [G loss: 12.820858] [gradient_penalty: 0.008134]\n",
            "[Epoch 0/200] [Batch 3130/3166] [D loss: 2.365520] [G loss: 12.253218] [gradient_penalty: 0.010084]\n",
            "[Epoch 0/200] [Batch 3131/3166] [D loss: 0.686936] [G loss: 12.577157] [gradient_penalty: 0.009994]\n",
            "[Epoch 0/200] [Batch 3132/3166] [D loss: 1.644979] [G loss: 12.235996] [gradient_penalty: 0.010499]\n",
            "[Epoch 0/200] [Batch 3133/3166] [D loss: 1.798159] [G loss: 13.295675] [gradient_penalty: 0.009689]\n",
            "[Epoch 0/200] [Batch 3134/3166] [D loss: 2.127328] [G loss: 11.572683] [gradient_penalty: 0.008335]\n",
            "[Epoch 0/200] [Batch 3135/3166] [D loss: 1.490973] [G loss: 12.489590] [gradient_penalty: 0.009855]\n",
            "[Epoch 0/200] [Batch 3136/3166] [D loss: 1.488807] [G loss: 11.891848] [gradient_penalty: 0.010549]\n",
            "[Epoch 0/200] [Batch 3137/3166] [D loss: 1.140716] [G loss: 12.014002] [gradient_penalty: 0.009790]\n",
            "[Epoch 0/200] [Batch 3138/3166] [D loss: 1.753313] [G loss: 12.857105] [gradient_penalty: 0.007839]\n",
            "[Epoch 0/200] [Batch 3139/3166] [D loss: 1.168836] [G loss: 11.947113] [gradient_penalty: 0.010286]\n",
            "[Epoch 0/200] [Batch 3140/3166] [D loss: 1.920826] [G loss: 12.577099] [gradient_penalty: 0.010557]\n",
            "[Epoch 0/200] [Batch 3141/3166] [D loss: 2.371711] [G loss: 12.890495] [gradient_penalty: 0.014347]\n",
            "[Epoch 0/200] [Batch 3142/3166] [D loss: 2.001266] [G loss: 13.756856] [gradient_penalty: 0.015219]\n",
            "[Epoch 0/200] [Batch 3143/3166] [D loss: 1.071170] [G loss: 12.614537] [gradient_penalty: 0.008761]\n",
            "[Epoch 0/200] [Batch 3144/3166] [D loss: 1.206722] [G loss: 12.915675] [gradient_penalty: 0.010152]\n",
            "[Epoch 0/200] [Batch 3145/3166] [D loss: 0.678389] [G loss: 13.347766] [gradient_penalty: 0.010451]\n",
            "[Epoch 0/200] [Batch 3146/3166] [D loss: 1.215175] [G loss: 12.708096] [gradient_penalty: 0.007803]\n",
            "[Epoch 0/200] [Batch 3147/3166] [D loss: 1.245649] [G loss: 13.348610] [gradient_penalty: 0.009277]\n",
            "[Epoch 0/200] [Batch 3148/3166] [D loss: 1.106089] [G loss: 13.443574] [gradient_penalty: 0.009926]\n",
            "[Epoch 0/200] [Batch 3149/3166] [D loss: 1.750987] [G loss: 13.431181] [gradient_penalty: 0.006497]\n",
            "[Epoch 0/200] [Batch 3150/3166] [D loss: 0.846369] [G loss: 13.081136] [gradient_penalty: 0.009726]\n",
            "[Epoch 0/200] [Batch 3151/3166] [D loss: 1.531687] [G loss: 12.707178] [gradient_penalty: 0.006173]\n",
            "[Epoch 0/200] [Batch 3152/3166] [D loss: 2.414314] [G loss: 12.950995] [gradient_penalty: 0.010127]\n",
            "[Epoch 0/200] [Batch 3153/3166] [D loss: 1.441384] [G loss: 12.510980] [gradient_penalty: 0.008080]\n",
            "[Epoch 0/200] [Batch 3154/3166] [D loss: 1.954271] [G loss: 13.475900] [gradient_penalty: 0.007474]\n",
            "[Epoch 0/200] [Batch 3155/3166] [D loss: 0.334440] [G loss: 12.805874] [gradient_penalty: 0.007521]\n",
            "[Epoch 0/200] [Batch 3156/3166] [D loss: 1.459833] [G loss: 13.006943] [gradient_penalty: 0.008178]\n",
            "[Epoch 0/200] [Batch 3157/3166] [D loss: 1.384649] [G loss: 13.789976] [gradient_penalty: 0.005663]\n",
            "[Epoch 0/200] [Batch 3158/3166] [D loss: 0.851864] [G loss: 12.980631] [gradient_penalty: 0.007705]\n",
            "[Epoch 0/200] [Batch 3159/3166] [D loss: 2.124050] [G loss: 13.376312] [gradient_penalty: 0.008863]\n",
            "[Epoch 0/200] [Batch 3160/3166] [D loss: 1.937213] [G loss: 13.225134] [gradient_penalty: 0.006996]\n",
            "[Epoch 0/200] [Batch 3161/3166] [D loss: 0.753759] [G loss: 13.343510] [gradient_penalty: 0.012479]\n",
            "[Epoch 0/200] [Batch 3162/3166] [D loss: 1.361628] [G loss: 13.222534] [gradient_penalty: 0.007544]\n",
            "[Epoch 0/200] [Batch 3163/3166] [D loss: 2.518853] [G loss: 13.395795] [gradient_penalty: 0.010975]\n",
            "[Epoch 0/200] [Batch 3164/3166] [D loss: 1.874730] [G loss: 14.037051] [gradient_penalty: 0.008104]\n",
            "[Epoch 0/200] [Batch 3165/3166] [D loss: 1.347618] [G loss: 14.287283] [gradient_penalty: 0.008875]\n",
            "[Epoch 1/200] [Batch 0/3166] [D loss: 2.162053] [G loss: 13.858151] [gradient_penalty: 0.007762]\n",
            "[Epoch 1/200] [Batch 1/3166] [D loss: 2.180297] [G loss: 13.679474] [gradient_penalty: 0.010313]\n",
            "[Epoch 1/200] [Batch 2/3166] [D loss: 1.267026] [G loss: 12.430253] [gradient_penalty: 0.010610]\n",
            "[Epoch 1/200] [Batch 3/3166] [D loss: 0.548281] [G loss: 13.653504] [gradient_penalty: 0.010142]\n",
            "[Epoch 1/200] [Batch 4/3166] [D loss: 0.891861] [G loss: 13.275172] [gradient_penalty: 0.007709]\n",
            "[Epoch 1/200] [Batch 5/3166] [D loss: 1.008608] [G loss: 14.006813] [gradient_penalty: 0.006916]\n",
            "[Epoch 1/200] [Batch 6/3166] [D loss: 0.979490] [G loss: 13.310450] [gradient_penalty: 0.009063]\n",
            "[Epoch 1/200] [Batch 7/3166] [D loss: 1.478814] [G loss: 13.435030] [gradient_penalty: 0.005919]\n",
            "[Epoch 1/200] [Batch 8/3166] [D loss: 1.183632] [G loss: 12.941029] [gradient_penalty: 0.008444]\n",
            "[Epoch 1/200] [Batch 9/3166] [D loss: 1.407968] [G loss: 12.981044] [gradient_penalty: 0.008294]\n",
            "[Epoch 1/200] [Batch 10/3166] [D loss: 2.276196] [G loss: 13.218288] [gradient_penalty: 0.007110]\n",
            "[Epoch 1/200] [Batch 11/3166] [D loss: 1.807544] [G loss: 13.489440] [gradient_penalty: 0.009819]\n",
            "[Epoch 1/200] [Batch 12/3166] [D loss: 2.048053] [G loss: 13.587759] [gradient_penalty: 0.009019]\n",
            "[Epoch 1/200] [Batch 13/3166] [D loss: 0.632668] [G loss: 13.328608] [gradient_penalty: 0.009428]\n",
            "[Epoch 1/200] [Batch 14/3166] [D loss: 1.187222] [G loss: 13.055674] [gradient_penalty: 0.013319]\n",
            "[Epoch 1/200] [Batch 15/3166] [D loss: 1.433519] [G loss: 13.004901] [gradient_penalty: 0.009904]\n",
            "[Epoch 1/200] [Batch 16/3166] [D loss: 1.455249] [G loss: 13.222428] [gradient_penalty: 0.005912]\n",
            "[Epoch 1/200] [Batch 17/3166] [D loss: 1.338793] [G loss: 13.120583] [gradient_penalty: 0.009143]\n",
            "[Epoch 1/200] [Batch 18/3166] [D loss: 0.172778] [G loss: 13.180957] [gradient_penalty: 0.007739]\n",
            "[Epoch 1/200] [Batch 19/3166] [D loss: 0.740820] [G loss: 13.867691] [gradient_penalty: 0.008499]\n",
            "[Epoch 1/200] [Batch 20/3166] [D loss: 1.081202] [G loss: 12.971851] [gradient_penalty: 0.010380]\n",
            "[Epoch 1/200] [Batch 21/3166] [D loss: 1.747981] [G loss: 13.273757] [gradient_penalty: 0.006120]\n",
            "[Epoch 1/200] [Batch 22/3166] [D loss: 1.545266] [G loss: 13.166378] [gradient_penalty: 0.008877]\n",
            "[Epoch 1/200] [Batch 23/3166] [D loss: 1.274805] [G loss: 13.497402] [gradient_penalty: 0.007161]\n",
            "[Epoch 1/200] [Batch 24/3166] [D loss: 1.757254] [G loss: 12.442596] [gradient_penalty: 0.009504]\n",
            "[Epoch 1/200] [Batch 25/3166] [D loss: 1.900285] [G loss: 13.289753] [gradient_penalty: 0.008217]\n",
            "[Epoch 1/200] [Batch 26/3166] [D loss: 0.967150] [G loss: 13.127990] [gradient_penalty: 0.010037]\n",
            "[Epoch 1/200] [Batch 27/3166] [D loss: 1.717896] [G loss: 12.979279] [gradient_penalty: 0.009582]\n",
            "[Epoch 1/200] [Batch 28/3166] [D loss: 1.787273] [G loss: 13.115664] [gradient_penalty: 0.006937]\n",
            "[Epoch 1/200] [Batch 29/3166] [D loss: 1.428873] [G loss: 13.071842] [gradient_penalty: 0.006771]\n",
            "[Epoch 1/200] [Batch 30/3166] [D loss: 1.449564] [G loss: 12.472053] [gradient_penalty: 0.010370]\n",
            "[Epoch 1/200] [Batch 31/3166] [D loss: 1.596419] [G loss: 12.215328] [gradient_penalty: 0.007174]\n",
            "[Epoch 1/200] [Batch 32/3166] [D loss: 2.276861] [G loss: 13.142771] [gradient_penalty: 0.008859]\n",
            "[Epoch 1/200] [Batch 33/3166] [D loss: 1.729337] [G loss: 12.923599] [gradient_penalty: 0.009701]\n",
            "[Epoch 1/200] [Batch 34/3166] [D loss: 1.762401] [G loss: 12.906570] [gradient_penalty: 0.012034]\n",
            "log\n",
            "[Epoch 1/200] [Batch 35/3166] [D loss: 1.941469] [G loss: 12.690112] [gradient_penalty: 0.006325]\n",
            "[Epoch 1/200] [Batch 36/3166] [D loss: 0.988429] [G loss: 12.475124] [gradient_penalty: 0.007312]\n",
            "[Epoch 1/200] [Batch 37/3166] [D loss: 1.245713] [G loss: 13.191415] [gradient_penalty: 0.006519]\n",
            "[Epoch 1/200] [Batch 38/3166] [D loss: 1.229596] [G loss: 13.064400] [gradient_penalty: 0.008658]\n",
            "[Epoch 1/200] [Batch 39/3166] [D loss: 1.069325] [G loss: 13.202475] [gradient_penalty: 0.008991]\n",
            "[Epoch 1/200] [Batch 40/3166] [D loss: 1.851712] [G loss: 13.228397] [gradient_penalty: 0.006420]\n",
            "[Epoch 1/200] [Batch 41/3166] [D loss: 2.492743] [G loss: 12.649710] [gradient_penalty: 0.009555]\n",
            "[Epoch 1/200] [Batch 42/3166] [D loss: 0.534301] [G loss: 12.402531] [gradient_penalty: 0.007972]\n",
            "[Epoch 1/200] [Batch 43/3166] [D loss: 1.670978] [G loss: 12.762276] [gradient_penalty: 0.011437]\n",
            "[Epoch 1/200] [Batch 44/3166] [D loss: 1.780118] [G loss: 12.344049] [gradient_penalty: 0.008342]\n",
            "[Epoch 1/200] [Batch 45/3166] [D loss: 1.865636] [G loss: 12.899527] [gradient_penalty: 0.010000]\n",
            "[Epoch 1/200] [Batch 46/3166] [D loss: 1.560552] [G loss: 12.619824] [gradient_penalty: 0.005798]\n",
            "[Epoch 1/200] [Batch 47/3166] [D loss: 2.132659] [G loss: 13.556989] [gradient_penalty: 0.009121]\n",
            "[Epoch 1/200] [Batch 48/3166] [D loss: 1.082643] [G loss: 13.435139] [gradient_penalty: 0.008747]\n",
            "[Epoch 1/200] [Batch 49/3166] [D loss: 2.348852] [G loss: 11.498921] [gradient_penalty: 0.007065]\n",
            "[Epoch 1/200] [Batch 50/3166] [D loss: 2.125747] [G loss: 12.364038] [gradient_penalty: 0.010852]\n",
            "[Epoch 1/200] [Batch 51/3166] [D loss: 1.829892] [G loss: 12.589404] [gradient_penalty: 0.010609]\n",
            "[Epoch 1/200] [Batch 52/3166] [D loss: 1.665126] [G loss: 12.246842] [gradient_penalty: 0.011377]\n",
            "[Epoch 1/200] [Batch 53/3166] [D loss: 0.588115] [G loss: 13.128298] [gradient_penalty: 0.007474]\n",
            "[Epoch 1/200] [Batch 54/3166] [D loss: 1.796087] [G loss: 12.658163] [gradient_penalty: 0.012544]\n",
            "[Epoch 1/200] [Batch 55/3166] [D loss: 2.380889] [G loss: 12.705791] [gradient_penalty: 0.009240]\n",
            "[Epoch 1/200] [Batch 56/3166] [D loss: 1.356804] [G loss: 12.456793] [gradient_penalty: 0.007481]\n",
            "[Epoch 1/200] [Batch 57/3166] [D loss: 2.002741] [G loss: 14.033442] [gradient_penalty: 0.019893]\n",
            "[Epoch 1/200] [Batch 58/3166] [D loss: 1.204578] [G loss: 13.124965] [gradient_penalty: 0.010782]\n",
            "[Epoch 1/200] [Batch 59/3166] [D loss: 0.794973] [G loss: 12.590221] [gradient_penalty: 0.008890]\n",
            "[Epoch 1/200] [Batch 60/3166] [D loss: 2.259227] [G loss: 13.494772] [gradient_penalty: 0.010300]\n",
            "[Epoch 1/200] [Batch 61/3166] [D loss: 1.395079] [G loss: 13.036269] [gradient_penalty: 0.008058]\n",
            "[Epoch 1/200] [Batch 62/3166] [D loss: 2.529255] [G loss: 13.408648] [gradient_penalty: 0.008897]\n",
            "[Epoch 1/200] [Batch 63/3166] [D loss: 1.513064] [G loss: 13.564347] [gradient_penalty: 0.007817]\n",
            "[Epoch 1/200] [Batch 64/3166] [D loss: 1.748676] [G loss: 13.069172] [gradient_penalty: 0.007074]\n",
            "[Epoch 1/200] [Batch 65/3166] [D loss: 1.687242] [G loss: 13.631505] [gradient_penalty: 0.006193]\n",
            "[Epoch 1/200] [Batch 66/3166] [D loss: 0.886357] [G loss: 12.941708] [gradient_penalty: 0.009018]\n",
            "[Epoch 1/200] [Batch 67/3166] [D loss: 1.949151] [G loss: 12.659866] [gradient_penalty: 0.007849]\n",
            "[Epoch 1/200] [Batch 68/3166] [D loss: 0.638970] [G loss: 13.206213] [gradient_penalty: 0.008793]\n",
            "[Epoch 1/200] [Batch 69/3166] [D loss: 1.690340] [G loss: 12.781862] [gradient_penalty: 0.009105]\n",
            "[Epoch 1/200] [Batch 70/3166] [D loss: 1.143088] [G loss: 13.241728] [gradient_penalty: 0.008907]\n",
            "[Epoch 1/200] [Batch 71/3166] [D loss: 1.050401] [G loss: 13.365375] [gradient_penalty: 0.007055]\n",
            "[Epoch 1/200] [Batch 72/3166] [D loss: 1.181241] [G loss: 13.212400] [gradient_penalty: 0.006494]\n",
            "[Epoch 1/200] [Batch 73/3166] [D loss: 1.913602] [G loss: 13.858294] [gradient_penalty: 0.009213]\n",
            "[Epoch 1/200] [Batch 74/3166] [D loss: 1.801916] [G loss: 12.657249] [gradient_penalty: 0.010273]\n",
            "[Epoch 1/200] [Batch 75/3166] [D loss: 0.806804] [G loss: 12.401090] [gradient_penalty: 0.008422]\n",
            "[Epoch 1/200] [Batch 76/3166] [D loss: 1.221539] [G loss: 13.495544] [gradient_penalty: 0.008806]\n",
            "[Epoch 1/200] [Batch 77/3166] [D loss: 0.802116] [G loss: 12.768093] [gradient_penalty: 0.007678]\n",
            "[Epoch 1/200] [Batch 78/3166] [D loss: 1.860224] [G loss: 13.157310] [gradient_penalty: 0.013133]\n",
            "[Epoch 1/200] [Batch 79/3166] [D loss: 1.230409] [G loss: 12.112583] [gradient_penalty: 0.007471]\n",
            "[Epoch 1/200] [Batch 80/3166] [D loss: 1.712512] [G loss: 12.662212] [gradient_penalty: 0.008117]\n",
            "[Epoch 1/200] [Batch 81/3166] [D loss: 1.606062] [G loss: 12.170175] [gradient_penalty: 0.010285]\n",
            "[Epoch 1/200] [Batch 82/3166] [D loss: 0.983013] [G loss: 13.339085] [gradient_penalty: 0.005553]\n",
            "[Epoch 1/200] [Batch 83/3166] [D loss: 1.772164] [G loss: 12.821928] [gradient_penalty: 0.008221]\n",
            "[Epoch 1/200] [Batch 84/3166] [D loss: 1.477432] [G loss: 12.324720] [gradient_penalty: 0.006630]\n",
            "[Epoch 1/200] [Batch 85/3166] [D loss: 1.046354] [G loss: 13.014553] [gradient_penalty: 0.012484]\n",
            "[Epoch 1/200] [Batch 86/3166] [D loss: 0.645238] [G loss: 12.649089] [gradient_penalty: 0.010642]\n",
            "[Epoch 1/200] [Batch 87/3166] [D loss: 1.034881] [G loss: 12.932448] [gradient_penalty: 0.009452]\n",
            "[Epoch 1/200] [Batch 88/3166] [D loss: 1.324070] [G loss: 12.855102] [gradient_penalty: 0.007659]\n",
            "[Epoch 1/200] [Batch 89/3166] [D loss: 1.849153] [G loss: 12.537979] [gradient_penalty: 0.009733]\n",
            "[Epoch 1/200] [Batch 90/3166] [D loss: 1.527320] [G loss: 13.262840] [gradient_penalty: 0.008351]\n",
            "[Epoch 1/200] [Batch 91/3166] [D loss: 0.417623] [G loss: 12.488282] [gradient_penalty: 0.006955]\n",
            "[Epoch 1/200] [Batch 92/3166] [D loss: 0.882510] [G loss: 13.247623] [gradient_penalty: 0.009811]\n",
            "[Epoch 1/200] [Batch 93/3166] [D loss: 1.297154] [G loss: 13.199031] [gradient_penalty: 0.006469]\n",
            "[Epoch 1/200] [Batch 94/3166] [D loss: 2.053717] [G loss: 13.369452] [gradient_penalty: 0.008330]\n",
            "[Epoch 1/200] [Batch 95/3166] [D loss: 2.026571] [G loss: 12.520631] [gradient_penalty: 0.009745]\n",
            "[Epoch 1/200] [Batch 96/3166] [D loss: 1.205075] [G loss: 12.717546] [gradient_penalty: 0.006331]\n",
            "[Epoch 1/200] [Batch 97/3166] [D loss: 1.453204] [G loss: 13.283900] [gradient_penalty: 0.007830]\n",
            "[Epoch 1/200] [Batch 98/3166] [D loss: 2.062672] [G loss: 13.153320] [gradient_penalty: 0.010884]\n",
            "[Epoch 1/200] [Batch 99/3166] [D loss: 1.123401] [G loss: 13.182865] [gradient_penalty: 0.009055]\n",
            "[Epoch 1/200] [Batch 100/3166] [D loss: 1.425488] [G loss: 12.892239] [gradient_penalty: 0.007687]\n",
            "[Epoch 1/200] [Batch 101/3166] [D loss: 1.848636] [G loss: 13.046838] [gradient_penalty: 0.008165]\n",
            "[Epoch 1/200] [Batch 102/3166] [D loss: 1.348704] [G loss: 12.751001] [gradient_penalty: 0.010243]\n",
            "[Epoch 1/200] [Batch 103/3166] [D loss: 1.142847] [G loss: 12.831562] [gradient_penalty: 0.008073]\n",
            "[Epoch 1/200] [Batch 104/3166] [D loss: 1.397998] [G loss: 14.100780] [gradient_penalty: 0.005149]\n",
            "[Epoch 1/200] [Batch 105/3166] [D loss: 0.999927] [G loss: 13.582273] [gradient_penalty: 0.009356]\n",
            "[Epoch 1/200] [Batch 106/3166] [D loss: 1.988238] [G loss: 12.949112] [gradient_penalty: 0.010876]\n",
            "[Epoch 1/200] [Batch 107/3166] [D loss: 1.898529] [G loss: 13.121731] [gradient_penalty: 0.007369]\n",
            "[Epoch 1/200] [Batch 108/3166] [D loss: 1.499959] [G loss: 13.311417] [gradient_penalty: 0.008130]\n",
            "[Epoch 1/200] [Batch 109/3166] [D loss: 1.418178] [G loss: 13.869801] [gradient_penalty: 0.009472]\n",
            "[Epoch 1/200] [Batch 110/3166] [D loss: 0.528557] [G loss: 13.388729] [gradient_penalty: 0.008873]\n",
            "[Epoch 1/200] [Batch 111/3166] [D loss: 1.307522] [G loss: 13.572829] [gradient_penalty: 0.012990]\n",
            "[Epoch 1/200] [Batch 112/3166] [D loss: 1.824445] [G loss: 14.081823] [gradient_penalty: 0.010346]\n",
            "[Epoch 1/200] [Batch 113/3166] [D loss: 0.977311] [G loss: 13.663906] [gradient_penalty: 0.012781]\n",
            "[Epoch 1/200] [Batch 114/3166] [D loss: 1.426577] [G loss: 13.354218] [gradient_penalty: 0.012213]\n",
            "[Epoch 1/200] [Batch 115/3166] [D loss: 0.747910] [G loss: 12.619186] [gradient_penalty: 0.011397]\n",
            "[Epoch 1/200] [Batch 116/3166] [D loss: 1.546684] [G loss: 12.466866] [gradient_penalty: 0.013577]\n",
            "[Epoch 1/200] [Batch 117/3166] [D loss: 2.167389] [G loss: 13.060361] [gradient_penalty: 0.008047]\n",
            "[Epoch 1/200] [Batch 118/3166] [D loss: 1.766097] [G loss: 13.454008] [gradient_penalty: 0.009442]\n",
            "[Epoch 1/200] [Batch 119/3166] [D loss: 1.546416] [G loss: 13.842666] [gradient_penalty: 0.008863]\n",
            "[Epoch 1/200] [Batch 120/3166] [D loss: 1.109379] [G loss: 13.215122] [gradient_penalty: 0.009791]\n",
            "[Epoch 1/200] [Batch 121/3166] [D loss: 1.526447] [G loss: 13.379464] [gradient_penalty: 0.008735]\n",
            "[Epoch 1/200] [Batch 122/3166] [D loss: 1.824898] [G loss: 12.554825] [gradient_penalty: 0.005502]\n",
            "[Epoch 1/200] [Batch 123/3166] [D loss: 0.630970] [G loss: 12.534262] [gradient_penalty: 0.009862]\n",
            "[Epoch 1/200] [Batch 124/3166] [D loss: 0.503769] [G loss: 12.765041] [gradient_penalty: 0.008891]\n",
            "[Epoch 1/200] [Batch 125/3166] [D loss: 1.875344] [G loss: 13.359865] [gradient_penalty: 0.008037]\n",
            "[Epoch 1/200] [Batch 126/3166] [D loss: 1.677853] [G loss: 13.133812] [gradient_penalty: 0.014936]\n",
            "[Epoch 1/200] [Batch 127/3166] [D loss: 1.911409] [G loss: 13.121727] [gradient_penalty: 0.011392]\n",
            "[Epoch 1/200] [Batch 128/3166] [D loss: 1.613776] [G loss: 14.090512] [gradient_penalty: 0.008681]\n",
            "[Epoch 1/200] [Batch 129/3166] [D loss: 1.883602] [G loss: 13.964390] [gradient_penalty: 0.008329]\n",
            "[Epoch 1/200] [Batch 130/3166] [D loss: 1.360775] [G loss: 14.509857] [gradient_penalty: 0.009374]\n",
            "[Epoch 1/200] [Batch 131/3166] [D loss: 2.454425] [G loss: 13.500370] [gradient_penalty: 0.011799]\n",
            "[Epoch 1/200] [Batch 132/3166] [D loss: 1.210968] [G loss: 13.429127] [gradient_penalty: 0.006494]\n",
            "[Epoch 1/200] [Batch 133/3166] [D loss: 2.085411] [G loss: 12.672975] [gradient_penalty: 0.011554]\n",
            "[Epoch 1/200] [Batch 134/3166] [D loss: 1.580973] [G loss: 12.528440] [gradient_penalty: 0.005847]\n",
            "log\n",
            "[Epoch 1/200] [Batch 135/3166] [D loss: 1.705135] [G loss: 13.274767] [gradient_penalty: 0.007846]\n",
            "[Epoch 1/200] [Batch 136/3166] [D loss: 1.047771] [G loss: 13.312703] [gradient_penalty: 0.007134]\n",
            "[Epoch 1/200] [Batch 137/3166] [D loss: 1.320847] [G loss: 14.103594] [gradient_penalty: 0.006725]\n",
            "[Epoch 1/200] [Batch 138/3166] [D loss: 1.514846] [G loss: 13.308584] [gradient_penalty: 0.007562]\n",
            "[Epoch 1/200] [Batch 139/3166] [D loss: 1.192127] [G loss: 12.668038] [gradient_penalty: 0.007475]\n",
            "[Epoch 1/200] [Batch 140/3166] [D loss: 1.902008] [G loss: 13.519502] [gradient_penalty: 0.009470]\n",
            "[Epoch 1/200] [Batch 141/3166] [D loss: 1.250389] [G loss: 13.091850] [gradient_penalty: 0.010071]\n",
            "[Epoch 1/200] [Batch 142/3166] [D loss: 0.640994] [G loss: 13.063126] [gradient_penalty: 0.008770]\n",
            "[Epoch 1/200] [Batch 143/3166] [D loss: 1.809945] [G loss: 13.514370] [gradient_penalty: 0.009713]\n",
            "[Epoch 1/200] [Batch 144/3166] [D loss: 0.925796] [G loss: 12.833276] [gradient_penalty: 0.006833]\n",
            "[Epoch 1/200] [Batch 145/3166] [D loss: 1.590530] [G loss: 13.368073] [gradient_penalty: 0.008612]\n",
            "[Epoch 1/200] [Batch 146/3166] [D loss: 1.349994] [G loss: 14.309780] [gradient_penalty: 0.009197]\n",
            "[Epoch 1/200] [Batch 147/3166] [D loss: 1.954982] [G loss: 14.532621] [gradient_penalty: 0.009279]\n",
            "[Epoch 1/200] [Batch 148/3166] [D loss: 0.873266] [G loss: 13.475531] [gradient_penalty: 0.010405]\n",
            "[Epoch 1/200] [Batch 149/3166] [D loss: 0.346297] [G loss: 14.196201] [gradient_penalty: 0.009811]\n",
            "[Epoch 1/200] [Batch 150/3166] [D loss: 1.324545] [G loss: 14.005079] [gradient_penalty: 0.010731]\n",
            "[Epoch 1/200] [Batch 151/3166] [D loss: 1.166597] [G loss: 13.722851] [gradient_penalty: 0.010207]\n",
            "[Epoch 1/200] [Batch 152/3166] [D loss: 1.600277] [G loss: 13.705416] [gradient_penalty: 0.008361]\n",
            "[Epoch 1/200] [Batch 153/3166] [D loss: 1.547295] [G loss: 13.588950] [gradient_penalty: 0.010778]\n",
            "[Epoch 1/200] [Batch 154/3166] [D loss: 1.695809] [G loss: 13.282209] [gradient_penalty: 0.006817]\n",
            "[Epoch 1/200] [Batch 155/3166] [D loss: 0.732787] [G loss: 13.838834] [gradient_penalty: 0.008373]\n",
            "[Epoch 1/200] [Batch 156/3166] [D loss: 1.651829] [G loss: 14.439114] [gradient_penalty: 0.008832]\n",
            "[Epoch 1/200] [Batch 157/3166] [D loss: 1.259199] [G loss: 14.050768] [gradient_penalty: 0.009868]\n",
            "[Epoch 1/200] [Batch 158/3166] [D loss: 0.584901] [G loss: 13.948352] [gradient_penalty: 0.007298]\n",
            "[Epoch 1/200] [Batch 159/3166] [D loss: 1.093831] [G loss: 13.517540] [gradient_penalty: 0.008319]\n",
            "[Epoch 1/200] [Batch 160/3166] [D loss: 1.962949] [G loss: 12.615656] [gradient_penalty: 0.007435]\n",
            "[Epoch 1/200] [Batch 161/3166] [D loss: 0.863373] [G loss: 13.785401] [gradient_penalty: 0.009715]\n",
            "[Epoch 1/200] [Batch 162/3166] [D loss: 1.039302] [G loss: 13.211660] [gradient_penalty: 0.008993]\n",
            "[Epoch 1/200] [Batch 163/3166] [D loss: 1.359118] [G loss: 13.534595] [gradient_penalty: 0.010481]\n",
            "[Epoch 1/200] [Batch 164/3166] [D loss: 1.391362] [G loss: 13.504271] [gradient_penalty: 0.010780]\n",
            "[Epoch 1/200] [Batch 165/3166] [D loss: 2.088786] [G loss: 13.533866] [gradient_penalty: 0.010857]\n",
            "[Epoch 1/200] [Batch 166/3166] [D loss: 1.366763] [G loss: 14.016272] [gradient_penalty: 0.010347]\n",
            "[Epoch 1/200] [Batch 167/3166] [D loss: -0.216484] [G loss: 13.370100] [gradient_penalty: 0.007901]\n",
            "[Epoch 1/200] [Batch 168/3166] [D loss: 2.181346] [G loss: 14.321507] [gradient_penalty: 0.011859]\n",
            "[Epoch 1/200] [Batch 169/3166] [D loss: 0.636978] [G loss: 13.941494] [gradient_penalty: 0.009680]\n",
            "[Epoch 1/200] [Batch 170/3166] [D loss: 0.639936] [G loss: 14.431520] [gradient_penalty: 0.009171]\n",
            "[Epoch 1/200] [Batch 171/3166] [D loss: 1.408876] [G loss: 14.127263] [gradient_penalty: 0.009680]\n",
            "[Epoch 1/200] [Batch 172/3166] [D loss: 1.085554] [G loss: 13.598444] [gradient_penalty: 0.014031]\n",
            "[Epoch 1/200] [Batch 173/3166] [D loss: 2.243455] [G loss: 13.957987] [gradient_penalty: 0.010705]\n",
            "[Epoch 1/200] [Batch 174/3166] [D loss: 2.013112] [G loss: 14.205221] [gradient_penalty: 0.014324]\n",
            "[Epoch 1/200] [Batch 175/3166] [D loss: 1.084514] [G loss: 14.713680] [gradient_penalty: 0.009706]\n",
            "[Epoch 1/200] [Batch 176/3166] [D loss: 0.631208] [G loss: 13.100561] [gradient_penalty: 0.007360]\n",
            "[Epoch 1/200] [Batch 177/3166] [D loss: 0.809877] [G loss: 13.450112] [gradient_penalty: 0.009522]\n",
            "[Epoch 1/200] [Batch 178/3166] [D loss: 1.406289] [G loss: 13.143183] [gradient_penalty: 0.007752]\n",
            "[Epoch 1/200] [Batch 179/3166] [D loss: 1.278461] [G loss: 14.067675] [gradient_penalty: 0.007276]\n",
            "[Epoch 1/200] [Batch 180/3166] [D loss: 1.616275] [G loss: 13.045794] [gradient_penalty: 0.009183]\n",
            "[Epoch 1/200] [Batch 181/3166] [D loss: 1.960212] [G loss: 13.566225] [gradient_penalty: 0.012596]\n",
            "[Epoch 1/200] [Batch 182/3166] [D loss: 2.144525] [G loss: 13.486645] [gradient_penalty: 0.009594]\n",
            "[Epoch 1/200] [Batch 183/3166] [D loss: 0.917460] [G loss: 13.984791] [gradient_penalty: 0.009101]\n",
            "[Epoch 1/200] [Batch 184/3166] [D loss: 1.636144] [G loss: 13.300323] [gradient_penalty: 0.009396]\n",
            "[Epoch 1/200] [Batch 185/3166] [D loss: 1.397690] [G loss: 13.743748] [gradient_penalty: 0.009467]\n",
            "[Epoch 1/200] [Batch 186/3166] [D loss: 1.320817] [G loss: 13.451466] [gradient_penalty: 0.008437]\n",
            "[Epoch 1/200] [Batch 187/3166] [D loss: 1.340296] [G loss: 13.597517] [gradient_penalty: 0.007724]\n",
            "[Epoch 1/200] [Batch 188/3166] [D loss: 0.977410] [G loss: 13.510206] [gradient_penalty: 0.008436]\n",
            "[Epoch 1/200] [Batch 189/3166] [D loss: 1.415154] [G loss: 14.662059] [gradient_penalty: 0.007165]\n",
            "[Epoch 1/200] [Batch 190/3166] [D loss: 1.119856] [G loss: 14.733404] [gradient_penalty: 0.006575]\n",
            "[Epoch 1/200] [Batch 191/3166] [D loss: 0.730772] [G loss: 14.008329] [gradient_penalty: 0.007373]\n",
            "[Epoch 1/200] [Batch 192/3166] [D loss: 0.863908] [G loss: 14.492963] [gradient_penalty: 0.007909]\n",
            "[Epoch 1/200] [Batch 193/3166] [D loss: 1.166345] [G loss: 14.201998] [gradient_penalty: 0.011006]\n",
            "[Epoch 1/200] [Batch 194/3166] [D loss: 1.499673] [G loss: 14.925636] [gradient_penalty: 0.011734]\n",
            "[Epoch 1/200] [Batch 195/3166] [D loss: 1.484864] [G loss: 14.528013] [gradient_penalty: 0.011877]\n",
            "[Epoch 1/200] [Batch 196/3166] [D loss: 1.123590] [G loss: 14.419257] [gradient_penalty: 0.008592]\n",
            "[Epoch 1/200] [Batch 197/3166] [D loss: 2.156225] [G loss: 13.879365] [gradient_penalty: 0.007232]\n",
            "[Epoch 1/200] [Batch 198/3166] [D loss: 1.543788] [G loss: 14.039860] [gradient_penalty: 0.007202]\n",
            "[Epoch 1/200] [Batch 199/3166] [D loss: 1.433391] [G loss: 13.754616] [gradient_penalty: 0.006362]\n",
            "[Epoch 1/200] [Batch 200/3166] [D loss: 0.876871] [G loss: 14.050544] [gradient_penalty: 0.008248]\n",
            "[Epoch 1/200] [Batch 201/3166] [D loss: 2.757596] [G loss: 14.675156] [gradient_penalty: 0.009969]\n",
            "[Epoch 1/200] [Batch 202/3166] [D loss: 1.778944] [G loss: 13.963554] [gradient_penalty: 0.009513]\n",
            "[Epoch 1/200] [Batch 203/3166] [D loss: 1.629932] [G loss: 13.742261] [gradient_penalty: 0.009054]\n",
            "[Epoch 1/200] [Batch 204/3166] [D loss: 1.612453] [G loss: 12.946318] [gradient_penalty: 0.009268]\n",
            "[Epoch 1/200] [Batch 205/3166] [D loss: 0.678989] [G loss: 13.789293] [gradient_penalty: 0.010138]\n",
            "[Epoch 1/200] [Batch 206/3166] [D loss: 1.210061] [G loss: 13.582048] [gradient_penalty: 0.008831]\n",
            "[Epoch 1/200] [Batch 207/3166] [D loss: 1.626426] [G loss: 13.337061] [gradient_penalty: 0.007455]\n",
            "[Epoch 1/200] [Batch 208/3166] [D loss: 0.481165] [G loss: 12.593691] [gradient_penalty: 0.007676]\n",
            "[Epoch 1/200] [Batch 209/3166] [D loss: 1.391226] [G loss: 13.211736] [gradient_penalty: 0.008434]\n",
            "[Epoch 1/200] [Batch 210/3166] [D loss: 1.241841] [G loss: 13.209151] [gradient_penalty: 0.010051]\n",
            "[Epoch 1/200] [Batch 211/3166] [D loss: 2.249924] [G loss: 13.182770] [gradient_penalty: 0.010075]\n",
            "[Epoch 1/200] [Batch 212/3166] [D loss: 1.762448] [G loss: 13.125132] [gradient_penalty: 0.008813]\n",
            "[Epoch 1/200] [Batch 213/3166] [D loss: 1.023918] [G loss: 13.163467] [gradient_penalty: 0.009272]\n",
            "[Epoch 1/200] [Batch 214/3166] [D loss: 1.668824] [G loss: 13.697745] [gradient_penalty: 0.011159]\n",
            "[Epoch 1/200] [Batch 215/3166] [D loss: 0.463464] [G loss: 13.321657] [gradient_penalty: 0.008078]\n",
            "[Epoch 1/200] [Batch 216/3166] [D loss: 1.624266] [G loss: 13.334447] [gradient_penalty: 0.012119]\n",
            "[Epoch 1/200] [Batch 217/3166] [D loss: 1.402768] [G loss: 13.664535] [gradient_penalty: 0.010780]\n",
            "[Epoch 1/200] [Batch 218/3166] [D loss: 1.741490] [G loss: 12.677996] [gradient_penalty: 0.009000]\n",
            "[Epoch 1/200] [Batch 219/3166] [D loss: 1.175077] [G loss: 13.386353] [gradient_penalty: 0.007509]\n",
            "[Epoch 1/200] [Batch 220/3166] [D loss: 1.001969] [G loss: 13.146530] [gradient_penalty: 0.008311]\n",
            "[Epoch 1/200] [Batch 221/3166] [D loss: 2.139074] [G loss: 12.439356] [gradient_penalty: 0.009596]\n",
            "[Epoch 1/200] [Batch 222/3166] [D loss: 1.222331] [G loss: 12.488235] [gradient_penalty: 0.009134]\n",
            "[Epoch 1/200] [Batch 223/3166] [D loss: 1.743018] [G loss: 12.397733] [gradient_penalty: 0.009323]\n",
            "[Epoch 1/200] [Batch 224/3166] [D loss: 0.540244] [G loss: 12.596825] [gradient_penalty: 0.007938]\n",
            "[Epoch 1/200] [Batch 225/3166] [D loss: 0.996331] [G loss: 12.857462] [gradient_penalty: 0.007531]\n",
            "[Epoch 1/200] [Batch 226/3166] [D loss: 1.019694] [G loss: 12.870476] [gradient_penalty: 0.010623]\n",
            "[Epoch 1/200] [Batch 227/3166] [D loss: 0.019994] [G loss: 12.720362] [gradient_penalty: 0.009179]\n",
            "[Epoch 1/200] [Batch 228/3166] [D loss: 0.562139] [G loss: 13.314619] [gradient_penalty: 0.008212]\n",
            "[Epoch 1/200] [Batch 229/3166] [D loss: 2.190141] [G loss: 12.584114] [gradient_penalty: 0.009833]\n",
            "[Epoch 1/200] [Batch 230/3166] [D loss: 1.014364] [G loss: 13.368383] [gradient_penalty: 0.011601]\n",
            "[Epoch 1/200] [Batch 231/3166] [D loss: 2.051813] [G loss: 12.357294] [gradient_penalty: 0.006857]\n",
            "[Epoch 1/200] [Batch 232/3166] [D loss: 1.976524] [G loss: 12.477696] [gradient_penalty: 0.007234]\n",
            "[Epoch 1/200] [Batch 233/3166] [D loss: 1.694662] [G loss: 12.096205] [gradient_penalty: 0.010623]\n",
            "[Epoch 1/200] [Batch 234/3166] [D loss: 1.419865] [G loss: 12.676302] [gradient_penalty: 0.007137]\n",
            "log\n",
            "[Epoch 1/200] [Batch 235/3166] [D loss: 1.372505] [G loss: 12.141867] [gradient_penalty: 0.006491]\n",
            "[Epoch 1/200] [Batch 236/3166] [D loss: 1.644161] [G loss: 12.548648] [gradient_penalty: 0.007805]\n",
            "[Epoch 1/200] [Batch 237/3166] [D loss: 1.743935] [G loss: 12.304264] [gradient_penalty: 0.009618]\n",
            "[Epoch 1/200] [Batch 238/3166] [D loss: 1.521917] [G loss: 12.274485] [gradient_penalty: 0.010574]\n",
            "[Epoch 1/200] [Batch 239/3166] [D loss: 0.812314] [G loss: 13.149649] [gradient_penalty: 0.011720]\n",
            "[Epoch 1/200] [Batch 240/3166] [D loss: 1.457147] [G loss: 13.845490] [gradient_penalty: 0.006950]\n",
            "[Epoch 1/200] [Batch 241/3166] [D loss: 1.340900] [G loss: 13.583523] [gradient_penalty: 0.011092]\n",
            "[Epoch 1/200] [Batch 242/3166] [D loss: 0.964671] [G loss: 13.264204] [gradient_penalty: 0.011847]\n",
            "[Epoch 1/200] [Batch 243/3166] [D loss: 1.338867] [G loss: 13.048508] [gradient_penalty: 0.009305]\n",
            "[Epoch 1/200] [Batch 244/3166] [D loss: 1.387577] [G loss: 13.718590] [gradient_penalty: 0.015948]\n",
            "[Epoch 1/200] [Batch 245/3166] [D loss: 2.247701] [G loss: 12.920709] [gradient_penalty: 0.012822]\n",
            "[Epoch 1/200] [Batch 246/3166] [D loss: -0.394622] [G loss: 12.163731] [gradient_penalty: 0.010817]\n",
            "[Epoch 1/200] [Batch 247/3166] [D loss: 1.062084] [G loss: 13.338284] [gradient_penalty: 0.010388]\n",
            "[Epoch 1/200] [Batch 248/3166] [D loss: 2.310965] [G loss: 12.795714] [gradient_penalty: 0.009387]\n",
            "[Epoch 1/200] [Batch 249/3166] [D loss: 1.366070] [G loss: 14.280003] [gradient_penalty: 0.011155]\n",
            "[Epoch 1/200] [Batch 250/3166] [D loss: 1.866583] [G loss: 13.465586] [gradient_penalty: 0.007627]\n",
            "[Epoch 1/200] [Batch 251/3166] [D loss: 1.302539] [G loss: 12.609455] [gradient_penalty: 0.009791]\n",
            "[Epoch 1/200] [Batch 252/3166] [D loss: 1.846809] [G loss: 12.975254] [gradient_penalty: 0.008042]\n",
            "[Epoch 1/200] [Batch 253/3166] [D loss: 1.449602] [G loss: 12.734217] [gradient_penalty: 0.008111]\n",
            "[Epoch 1/200] [Batch 254/3166] [D loss: 1.541594] [G loss: 12.945757] [gradient_penalty: 0.009020]\n",
            "[Epoch 1/200] [Batch 255/3166] [D loss: 0.756088] [G loss: 12.676141] [gradient_penalty: 0.009101]\n",
            "[Epoch 1/200] [Batch 256/3166] [D loss: 1.187551] [G loss: 12.044281] [gradient_penalty: 0.009229]\n",
            "[Epoch 1/200] [Batch 257/3166] [D loss: 2.505614] [G loss: 12.117526] [gradient_penalty: 0.008778]\n",
            "[Epoch 1/200] [Batch 258/3166] [D loss: 1.696193] [G loss: 11.237555] [gradient_penalty: 0.006575]\n",
            "[Epoch 1/200] [Batch 259/3166] [D loss: 0.645663] [G loss: 12.407215] [gradient_penalty: 0.007500]\n",
            "[Epoch 1/200] [Batch 260/3166] [D loss: 1.699094] [G loss: 12.137804] [gradient_penalty: 0.007303]\n",
            "[Epoch 1/200] [Batch 261/3166] [D loss: 1.416818] [G loss: 12.221846] [gradient_penalty: 0.008620]\n",
            "[Epoch 1/200] [Batch 262/3166] [D loss: 1.203349] [G loss: 13.632525] [gradient_penalty: 0.008032]\n",
            "[Epoch 1/200] [Batch 263/3166] [D loss: 0.179095] [G loss: 14.004551] [gradient_penalty: 0.008449]\n",
            "[Epoch 1/200] [Batch 264/3166] [D loss: 1.015790] [G loss: 12.367463] [gradient_penalty: 0.009150]\n",
            "[Epoch 1/200] [Batch 265/3166] [D loss: 0.132966] [G loss: 13.933270] [gradient_penalty: 0.009369]\n",
            "[Epoch 1/200] [Batch 266/3166] [D loss: 0.116184] [G loss: 12.745449] [gradient_penalty: 0.010680]\n",
            "[Epoch 1/200] [Batch 267/3166] [D loss: 1.488298] [G loss: 13.273480] [gradient_penalty: 0.009017]\n",
            "[Epoch 1/200] [Batch 268/3166] [D loss: 1.376574] [G loss: 13.116510] [gradient_penalty: 0.014165]\n",
            "[Epoch 1/200] [Batch 269/3166] [D loss: 0.792873] [G loss: 14.064697] [gradient_penalty: 0.012478]\n",
            "[Epoch 1/200] [Batch 270/3166] [D loss: 2.130871] [G loss: 13.125183] [gradient_penalty: 0.011101]\n",
            "[Epoch 1/200] [Batch 271/3166] [D loss: 1.778399] [G loss: 12.419688] [gradient_penalty: 0.012746]\n",
            "[Epoch 1/200] [Batch 272/3166] [D loss: 1.356012] [G loss: 13.232630] [gradient_penalty: 0.010942]\n",
            "[Epoch 1/200] [Batch 273/3166] [D loss: 2.123525] [G loss: 14.025267] [gradient_penalty: 0.012291]\n",
            "[Epoch 1/200] [Batch 274/3166] [D loss: 1.242077] [G loss: 13.633068] [gradient_penalty: 0.008088]\n",
            "[Epoch 1/200] [Batch 275/3166] [D loss: 1.151712] [G loss: 13.763622] [gradient_penalty: 0.007052]\n",
            "[Epoch 1/200] [Batch 276/3166] [D loss: 0.993377] [G loss: 13.314072] [gradient_penalty: 0.007333]\n",
            "[Epoch 1/200] [Batch 277/3166] [D loss: 0.926494] [G loss: 14.053477] [gradient_penalty: 0.011413]\n",
            "[Epoch 1/200] [Batch 278/3166] [D loss: 0.071515] [G loss: 13.316698] [gradient_penalty: 0.009663]\n",
            "[Epoch 1/200] [Batch 279/3166] [D loss: 0.606668] [G loss: 12.791298] [gradient_penalty: 0.008508]\n",
            "[Epoch 1/200] [Batch 280/3166] [D loss: 2.173113] [G loss: 12.593672] [gradient_penalty: 0.008809]\n",
            "[Epoch 1/200] [Batch 281/3166] [D loss: 0.915531] [G loss: 12.464853] [gradient_penalty: 0.009725]\n",
            "[Epoch 1/200] [Batch 282/3166] [D loss: 1.633751] [G loss: 12.786579] [gradient_penalty: 0.009427]\n",
            "[Epoch 1/200] [Batch 283/3166] [D loss: 1.401768] [G loss: 12.490519] [gradient_penalty: 0.010765]\n",
            "[Epoch 1/200] [Batch 284/3166] [D loss: 1.073897] [G loss: 12.662193] [gradient_penalty: 0.006029]\n",
            "[Epoch 1/200] [Batch 285/3166] [D loss: 2.364405] [G loss: 12.710578] [gradient_penalty: 0.010405]\n",
            "[Epoch 1/200] [Batch 286/3166] [D loss: 1.542297] [G loss: 13.002033] [gradient_penalty: 0.009772]\n",
            "[Epoch 1/200] [Batch 287/3166] [D loss: 1.212858] [G loss: 13.848738] [gradient_penalty: 0.009056]\n",
            "[Epoch 1/200] [Batch 288/3166] [D loss: 1.912609] [G loss: 14.436006] [gradient_penalty: 0.010196]\n",
            "[Epoch 1/200] [Batch 289/3166] [D loss: 0.814461] [G loss: 14.413055] [gradient_penalty: 0.009463]\n",
            "[Epoch 1/200] [Batch 290/3166] [D loss: 0.441303] [G loss: 15.165380] [gradient_penalty: 0.009990]\n",
            "[Epoch 1/200] [Batch 291/3166] [D loss: 2.399689] [G loss: 14.665077] [gradient_penalty: 0.009138]\n",
            "[Epoch 1/200] [Batch 292/3166] [D loss: 2.158306] [G loss: 13.875006] [gradient_penalty: 0.009474]\n",
            "[Epoch 1/200] [Batch 293/3166] [D loss: 0.913153] [G loss: 13.429777] [gradient_penalty: 0.010831]\n",
            "[Epoch 1/200] [Batch 294/3166] [D loss: 0.119688] [G loss: 13.609644] [gradient_penalty: 0.014781]\n",
            "[Epoch 1/200] [Batch 295/3166] [D loss: 1.407094] [G loss: 14.193223] [gradient_penalty: 0.011789]\n",
            "[Epoch 1/200] [Batch 296/3166] [D loss: 2.690619] [G loss: 13.136154] [gradient_penalty: 0.010244]\n",
            "[Epoch 1/200] [Batch 297/3166] [D loss: 2.696005] [G loss: 13.017433] [gradient_penalty: 0.009754]\n",
            "[Epoch 1/200] [Batch 298/3166] [D loss: 2.756529] [G loss: 13.708872] [gradient_penalty: 0.011327]\n",
            "[Epoch 1/200] [Batch 299/3166] [D loss: 1.375437] [G loss: 14.515882] [gradient_penalty: 0.011735]\n",
            "[Epoch 1/200] [Batch 300/3166] [D loss: 0.538677] [G loss: 12.791206] [gradient_penalty: 0.009300]\n",
            "[Epoch 1/200] [Batch 301/3166] [D loss: 1.041292] [G loss: 13.497910] [gradient_penalty: 0.008809]\n",
            "[Epoch 1/200] [Batch 302/3166] [D loss: -0.032337] [G loss: 13.073881] [gradient_penalty: 0.009523]\n",
            "[Epoch 1/200] [Batch 303/3166] [D loss: 0.709401] [G loss: 13.496635] [gradient_penalty: 0.006124]\n",
            "[Epoch 1/200] [Batch 304/3166] [D loss: 1.267660] [G loss: 12.839768] [gradient_penalty: 0.010634]\n",
            "[Epoch 1/200] [Batch 305/3166] [D loss: 1.165779] [G loss: 13.545437] [gradient_penalty: 0.012567]\n",
            "[Epoch 1/200] [Batch 306/3166] [D loss: 0.911228] [G loss: 14.496466] [gradient_penalty: 0.013246]\n",
            "[Epoch 1/200] [Batch 307/3166] [D loss: 1.077360] [G loss: 13.375451] [gradient_penalty: 0.008944]\n",
            "[Epoch 1/200] [Batch 308/3166] [D loss: 0.965583] [G loss: 12.785498] [gradient_penalty: 0.009700]\n",
            "[Epoch 1/200] [Batch 309/3166] [D loss: 2.217859] [G loss: 11.999170] [gradient_penalty: 0.010292]\n",
            "[Epoch 1/200] [Batch 310/3166] [D loss: 1.235782] [G loss: 12.200933] [gradient_penalty: 0.007899]\n",
            "[Epoch 1/200] [Batch 311/3166] [D loss: 2.277215] [G loss: 11.997597] [gradient_penalty: 0.008387]\n",
            "[Epoch 1/200] [Batch 312/3166] [D loss: 1.839747] [G loss: 12.268372] [gradient_penalty: 0.008476]\n",
            "[Epoch 1/200] [Batch 313/3166] [D loss: 1.904213] [G loss: 12.896422] [gradient_penalty: 0.009322]\n",
            "[Epoch 1/200] [Batch 314/3166] [D loss: 0.060471] [G loss: 12.660929] [gradient_penalty: 0.005426]\n",
            "[Epoch 1/200] [Batch 315/3166] [D loss: -0.068075] [G loss: 13.797756] [gradient_penalty: 0.007644]\n",
            "[Epoch 1/200] [Batch 316/3166] [D loss: 0.290904] [G loss: 14.094687] [gradient_penalty: 0.009562]\n",
            "[Epoch 1/200] [Batch 317/3166] [D loss: 0.112527] [G loss: 14.846493] [gradient_penalty: 0.009040]\n",
            "[Epoch 1/200] [Batch 318/3166] [D loss: -0.184653] [G loss: 15.584922] [gradient_penalty: 0.009791]\n",
            "[Epoch 1/200] [Batch 319/3166] [D loss: 0.562240] [G loss: 14.843479] [gradient_penalty: 0.008382]\n",
            "[Epoch 1/200] [Batch 320/3166] [D loss: 1.514720] [G loss: 15.007130] [gradient_penalty: 0.012557]\n",
            "[Epoch 1/200] [Batch 321/3166] [D loss: 0.157090] [G loss: 13.079590] [gradient_penalty: 0.011461]\n",
            "[Epoch 1/200] [Batch 322/3166] [D loss: 1.502694] [G loss: 13.542419] [gradient_penalty: 0.008245]\n",
            "[Epoch 1/200] [Batch 323/3166] [D loss: 1.962397] [G loss: 13.714580] [gradient_penalty: 0.012497]\n",
            "[Epoch 1/200] [Batch 324/3166] [D loss: 2.689020] [G loss: 12.929661] [gradient_penalty: 0.010461]\n",
            "[Epoch 1/200] [Batch 325/3166] [D loss: 1.267632] [G loss: 12.419518] [gradient_penalty: 0.012038]\n",
            "[Epoch 1/200] [Batch 326/3166] [D loss: 1.096341] [G loss: 11.549408] [gradient_penalty: 0.007574]\n",
            "[Epoch 1/200] [Batch 327/3166] [D loss: 2.184556] [G loss: 13.432482] [gradient_penalty: 0.009605]\n",
            "[Epoch 1/200] [Batch 328/3166] [D loss: 1.109240] [G loss: 12.947348] [gradient_penalty: 0.008355]\n",
            "[Epoch 1/200] [Batch 329/3166] [D loss: 1.516238] [G loss: 14.072637] [gradient_penalty: 0.007765]\n",
            "[Epoch 1/200] [Batch 330/3166] [D loss: 0.822896] [G loss: 13.225513] [gradient_penalty: 0.010290]\n",
            "[Epoch 1/200] [Batch 331/3166] [D loss: 0.538968] [G loss: 13.646679] [gradient_penalty: 0.011874]\n",
            "[Epoch 1/200] [Batch 332/3166] [D loss: 1.399481] [G loss: 14.521060] [gradient_penalty: 0.010277]\n",
            "[Epoch 1/200] [Batch 333/3166] [D loss: 0.993478] [G loss: 13.429052] [gradient_penalty: 0.009543]\n",
            "[Epoch 1/200] [Batch 334/3166] [D loss: 0.745994] [G loss: 13.235827] [gradient_penalty: 0.010751]\n",
            "log\n",
            "[Epoch 1/200] [Batch 335/3166] [D loss: 1.541502] [G loss: 13.163500] [gradient_penalty: 0.010305]\n",
            "[Epoch 1/200] [Batch 336/3166] [D loss: 0.596779] [G loss: 13.102377] [gradient_penalty: 0.006218]\n",
            "[Epoch 1/200] [Batch 337/3166] [D loss: 0.875074] [G loss: 12.922785] [gradient_penalty: 0.009429]\n",
            "[Epoch 1/200] [Batch 338/3166] [D loss: 0.270782] [G loss: 11.415976] [gradient_penalty: 0.007291]\n",
            "[Epoch 1/200] [Batch 339/3166] [D loss: 1.434853] [G loss: 12.283010] [gradient_penalty: 0.007656]\n",
            "[Epoch 1/200] [Batch 340/3166] [D loss: 2.020011] [G loss: 12.109423] [gradient_penalty: 0.007313]\n",
            "[Epoch 1/200] [Batch 341/3166] [D loss: 1.990505] [G loss: 12.214860] [gradient_penalty: 0.009012]\n",
            "[Epoch 1/200] [Batch 342/3166] [D loss: 1.902418] [G loss: 11.930166] [gradient_penalty: 0.006301]\n",
            "[Epoch 1/200] [Batch 343/3166] [D loss: 0.723365] [G loss: 12.525025] [gradient_penalty: 0.008016]\n",
            "[Epoch 1/200] [Batch 344/3166] [D loss: 1.763707] [G loss: 13.154685] [gradient_penalty: 0.008005]\n",
            "[Epoch 1/200] [Batch 345/3166] [D loss: 0.089175] [G loss: 13.207895] [gradient_penalty: 0.005249]\n",
            "[Epoch 1/200] [Batch 346/3166] [D loss: 0.013196] [G loss: 13.757135] [gradient_penalty: 0.008030]\n",
            "[Epoch 1/200] [Batch 347/3166] [D loss: 0.005440] [G loss: 14.683666] [gradient_penalty: 0.013628]\n",
            "[Epoch 1/200] [Batch 348/3166] [D loss: 0.724116] [G loss: 14.873804] [gradient_penalty: 0.014945]\n",
            "[Epoch 1/200] [Batch 349/3166] [D loss: -0.681852] [G loss: 13.727770] [gradient_penalty: 0.008806]\n",
            "[Epoch 1/200] [Batch 350/3166] [D loss: 1.620172] [G loss: 14.523531] [gradient_penalty: 0.013147]\n",
            "[Epoch 1/200] [Batch 351/3166] [D loss: -0.058437] [G loss: 13.052799] [gradient_penalty: 0.009648]\n",
            "[Epoch 1/200] [Batch 352/3166] [D loss: 0.485417] [G loss: 13.728023] [gradient_penalty: 0.010501]\n",
            "[Epoch 1/200] [Batch 353/3166] [D loss: 1.711657] [G loss: 13.780412] [gradient_penalty: 0.009545]\n",
            "[Epoch 1/200] [Batch 354/3166] [D loss: 0.936849] [G loss: 13.248866] [gradient_penalty: 0.012511]\n",
            "[Epoch 1/200] [Batch 355/3166] [D loss: -0.179147] [G loss: 13.674664] [gradient_penalty: 0.010369]\n",
            "[Epoch 1/200] [Batch 356/3166] [D loss: 1.722030] [G loss: 13.016558] [gradient_penalty: 0.012165]\n",
            "[Epoch 1/200] [Batch 357/3166] [D loss: 2.378668] [G loss: 13.367201] [gradient_penalty: 0.007865]\n",
            "[Epoch 1/200] [Batch 358/3166] [D loss: 1.895648] [G loss: 12.635187] [gradient_penalty: 0.008922]\n",
            "[Epoch 1/200] [Batch 359/3166] [D loss: 2.415501] [G loss: 13.025881] [gradient_penalty: 0.011224]\n",
            "[Epoch 1/200] [Batch 360/3166] [D loss: 1.793488] [G loss: 13.011600] [gradient_penalty: 0.012456]\n",
            "[Epoch 1/200] [Batch 361/3166] [D loss: -0.033669] [G loss: 13.699723] [gradient_penalty: 0.010511]\n",
            "[Epoch 1/200] [Batch 362/3166] [D loss: 1.418612] [G loss: 13.334860] [gradient_penalty: 0.010386]\n",
            "[Epoch 1/200] [Batch 363/3166] [D loss: 1.118743] [G loss: 13.487712] [gradient_penalty: 0.010628]\n",
            "[Epoch 1/200] [Batch 364/3166] [D loss: -0.017211] [G loss: 14.140081] [gradient_penalty: 0.009765]\n",
            "[Epoch 1/200] [Batch 365/3166] [D loss: 0.929616] [G loss: 13.580042] [gradient_penalty: 0.007819]\n",
            "[Epoch 1/200] [Batch 366/3166] [D loss: 0.936474] [G loss: 14.266668] [gradient_penalty: 0.008920]\n",
            "[Epoch 1/200] [Batch 367/3166] [D loss: 2.004166] [G loss: 12.454697] [gradient_penalty: 0.012490]\n",
            "[Epoch 1/200] [Batch 368/3166] [D loss: 0.657426] [G loss: 13.264162] [gradient_penalty: 0.008304]\n",
            "[Epoch 1/200] [Batch 369/3166] [D loss: 1.491369] [G loss: 11.881586] [gradient_penalty: 0.008685]\n",
            "[Epoch 1/200] [Batch 370/3166] [D loss: 1.806500] [G loss: 12.123226] [gradient_penalty: 0.010116]\n",
            "[Epoch 1/200] [Batch 371/3166] [D loss: 1.241408] [G loss: 12.208035] [gradient_penalty: 0.009187]\n",
            "[Epoch 1/200] [Batch 372/3166] [D loss: 2.243266] [G loss: 11.824996] [gradient_penalty: 0.008440]\n",
            "[Epoch 1/200] [Batch 373/3166] [D loss: 1.754622] [G loss: 11.285696] [gradient_penalty: 0.007660]\n",
            "[Epoch 1/200] [Batch 374/3166] [D loss: 2.123331] [G loss: 11.721869] [gradient_penalty: 0.012636]\n",
            "[Epoch 1/200] [Batch 375/3166] [D loss: 1.610767] [G loss: 11.436008] [gradient_penalty: 0.006926]\n",
            "[Epoch 1/200] [Batch 376/3166] [D loss: 2.306812] [G loss: 11.481270] [gradient_penalty: 0.006427]\n",
            "[Epoch 1/200] [Batch 377/3166] [D loss: 1.775633] [G loss: 11.070695] [gradient_penalty: 0.008702]\n",
            "[Epoch 1/200] [Batch 378/3166] [D loss: 0.899117] [G loss: 12.171095] [gradient_penalty: 0.007455]\n",
            "[Epoch 1/200] [Batch 379/3166] [D loss: 1.237742] [G loss: 12.324239] [gradient_penalty: 0.008234]\n",
            "[Epoch 1/200] [Batch 380/3166] [D loss: 0.308814] [G loss: 12.703913] [gradient_penalty: 0.009982]\n",
            "[Epoch 1/200] [Batch 381/3166] [D loss: 0.032345] [G loss: 14.214445] [gradient_penalty: 0.009570]\n",
            "[Epoch 1/200] [Batch 382/3166] [D loss: 0.620166] [G loss: 14.026428] [gradient_penalty: 0.009384]\n",
            "[Epoch 1/200] [Batch 383/3166] [D loss: -0.163421] [G loss: 14.637812] [gradient_penalty: 0.009414]\n",
            "[Epoch 1/200] [Batch 384/3166] [D loss: -0.882720] [G loss: 13.955452] [gradient_penalty: 0.011889]\n",
            "[Epoch 1/200] [Batch 385/3166] [D loss: 0.499523] [G loss: 14.375998] [gradient_penalty: 0.010922]\n",
            "[Epoch 1/200] [Batch 386/3166] [D loss: 0.038325] [G loss: 13.547886] [gradient_penalty: 0.011948]\n",
            "[Epoch 1/200] [Batch 387/3166] [D loss: 2.405538] [G loss: 13.917266] [gradient_penalty: 0.014730]\n",
            "[Epoch 1/200] [Batch 388/3166] [D loss: 2.100361] [G loss: 13.856409] [gradient_penalty: 0.011793]\n",
            "[Epoch 1/200] [Batch 389/3166] [D loss: 0.826610] [G loss: 13.407650] [gradient_penalty: 0.020572]\n",
            "[Epoch 1/200] [Batch 390/3166] [D loss: 2.137549] [G loss: 12.531471] [gradient_penalty: 0.012209]\n",
            "[Epoch 1/200] [Batch 391/3166] [D loss: 2.048086] [G loss: 12.966381] [gradient_penalty: 0.009728]\n",
            "[Epoch 1/200] [Batch 392/3166] [D loss: 3.990571] [G loss: 11.601038] [gradient_penalty: 0.010219]\n",
            "[Epoch 1/200] [Batch 393/3166] [D loss: 3.968663] [G loss: 11.847361] [gradient_penalty: 0.011209]\n",
            "[Epoch 1/200] [Batch 394/3166] [D loss: 1.135540] [G loss: 12.421877] [gradient_penalty: 0.010199]\n",
            "[Epoch 1/200] [Batch 395/3166] [D loss: 1.697484] [G loss: 11.902662] [gradient_penalty: 0.008240]\n",
            "[Epoch 1/200] [Batch 396/3166] [D loss: 2.093979] [G loss: 12.202066] [gradient_penalty: 0.013582]\n",
            "[Epoch 1/200] [Batch 397/3166] [D loss: 0.723412] [G loss: 12.868809] [gradient_penalty: 0.009394]\n",
            "[Epoch 1/200] [Batch 398/3166] [D loss: 1.160623] [G loss: 12.067500] [gradient_penalty: 0.008238]\n",
            "[Epoch 1/200] [Batch 399/3166] [D loss: 0.780937] [G loss: 12.813965] [gradient_penalty: 0.006553]\n",
            "[Epoch 1/200] [Batch 400/3166] [D loss: 0.094311] [G loss: 13.203982] [gradient_penalty: 0.007714]\n",
            "[Epoch 1/200] [Batch 401/3166] [D loss: 0.370880] [G loss: 13.309767] [gradient_penalty: 0.012690]\n",
            "[Epoch 1/200] [Batch 402/3166] [D loss: 0.275647] [G loss: 12.897053] [gradient_penalty: 0.006481]\n",
            "[Epoch 1/200] [Batch 403/3166] [D loss: -1.245513] [G loss: 11.759019] [gradient_penalty: 0.005439]\n",
            "[Epoch 1/200] [Batch 404/3166] [D loss: 0.712560] [G loss: 12.728342] [gradient_penalty: 0.008530]\n",
            "[Epoch 1/200] [Batch 405/3166] [D loss: 0.957516] [G loss: 12.255163] [gradient_penalty: 0.012199]\n",
            "[Epoch 1/200] [Batch 406/3166] [D loss: 0.644147] [G loss: 12.200382] [gradient_penalty: 0.006424]\n",
            "[Epoch 1/200] [Batch 407/3166] [D loss: 0.523619] [G loss: 12.736046] [gradient_penalty: 0.007419]\n",
            "[Epoch 1/200] [Batch 408/3166] [D loss: 1.014332] [G loss: 12.377368] [gradient_penalty: 0.010977]\n",
            "[Epoch 1/200] [Batch 409/3166] [D loss: 2.052135] [G loss: 12.259581] [gradient_penalty: 0.007810]\n",
            "[Epoch 1/200] [Batch 410/3166] [D loss: 2.327347] [G loss: 12.134672] [gradient_penalty: 0.004640]\n",
            "[Epoch 1/200] [Batch 411/3166] [D loss: 1.074495] [G loss: 12.189544] [gradient_penalty: 0.007688]\n",
            "[Epoch 1/200] [Batch 412/3166] [D loss: 0.945212] [G loss: 12.001033] [gradient_penalty: 0.008855]\n",
            "[Epoch 1/200] [Batch 413/3166] [D loss: 2.176762] [G loss: 11.674181] [gradient_penalty: 0.009978]\n",
            "[Epoch 1/200] [Batch 414/3166] [D loss: 1.297052] [G loss: 12.231104] [gradient_penalty: 0.008219]\n",
            "[Epoch 1/200] [Batch 415/3166] [D loss: 0.489413] [G loss: 12.092054] [gradient_penalty: 0.007117]\n",
            "[Epoch 1/200] [Batch 416/3166] [D loss: 0.450626] [G loss: 12.526669] [gradient_penalty: 0.005399]\n",
            "[Epoch 1/200] [Batch 417/3166] [D loss: 0.588108] [G loss: 12.637449] [gradient_penalty: 0.009442]\n",
            "[Epoch 1/200] [Batch 418/3166] [D loss: -0.100212] [G loss: 13.158384] [gradient_penalty: 0.007311]\n",
            "[Epoch 1/200] [Batch 419/3166] [D loss: -0.040029] [G loss: 13.305984] [gradient_penalty: 0.006823]\n",
            "[Epoch 1/200] [Batch 420/3166] [D loss: -0.681652] [G loss: 13.133942] [gradient_penalty: 0.008397]\n",
            "[Epoch 1/200] [Batch 421/3166] [D loss: -0.425624] [G loss: 13.688862] [gradient_penalty: 0.008414]\n",
            "[Epoch 1/200] [Batch 422/3166] [D loss: 1.536951] [G loss: 13.435732] [gradient_penalty: 0.010587]\n",
            "[Epoch 1/200] [Batch 423/3166] [D loss: 1.772690] [G loss: 12.728810] [gradient_penalty: 0.008355]\n",
            "[Epoch 1/200] [Batch 424/3166] [D loss: 1.156720] [G loss: 12.928391] [gradient_penalty: 0.011323]\n",
            "[Epoch 1/200] [Batch 425/3166] [D loss: 1.097341] [G loss: 13.369282] [gradient_penalty: 0.011712]\n",
            "[Epoch 1/200] [Batch 426/3166] [D loss: 2.051752] [G loss: 13.259393] [gradient_penalty: 0.010435]\n",
            "[Epoch 1/200] [Batch 427/3166] [D loss: 1.398987] [G loss: 12.990944] [gradient_penalty: 0.011180]\n",
            "[Epoch 1/200] [Batch 428/3166] [D loss: 2.116082] [G loss: 12.539978] [gradient_penalty: 0.011825]\n",
            "[Epoch 1/200] [Batch 429/3166] [D loss: 0.770560] [G loss: 12.881996] [gradient_penalty: 0.009022]\n",
            "[Epoch 1/200] [Batch 430/3166] [D loss: 0.173579] [G loss: 13.010862] [gradient_penalty: 0.007430]\n",
            "[Epoch 1/200] [Batch 431/3166] [D loss: 2.214301] [G loss: 13.320086] [gradient_penalty: 0.007911]\n",
            "[Epoch 1/200] [Batch 432/3166] [D loss: 3.325267] [G loss: 12.954842] [gradient_penalty: 0.008644]\n",
            "[Epoch 1/200] [Batch 433/3166] [D loss: 1.607885] [G loss: 13.905316] [gradient_penalty: 0.008357]\n",
            "[Epoch 1/200] [Batch 434/3166] [D loss: -0.242066] [G loss: 13.646911] [gradient_penalty: 0.008907]\n",
            "log\n",
            "[Epoch 1/200] [Batch 435/3166] [D loss: -0.719410] [G loss: 14.363325] [gradient_penalty: 0.009840]\n",
            "[Epoch 1/200] [Batch 436/3166] [D loss: -0.742397] [G loss: 15.382624] [gradient_penalty: 0.015898]\n",
            "[Epoch 1/200] [Batch 437/3166] [D loss: 0.423385] [G loss: 14.988249] [gradient_penalty: 0.009266]\n",
            "[Epoch 1/200] [Batch 438/3166] [D loss: -0.234713] [G loss: 14.483171] [gradient_penalty: 0.011145]\n",
            "[Epoch 1/200] [Batch 439/3166] [D loss: 1.585935] [G loss: 13.691742] [gradient_penalty: 0.009078]\n",
            "[Epoch 1/200] [Batch 440/3166] [D loss: 0.838513] [G loss: 14.299774] [gradient_penalty: 0.009476]\n",
            "[Epoch 1/200] [Batch 441/3166] [D loss: -0.031072] [G loss: 13.189201] [gradient_penalty: 0.008491]\n",
            "[Epoch 1/200] [Batch 442/3166] [D loss: 0.635146] [G loss: 13.704319] [gradient_penalty: 0.008663]\n",
            "[Epoch 1/200] [Batch 443/3166] [D loss: 2.033513] [G loss: 13.653713] [gradient_penalty: 0.007636]\n",
            "[Epoch 1/200] [Batch 444/3166] [D loss: 1.108998] [G loss: 13.163165] [gradient_penalty: 0.008238]\n",
            "[Epoch 1/200] [Batch 445/3166] [D loss: 1.429315] [G loss: 13.028817] [gradient_penalty: 0.008201]\n",
            "[Epoch 1/200] [Batch 446/3166] [D loss: 1.840766] [G loss: 13.969107] [gradient_penalty: 0.009780]\n",
            "[Epoch 1/200] [Batch 447/3166] [D loss: 1.763025] [G loss: 13.000227] [gradient_penalty: 0.008828]\n",
            "[Epoch 1/200] [Batch 448/3166] [D loss: 1.247446] [G loss: 13.857449] [gradient_penalty: 0.007163]\n",
            "[Epoch 1/200] [Batch 449/3166] [D loss: 0.799062] [G loss: 13.761948] [gradient_penalty: 0.004923]\n",
            "[Epoch 1/200] [Batch 450/3166] [D loss: 1.506776] [G loss: 13.477719] [gradient_penalty: 0.007598]\n",
            "[Epoch 1/200] [Batch 451/3166] [D loss: 0.648699] [G loss: 13.648201] [gradient_penalty: 0.007597]\n",
            "[Epoch 1/200] [Batch 452/3166] [D loss: 0.945371] [G loss: 13.650023] [gradient_penalty: 0.012365]\n",
            "[Epoch 1/200] [Batch 453/3166] [D loss: 0.918737] [G loss: 14.103463] [gradient_penalty: 0.009055]\n",
            "[Epoch 1/200] [Batch 454/3166] [D loss: 0.946098] [G loss: 13.498390] [gradient_penalty: 0.009374]\n",
            "[Epoch 1/200] [Batch 455/3166] [D loss: -0.002100] [G loss: 13.947725] [gradient_penalty: 0.009660]\n",
            "[Epoch 1/200] [Batch 456/3166] [D loss: 0.727838] [G loss: 14.524963] [gradient_penalty: 0.009606]\n",
            "[Epoch 1/200] [Batch 457/3166] [D loss: -0.070481] [G loss: 13.728584] [gradient_penalty: 0.006595]\n",
            "[Epoch 1/200] [Batch 458/3166] [D loss: 0.577854] [G loss: 13.631756] [gradient_penalty: 0.010174]\n",
            "[Epoch 1/200] [Batch 459/3166] [D loss: 1.298677] [G loss: 14.217938] [gradient_penalty: 0.013296]\n",
            "[Epoch 1/200] [Batch 460/3166] [D loss: 1.708316] [G loss: 13.348792] [gradient_penalty: 0.010267]\n",
            "[Epoch 1/200] [Batch 461/3166] [D loss: 0.732966] [G loss: 13.593952] [gradient_penalty: 0.010523]\n",
            "[Epoch 1/200] [Batch 462/3166] [D loss: 0.946405] [G loss: 13.221167] [gradient_penalty: 0.013856]\n",
            "[Epoch 1/200] [Batch 463/3166] [D loss: 2.407093] [G loss: 13.908881] [gradient_penalty: 0.014075]\n",
            "[Epoch 1/200] [Batch 464/3166] [D loss: 2.124477] [G loss: 13.410475] [gradient_penalty: 0.011493]\n",
            "[Epoch 1/200] [Batch 465/3166] [D loss: -0.121678] [G loss: 12.610867] [gradient_penalty: 0.012155]\n",
            "[Epoch 1/200] [Batch 466/3166] [D loss: 2.223714] [G loss: 13.501166] [gradient_penalty: 0.011804]\n",
            "[Epoch 1/200] [Batch 467/3166] [D loss: 2.149177] [G loss: 13.072575] [gradient_penalty: 0.012085]\n",
            "[Epoch 1/200] [Batch 468/3166] [D loss: 1.359187] [G loss: 13.691153] [gradient_penalty: 0.009589]\n",
            "[Epoch 1/200] [Batch 469/3166] [D loss: 1.005099] [G loss: 12.797989] [gradient_penalty: 0.007842]\n",
            "[Epoch 1/200] [Batch 470/3166] [D loss: 2.042163] [G loss: 12.817671] [gradient_penalty: 0.011069]\n",
            "[Epoch 1/200] [Batch 471/3166] [D loss: 1.152179] [G loss: 13.159323] [gradient_penalty: 0.010760]\n",
            "[Epoch 1/200] [Batch 472/3166] [D loss: 0.530427] [G loss: 13.476835] [gradient_penalty: 0.007020]\n",
            "[Epoch 1/200] [Batch 473/3166] [D loss: 0.329973] [G loss: 12.347834] [gradient_penalty: 0.007672]\n",
            "[Epoch 1/200] [Batch 474/3166] [D loss: -0.140140] [G loss: 13.560103] [gradient_penalty: 0.006485]\n",
            "[Epoch 1/200] [Batch 475/3166] [D loss: 1.317087] [G loss: 13.321459] [gradient_penalty: 0.009006]\n",
            "[Epoch 1/200] [Batch 476/3166] [D loss: 1.143967] [G loss: 13.258637] [gradient_penalty: 0.009706]\n",
            "[Epoch 1/200] [Batch 477/3166] [D loss: 0.899299] [G loss: 12.946841] [gradient_penalty: 0.009933]\n",
            "[Epoch 1/200] [Batch 478/3166] [D loss: 1.906874] [G loss: 12.152762] [gradient_penalty: 0.007152]\n",
            "[Epoch 1/200] [Batch 479/3166] [D loss: 0.033313] [G loss: 12.620950] [gradient_penalty: 0.009113]\n",
            "[Epoch 1/200] [Batch 480/3166] [D loss: 1.881620] [G loss: 12.920436] [gradient_penalty: 0.010423]\n",
            "[Epoch 1/200] [Batch 481/3166] [D loss: 1.013641] [G loss: 12.486280] [gradient_penalty: 0.008953]\n",
            "[Epoch 1/200] [Batch 482/3166] [D loss: 0.824630] [G loss: 12.856568] [gradient_penalty: 0.009445]\n",
            "[Epoch 1/200] [Batch 483/3166] [D loss: 1.365744] [G loss: 13.155638] [gradient_penalty: 0.007219]\n",
            "[Epoch 1/200] [Batch 484/3166] [D loss: 0.821477] [G loss: 13.253631] [gradient_penalty: 0.007712]\n",
            "[Epoch 1/200] [Batch 485/3166] [D loss: 0.884952] [G loss: 12.778874] [gradient_penalty: 0.007356]\n",
            "[Epoch 1/200] [Batch 486/3166] [D loss: 1.301451] [G loss: 12.162684] [gradient_penalty: 0.009830]\n",
            "[Epoch 1/200] [Batch 487/3166] [D loss: 1.471278] [G loss: 12.360418] [gradient_penalty: 0.010676]\n",
            "[Epoch 1/200] [Batch 488/3166] [D loss: 0.204458] [G loss: 13.023949] [gradient_penalty: 0.010054]\n",
            "[Epoch 1/200] [Batch 489/3166] [D loss: 0.343427] [G loss: 13.037054] [gradient_penalty: 0.007863]\n",
            "[Epoch 1/200] [Batch 490/3166] [D loss: 1.201886] [G loss: 12.053808] [gradient_penalty: 0.009613]\n",
            "[Epoch 1/200] [Batch 491/3166] [D loss: -0.320869] [G loss: 13.546931] [gradient_penalty: 0.009009]\n",
            "[Epoch 1/200] [Batch 492/3166] [D loss: 1.533356] [G loss: 13.117718] [gradient_penalty: 0.008144]\n",
            "[Epoch 1/200] [Batch 493/3166] [D loss: 1.762699] [G loss: 13.259007] [gradient_penalty: 0.007418]\n",
            "[Epoch 1/200] [Batch 494/3166] [D loss: 0.731185] [G loss: 13.045421] [gradient_penalty: 0.009333]\n",
            "[Epoch 1/200] [Batch 495/3166] [D loss: -0.094425] [G loss: 13.184062] [gradient_penalty: 0.007527]\n",
            "[Epoch 1/200] [Batch 496/3166] [D loss: -0.130120] [G loss: 12.925824] [gradient_penalty: 0.008209]\n",
            "[Epoch 1/200] [Batch 497/3166] [D loss: 0.754716] [G loss: 12.890229] [gradient_penalty: 0.010177]\n",
            "[Epoch 1/200] [Batch 498/3166] [D loss: 1.773097] [G loss: 14.102252] [gradient_penalty: 0.008398]\n",
            "[Epoch 1/200] [Batch 499/3166] [D loss: 1.202495] [G loss: 12.985710] [gradient_penalty: 0.012044]\n",
            "[Epoch 1/200] [Batch 500/3166] [D loss: 3.490629] [G loss: 13.317896] [gradient_penalty: 0.010338]\n",
            "[Epoch 1/200] [Batch 501/3166] [D loss: 1.020523] [G loss: 13.186432] [gradient_penalty: 0.007628]\n",
            "[Epoch 1/200] [Batch 502/3166] [D loss: 0.364292] [G loss: 13.283316] [gradient_penalty: 0.009485]\n",
            "[Epoch 1/200] [Batch 503/3166] [D loss: 1.057688] [G loss: 13.647673] [gradient_penalty: 0.009909]\n",
            "[Epoch 1/200] [Batch 504/3166] [D loss: 1.338218] [G loss: 12.619618] [gradient_penalty: 0.008411]\n",
            "[Epoch 1/200] [Batch 505/3166] [D loss: 0.968075] [G loss: 13.402602] [gradient_penalty: 0.007751]\n",
            "[Epoch 1/200] [Batch 506/3166] [D loss: 1.584041] [G loss: 12.869676] [gradient_penalty: 0.010313]\n",
            "[Epoch 1/200] [Batch 507/3166] [D loss: 2.111631] [G loss: 13.219640] [gradient_penalty: 0.008156]\n",
            "[Epoch 1/200] [Batch 508/3166] [D loss: 2.414971] [G loss: 13.444817] [gradient_penalty: 0.009921]\n",
            "[Epoch 1/200] [Batch 509/3166] [D loss: 0.571858] [G loss: 13.458994] [gradient_penalty: 0.007712]\n",
            "[Epoch 1/200] [Batch 510/3166] [D loss: 0.464175] [G loss: 13.113245] [gradient_penalty: 0.007039]\n",
            "[Epoch 1/200] [Batch 511/3166] [D loss: 0.586403] [G loss: 13.173739] [gradient_penalty: 0.007895]\n",
            "[Epoch 1/200] [Batch 512/3166] [D loss: 0.247881] [G loss: 13.146322] [gradient_penalty: 0.010276]\n",
            "[Epoch 1/200] [Batch 513/3166] [D loss: 1.499099] [G loss: 12.881098] [gradient_penalty: 0.007352]\n",
            "[Epoch 1/200] [Batch 514/3166] [D loss: 0.568381] [G loss: 12.946984] [gradient_penalty: 0.008895]\n",
            "[Epoch 1/200] [Batch 515/3166] [D loss: 0.179930] [G loss: 12.997699] [gradient_penalty: 0.008990]\n",
            "[Epoch 1/200] [Batch 516/3166] [D loss: 0.643963] [G loss: 13.069883] [gradient_penalty: 0.007942]\n",
            "[Epoch 1/200] [Batch 517/3166] [D loss: 0.615912] [G loss: 12.689177] [gradient_penalty: 0.007768]\n",
            "[Epoch 1/200] [Batch 518/3166] [D loss: 0.260729] [G loss: 12.642496] [gradient_penalty: 0.010265]\n",
            "[Epoch 1/200] [Batch 519/3166] [D loss: 0.734696] [G loss: 13.222309] [gradient_penalty: 0.009341]\n",
            "[Epoch 1/200] [Batch 520/3166] [D loss: 0.650933] [G loss: 13.107063] [gradient_penalty: 0.005531]\n",
            "[Epoch 1/200] [Batch 521/3166] [D loss: 0.979417] [G loss: 12.715441] [gradient_penalty: 0.006699]\n",
            "[Epoch 1/200] [Batch 522/3166] [D loss: 0.891584] [G loss: 12.515779] [gradient_penalty: 0.005343]\n",
            "[Epoch 1/200] [Batch 523/3166] [D loss: 1.206044] [G loss: 12.336535] [gradient_penalty: 0.006895]\n",
            "[Epoch 1/200] [Batch 524/3166] [D loss: 1.874517] [G loss: 13.214892] [gradient_penalty: 0.008947]\n",
            "[Epoch 1/200] [Batch 525/3166] [D loss: 0.326787] [G loss: 12.980345] [gradient_penalty: 0.006379]\n",
            "[Epoch 1/200] [Batch 526/3166] [D loss: 1.461166] [G loss: 12.535498] [gradient_penalty: 0.007448]\n",
            "[Epoch 1/200] [Batch 527/3166] [D loss: 0.866030] [G loss: 12.708790] [gradient_penalty: 0.007138]\n",
            "[Epoch 1/200] [Batch 528/3166] [D loss: 1.141266] [G loss: 13.269398] [gradient_penalty: 0.005303]\n",
            "[Epoch 1/200] [Batch 529/3166] [D loss: 0.731996] [G loss: 13.013023] [gradient_penalty: 0.008957]\n",
            "[Epoch 1/200] [Batch 530/3166] [D loss: 1.971320] [G loss: 13.069764] [gradient_penalty: 0.007281]\n",
            "[Epoch 1/200] [Batch 531/3166] [D loss: 2.203235] [G loss: 12.832842] [gradient_penalty: 0.010243]\n",
            "[Epoch 1/200] [Batch 532/3166] [D loss: 1.393445] [G loss: 12.517773] [gradient_penalty: 0.008732]\n",
            "[Epoch 1/200] [Batch 533/3166] [D loss: 0.090172] [G loss: 12.580991] [gradient_penalty: 0.006535]\n",
            "[Epoch 1/200] [Batch 534/3166] [D loss: 0.868652] [G loss: 13.344735] [gradient_penalty: 0.007044]\n",
            "log\n",
            "[Epoch 1/200] [Batch 535/3166] [D loss: 1.078532] [G loss: 13.040148] [gradient_penalty: 0.006965]\n",
            "[Epoch 1/200] [Batch 536/3166] [D loss: 0.726131] [G loss: 13.549383] [gradient_penalty: 0.008543]\n",
            "[Epoch 1/200] [Batch 537/3166] [D loss: 1.969444] [G loss: 12.479733] [gradient_penalty: 0.010830]\n",
            "[Epoch 1/200] [Batch 538/3166] [D loss: 0.459021] [G loss: 13.282031] [gradient_penalty: 0.011019]\n",
            "[Epoch 1/200] [Batch 539/3166] [D loss: 0.684291] [G loss: 12.834980] [gradient_penalty: 0.010940]\n",
            "[Epoch 1/200] [Batch 540/3166] [D loss: 1.128391] [G loss: 12.732157] [gradient_penalty: 0.008399]\n",
            "[Epoch 1/200] [Batch 541/3166] [D loss: 0.475058] [G loss: 13.225487] [gradient_penalty: 0.010591]\n",
            "[Epoch 1/200] [Batch 542/3166] [D loss: 1.070502] [G loss: 12.690316] [gradient_penalty: 0.006685]\n",
            "[Epoch 1/200] [Batch 543/3166] [D loss: 1.552301] [G loss: 12.704772] [gradient_penalty: 0.009217]\n",
            "[Epoch 1/200] [Batch 544/3166] [D loss: 0.266190] [G loss: 11.949597] [gradient_penalty: 0.006898]\n",
            "[Epoch 1/200] [Batch 545/3166] [D loss: 0.868050] [G loss: 11.287746] [gradient_penalty: 0.009508]\n",
            "[Epoch 1/200] [Batch 546/3166] [D loss: 1.757472] [G loss: 11.907703] [gradient_penalty: 0.008894]\n",
            "[Epoch 1/200] [Batch 547/3166] [D loss: 1.596669] [G loss: 12.390542] [gradient_penalty: 0.008017]\n",
            "[Epoch 1/200] [Batch 548/3166] [D loss: 1.429313] [G loss: 12.668344] [gradient_penalty: 0.010568]\n",
            "[Epoch 1/200] [Batch 549/3166] [D loss: 1.078869] [G loss: 12.197538] [gradient_penalty: 0.008397]\n",
            "[Epoch 1/200] [Batch 550/3166] [D loss: 1.071265] [G loss: 11.651278] [gradient_penalty: 0.011182]\n",
            "[Epoch 1/200] [Batch 551/3166] [D loss: 0.910087] [G loss: 12.258938] [gradient_penalty: 0.008223]\n",
            "[Epoch 1/200] [Batch 552/3166] [D loss: 1.845167] [G loss: 12.348731] [gradient_penalty: 0.008036]\n",
            "[Epoch 1/200] [Batch 553/3166] [D loss: 0.036342] [G loss: 11.338270] [gradient_penalty: 0.008718]\n",
            "[Epoch 1/200] [Batch 554/3166] [D loss: -0.263715] [G loss: 12.738959] [gradient_penalty: 0.005626]\n",
            "[Epoch 1/200] [Batch 555/3166] [D loss: 1.157258] [G loss: 12.367224] [gradient_penalty: 0.008567]\n",
            "[Epoch 1/200] [Batch 556/3166] [D loss: 3.106408] [G loss: 12.591372] [gradient_penalty: 0.009686]\n",
            "[Epoch 1/200] [Batch 557/3166] [D loss: 1.042920] [G loss: 12.585693] [gradient_penalty: 0.007591]\n",
            "[Epoch 1/200] [Batch 558/3166] [D loss: 1.727889] [G loss: 12.168064] [gradient_penalty: 0.011089]\n",
            "[Epoch 1/200] [Batch 559/3166] [D loss: 0.926984] [G loss: 12.483698] [gradient_penalty: 0.007656]\n",
            "[Epoch 1/200] [Batch 560/3166] [D loss: 0.807094] [G loss: 12.526182] [gradient_penalty: 0.007833]\n",
            "[Epoch 1/200] [Batch 561/3166] [D loss: 1.374068] [G loss: 12.514006] [gradient_penalty: 0.009420]\n",
            "[Epoch 1/200] [Batch 562/3166] [D loss: 0.912132] [G loss: 11.800451] [gradient_penalty: 0.006462]\n",
            "[Epoch 1/200] [Batch 563/3166] [D loss: 0.212491] [G loss: 12.253730] [gradient_penalty: 0.010188]\n",
            "[Epoch 1/200] [Batch 564/3166] [D loss: 0.303235] [G loss: 12.707647] [gradient_penalty: 0.009972]\n",
            "[Epoch 1/200] [Batch 565/3166] [D loss: 1.267125] [G loss: 12.686553] [gradient_penalty: 0.009457]\n",
            "[Epoch 1/200] [Batch 566/3166] [D loss: 0.610459] [G loss: 11.817435] [gradient_penalty: 0.007268]\n",
            "[Epoch 1/200] [Batch 567/3166] [D loss: 1.559966] [G loss: 12.438011] [gradient_penalty: 0.006637]\n",
            "[Epoch 1/200] [Batch 568/3166] [D loss: 1.193088] [G loss: 12.631098] [gradient_penalty: 0.013547]\n",
            "[Epoch 1/200] [Batch 569/3166] [D loss: 0.367530] [G loss: 12.384182] [gradient_penalty: 0.007490]\n",
            "[Epoch 1/200] [Batch 570/3166] [D loss: 0.855452] [G loss: 12.818429] [gradient_penalty: 0.008142]\n",
            "[Epoch 1/200] [Batch 571/3166] [D loss: 1.235332] [G loss: 12.577421] [gradient_penalty: 0.009119]\n",
            "[Epoch 1/200] [Batch 572/3166] [D loss: 0.673585] [G loss: 13.004560] [gradient_penalty: 0.009481]\n",
            "[Epoch 1/200] [Batch 573/3166] [D loss: 2.565044] [G loss: 12.379236] [gradient_penalty: 0.012481]\n",
            "[Epoch 1/200] [Batch 574/3166] [D loss: 1.777902] [G loss: 12.554647] [gradient_penalty: 0.010109]\n",
            "[Epoch 1/200] [Batch 575/3166] [D loss: 0.328222] [G loss: 12.853817] [gradient_penalty: 0.007408]\n",
            "[Epoch 1/200] [Batch 576/3166] [D loss: 0.060104] [G loss: 13.071661] [gradient_penalty: 0.007477]\n",
            "[Epoch 1/200] [Batch 577/3166] [D loss: 1.029272] [G loss: 12.987421] [gradient_penalty: 0.006641]\n",
            "[Epoch 1/200] [Batch 578/3166] [D loss: 1.402986] [G loss: 12.587982] [gradient_penalty: 0.007543]\n",
            "[Epoch 1/200] [Batch 579/3166] [D loss: 1.813334] [G loss: 12.813126] [gradient_penalty: 0.010061]\n",
            "[Epoch 1/200] [Batch 580/3166] [D loss: 1.013176] [G loss: 12.453125] [gradient_penalty: 0.007651]\n",
            "[Epoch 1/200] [Batch 581/3166] [D loss: 2.119900] [G loss: 11.753156] [gradient_penalty: 0.012237]\n",
            "[Epoch 1/200] [Batch 582/3166] [D loss: 1.116347] [G loss: 12.014648] [gradient_penalty: 0.010199]\n",
            "[Epoch 1/200] [Batch 583/3166] [D loss: 1.456071] [G loss: 12.424780] [gradient_penalty: 0.008786]\n",
            "[Epoch 1/200] [Batch 584/3166] [D loss: 0.869210] [G loss: 11.519780] [gradient_penalty: 0.009072]\n",
            "[Epoch 1/200] [Batch 585/3166] [D loss: 0.912456] [G loss: 11.922913] [gradient_penalty: 0.010073]\n",
            "[Epoch 1/200] [Batch 586/3166] [D loss: 1.070263] [G loss: 11.917339] [gradient_penalty: 0.009050]\n",
            "[Epoch 1/200] [Batch 587/3166] [D loss: 0.801846] [G loss: 11.732974] [gradient_penalty: 0.008712]\n",
            "[Epoch 1/200] [Batch 588/3166] [D loss: 1.792328] [G loss: 12.122704] [gradient_penalty: 0.007676]\n",
            "[Epoch 1/200] [Batch 589/3166] [D loss: 0.584891] [G loss: 11.844492] [gradient_penalty: 0.009364]\n",
            "[Epoch 1/200] [Batch 590/3166] [D loss: 1.436715] [G loss: 11.643434] [gradient_penalty: 0.007462]\n",
            "[Epoch 1/200] [Batch 591/3166] [D loss: 1.888858] [G loss: 11.863762] [gradient_penalty: 0.011839]\n",
            "[Epoch 1/200] [Batch 592/3166] [D loss: 0.455787] [G loss: 12.245384] [gradient_penalty: 0.007433]\n",
            "[Epoch 1/200] [Batch 593/3166] [D loss: 0.507513] [G loss: 11.803689] [gradient_penalty: 0.006057]\n",
            "[Epoch 1/200] [Batch 594/3166] [D loss: 0.530419] [G loss: 12.493947] [gradient_penalty: 0.009187]\n",
            "[Epoch 1/200] [Batch 595/3166] [D loss: 1.126918] [G loss: 12.616214] [gradient_penalty: 0.008902]\n",
            "[Epoch 1/200] [Batch 596/3166] [D loss: 2.241812] [G loss: 12.058793] [gradient_penalty: 0.007286]\n",
            "[Epoch 1/200] [Batch 597/3166] [D loss: 0.763901] [G loss: 11.938293] [gradient_penalty: 0.008545]\n",
            "[Epoch 1/200] [Batch 598/3166] [D loss: 0.750368] [G loss: 11.757122] [gradient_penalty: 0.008306]\n",
            "[Epoch 1/200] [Batch 599/3166] [D loss: 1.054410] [G loss: 12.161259] [gradient_penalty: 0.007037]\n",
            "[Epoch 1/200] [Batch 600/3166] [D loss: 1.016504] [G loss: 11.871237] [gradient_penalty: 0.008852]\n",
            "[Epoch 1/200] [Batch 601/3166] [D loss: -0.342343] [G loss: 12.312811] [gradient_penalty: 0.008353]\n",
            "[Epoch 1/200] [Batch 602/3166] [D loss: 0.562075] [G loss: 11.930761] [gradient_penalty: 0.010023]\n",
            "[Epoch 1/200] [Batch 603/3166] [D loss: 0.635356] [G loss: 12.729198] [gradient_penalty: 0.005056]\n",
            "[Epoch 1/200] [Batch 604/3166] [D loss: 0.346716] [G loss: 12.395164] [gradient_penalty: 0.006516]\n",
            "[Epoch 1/200] [Batch 605/3166] [D loss: 1.838454] [G loss: 11.798140] [gradient_penalty: 0.010499]\n",
            "[Epoch 1/200] [Batch 606/3166] [D loss: 1.031763] [G loss: 12.289396] [gradient_penalty: 0.009231]\n",
            "[Epoch 1/200] [Batch 607/3166] [D loss: -0.039752] [G loss: 12.339362] [gradient_penalty: 0.009013]\n",
            "[Epoch 1/200] [Batch 608/3166] [D loss: 0.278864] [G loss: 12.097342] [gradient_penalty: 0.010659]\n",
            "[Epoch 1/200] [Batch 609/3166] [D loss: 2.209902] [G loss: 12.386250] [gradient_penalty: 0.007970]\n",
            "[Epoch 1/200] [Batch 610/3166] [D loss: 1.858227] [G loss: 11.174393] [gradient_penalty: 0.006704]\n",
            "[Epoch 1/200] [Batch 611/3166] [D loss: 0.487576] [G loss: 12.096365] [gradient_penalty: 0.008681]\n",
            "[Epoch 1/200] [Batch 612/3166] [D loss: 0.590612] [G loss: 12.654442] [gradient_penalty: 0.007542]\n",
            "[Epoch 1/200] [Batch 613/3166] [D loss: 1.143215] [G loss: 12.792641] [gradient_penalty: 0.007539]\n",
            "[Epoch 1/200] [Batch 614/3166] [D loss: 1.516153] [G loss: 12.315577] [gradient_penalty: 0.009420]\n",
            "[Epoch 1/200] [Batch 615/3166] [D loss: 1.568297] [G loss: 12.037033] [gradient_penalty: 0.006963]\n",
            "[Epoch 1/200] [Batch 616/3166] [D loss: 0.198223] [G loss: 11.508415] [gradient_penalty: 0.007444]\n",
            "[Epoch 1/200] [Batch 617/3166] [D loss: 0.462742] [G loss: 12.115348] [gradient_penalty: 0.008586]\n",
            "[Epoch 1/200] [Batch 618/3166] [D loss: 1.752254] [G loss: 12.512196] [gradient_penalty: 0.007240]\n",
            "[Epoch 1/200] [Batch 619/3166] [D loss: 0.331147] [G loss: 12.316210] [gradient_penalty: 0.006122]\n",
            "[Epoch 1/200] [Batch 620/3166] [D loss: -0.108275] [G loss: 11.897571] [gradient_penalty: 0.009328]\n",
            "[Epoch 1/200] [Batch 621/3166] [D loss: 0.626196] [G loss: 11.854585] [gradient_penalty: 0.009598]\n",
            "[Epoch 1/200] [Batch 622/3166] [D loss: 1.233334] [G loss: 11.689786] [gradient_penalty: 0.010820]\n",
            "[Epoch 1/200] [Batch 623/3166] [D loss: 1.630703] [G loss: 12.386383] [gradient_penalty: 0.010369]\n",
            "[Epoch 1/200] [Batch 624/3166] [D loss: 1.704210] [G loss: 11.602348] [gradient_penalty: 0.009517]\n",
            "[Epoch 1/200] [Batch 625/3166] [D loss: 1.555826] [G loss: 12.286370] [gradient_penalty: 0.008431]\n",
            "[Epoch 1/200] [Batch 626/3166] [D loss: 1.679739] [G loss: 11.596306] [gradient_penalty: 0.006235]\n",
            "[Epoch 1/200] [Batch 627/3166] [D loss: 0.965713] [G loss: 12.013254] [gradient_penalty: 0.011022]\n",
            "[Epoch 1/200] [Batch 628/3166] [D loss: 0.923941] [G loss: 11.818412] [gradient_penalty: 0.008955]\n",
            "[Epoch 1/200] [Batch 629/3166] [D loss: 1.117342] [G loss: 12.015093] [gradient_penalty: 0.007607]\n",
            "[Epoch 1/200] [Batch 630/3166] [D loss: 0.744829] [G loss: 12.587144] [gradient_penalty: 0.005836]\n",
            "[Epoch 1/200] [Batch 631/3166] [D loss: 0.152444] [G loss: 12.033698] [gradient_penalty: 0.006931]\n",
            "[Epoch 1/200] [Batch 632/3166] [D loss: 1.016491] [G loss: 12.576442] [gradient_penalty: 0.009124]\n",
            "[Epoch 1/200] [Batch 633/3166] [D loss: 0.076935] [G loss: 13.060091] [gradient_penalty: 0.006857]\n",
            "[Epoch 1/200] [Batch 634/3166] [D loss: 1.854457] [G loss: 12.796939] [gradient_penalty: 0.007498]\n",
            "log\n",
            "[Epoch 1/200] [Batch 635/3166] [D loss: 1.212962] [G loss: 12.555166] [gradient_penalty: 0.006468]\n",
            "[Epoch 1/200] [Batch 636/3166] [D loss: 0.759866] [G loss: 12.266985] [gradient_penalty: 0.007299]\n",
            "[Epoch 1/200] [Batch 637/3166] [D loss: 1.056270] [G loss: 12.700362] [gradient_penalty: 0.007820]\n",
            "[Epoch 1/200] [Batch 638/3166] [D loss: 0.504397] [G loss: 12.464008] [gradient_penalty: 0.006142]\n",
            "[Epoch 1/200] [Batch 639/3166] [D loss: 1.834810] [G loss: 11.791363] [gradient_penalty: 0.009460]\n",
            "[Epoch 1/200] [Batch 640/3166] [D loss: -0.026048] [G loss: 12.216276] [gradient_penalty: 0.008500]\n",
            "[Epoch 1/200] [Batch 641/3166] [D loss: 1.680957] [G loss: 11.933846] [gradient_penalty: 0.009431]\n",
            "[Epoch 1/200] [Batch 642/3166] [D loss: 0.527606] [G loss: 12.268797] [gradient_penalty: 0.008799]\n",
            "[Epoch 1/200] [Batch 643/3166] [D loss: 0.180369] [G loss: 12.373249] [gradient_penalty: 0.012059]\n",
            "[Epoch 1/200] [Batch 644/3166] [D loss: 1.678404] [G loss: 11.831939] [gradient_penalty: 0.008554]\n",
            "[Epoch 1/200] [Batch 645/3166] [D loss: 1.349455] [G loss: 11.804226] [gradient_penalty: 0.006655]\n",
            "[Epoch 1/200] [Batch 646/3166] [D loss: 0.718849] [G loss: 11.629898] [gradient_penalty: 0.007520]\n",
            "[Epoch 1/200] [Batch 647/3166] [D loss: 1.486788] [G loss: 12.180981] [gradient_penalty: 0.005198]\n",
            "[Epoch 1/200] [Batch 648/3166] [D loss: 0.713681] [G loss: 11.114000] [gradient_penalty: 0.008617]\n",
            "[Epoch 1/200] [Batch 649/3166] [D loss: 2.072445] [G loss: 11.852826] [gradient_penalty: 0.008845]\n",
            "[Epoch 1/200] [Batch 650/3166] [D loss: 0.657755] [G loss: 11.781281] [gradient_penalty: 0.007055]\n",
            "[Epoch 1/200] [Batch 651/3166] [D loss: 1.086590] [G loss: 11.724975] [gradient_penalty: 0.010956]\n",
            "[Epoch 1/200] [Batch 652/3166] [D loss: 0.236772] [G loss: 12.653685] [gradient_penalty: 0.006919]\n",
            "[Epoch 1/200] [Batch 653/3166] [D loss: 1.519892] [G loss: 11.277884] [gradient_penalty: 0.006182]\n",
            "[Epoch 1/200] [Batch 654/3166] [D loss: 0.662764] [G loss: 11.757490] [gradient_penalty: 0.007181]\n",
            "[Epoch 1/200] [Batch 655/3166] [D loss: 0.889100] [G loss: 12.426853] [gradient_penalty: 0.007286]\n",
            "[Epoch 1/200] [Batch 656/3166] [D loss: 0.745564] [G loss: 13.012327] [gradient_penalty: 0.009663]\n",
            "[Epoch 1/200] [Batch 657/3166] [D loss: 0.782989] [G loss: 12.491568] [gradient_penalty: 0.008482]\n",
            "[Epoch 1/200] [Batch 658/3166] [D loss: 0.758956] [G loss: 11.757572] [gradient_penalty: 0.010983]\n",
            "[Epoch 1/200] [Batch 659/3166] [D loss: 1.082014] [G loss: 12.131920] [gradient_penalty: 0.006473]\n",
            "[Epoch 1/200] [Batch 660/3166] [D loss: 0.804394] [G loss: 12.623777] [gradient_penalty: 0.005539]\n",
            "[Epoch 1/200] [Batch 661/3166] [D loss: 0.851665] [G loss: 12.405199] [gradient_penalty: 0.005955]\n",
            "[Epoch 1/200] [Batch 662/3166] [D loss: 1.034992] [G loss: 11.904210] [gradient_penalty: 0.007211]\n",
            "[Epoch 1/200] [Batch 663/3166] [D loss: 1.399207] [G loss: 12.093129] [gradient_penalty: 0.010547]\n",
            "[Epoch 1/200] [Batch 664/3166] [D loss: 1.188416] [G loss: 12.449112] [gradient_penalty: 0.007024]\n",
            "[Epoch 1/200] [Batch 665/3166] [D loss: 0.496867] [G loss: 11.894001] [gradient_penalty: 0.008257]\n",
            "[Epoch 1/200] [Batch 666/3166] [D loss: 0.977978] [G loss: 12.400080] [gradient_penalty: 0.008715]\n",
            "[Epoch 1/200] [Batch 667/3166] [D loss: 1.543654] [G loss: 12.602036] [gradient_penalty: 0.007588]\n",
            "[Epoch 1/200] [Batch 668/3166] [D loss: 1.059948] [G loss: 12.497012] [gradient_penalty: 0.007744]\n",
            "[Epoch 1/200] [Batch 669/3166] [D loss: 0.731875] [G loss: 12.280544] [gradient_penalty: 0.008924]\n",
            "[Epoch 1/200] [Batch 670/3166] [D loss: 1.435813] [G loss: 12.625402] [gradient_penalty: 0.007678]\n",
            "[Epoch 1/200] [Batch 671/3166] [D loss: 0.078777] [G loss: 12.956778] [gradient_penalty: 0.009088]\n",
            "[Epoch 1/200] [Batch 672/3166] [D loss: 2.144367] [G loss: 12.436306] [gradient_penalty: 0.009104]\n",
            "[Epoch 1/200] [Batch 673/3166] [D loss: 1.357121] [G loss: 11.896138] [gradient_penalty: 0.010022]\n",
            "[Epoch 1/200] [Batch 674/3166] [D loss: 0.776963] [G loss: 12.546110] [gradient_penalty: 0.007047]\n",
            "[Epoch 1/200] [Batch 675/3166] [D loss: 1.754652] [G loss: 11.595688] [gradient_penalty: 0.006724]\n",
            "[Epoch 1/200] [Batch 676/3166] [D loss: 0.863764] [G loss: 12.373451] [gradient_penalty: 0.009452]\n",
            "[Epoch 1/200] [Batch 677/3166] [D loss: 0.952565] [G loss: 11.841037] [gradient_penalty: 0.007862]\n",
            "[Epoch 1/200] [Batch 678/3166] [D loss: 0.670429] [G loss: 12.325758] [gradient_penalty: 0.009019]\n",
            "[Epoch 1/200] [Batch 679/3166] [D loss: 0.998969] [G loss: 12.296257] [gradient_penalty: 0.007765]\n",
            "[Epoch 1/200] [Batch 680/3166] [D loss: 0.323760] [G loss: 12.148161] [gradient_penalty: 0.009136]\n",
            "[Epoch 1/200] [Batch 681/3166] [D loss: 1.468878] [G loss: 12.076595] [gradient_penalty: 0.007408]\n",
            "[Epoch 1/200] [Batch 682/3166] [D loss: 0.879662] [G loss: 12.400145] [gradient_penalty: 0.009117]\n",
            "[Epoch 1/200] [Batch 683/3166] [D loss: 0.647031] [G loss: 11.612459] [gradient_penalty: 0.008247]\n",
            "[Epoch 1/200] [Batch 684/3166] [D loss: 1.054658] [G loss: 12.034393] [gradient_penalty: 0.007553]\n",
            "[Epoch 1/200] [Batch 685/3166] [D loss: 0.711926] [G loss: 12.070269] [gradient_penalty: 0.008369]\n",
            "[Epoch 1/200] [Batch 686/3166] [D loss: 1.275053] [G loss: 12.239913] [gradient_penalty: 0.007941]\n",
            "[Epoch 1/200] [Batch 687/3166] [D loss: -0.221746] [G loss: 13.176204] [gradient_penalty: 0.007881]\n",
            "[Epoch 1/200] [Batch 688/3166] [D loss: 0.847972] [G loss: 13.089235] [gradient_penalty: 0.007613]\n",
            "[Epoch 1/200] [Batch 689/3166] [D loss: 2.501589] [G loss: 11.786509] [gradient_penalty: 0.009975]\n",
            "[Epoch 1/200] [Batch 690/3166] [D loss: 0.117037] [G loss: 12.227693] [gradient_penalty: 0.008817]\n",
            "[Epoch 1/200] [Batch 691/3166] [D loss: -0.371847] [G loss: 12.344633] [gradient_penalty: 0.007556]\n",
            "[Epoch 1/200] [Batch 692/3166] [D loss: 1.403846] [G loss: 12.358355] [gradient_penalty: 0.008404]\n",
            "[Epoch 1/200] [Batch 693/3166] [D loss: 0.804698] [G loss: 12.313669] [gradient_penalty: 0.007206]\n",
            "[Epoch 1/200] [Batch 694/3166] [D loss: 1.268913] [G loss: 12.801031] [gradient_penalty: 0.011280]\n",
            "[Epoch 1/200] [Batch 695/3166] [D loss: -0.148784] [G loss: 12.837951] [gradient_penalty: 0.007104]\n",
            "[Epoch 1/200] [Batch 696/3166] [D loss: 1.182250] [G loss: 12.764216] [gradient_penalty: 0.008302]\n",
            "[Epoch 1/200] [Batch 697/3166] [D loss: 1.693976] [G loss: 12.408686] [gradient_penalty: 0.007477]\n",
            "[Epoch 1/200] [Batch 698/3166] [D loss: 0.541911] [G loss: 12.829487] [gradient_penalty: 0.006283]\n",
            "[Epoch 1/200] [Batch 699/3166] [D loss: 1.479903] [G loss: 12.193367] [gradient_penalty: 0.008201]\n",
            "[Epoch 1/200] [Batch 700/3166] [D loss: 2.149332] [G loss: 12.172340] [gradient_penalty: 0.007498]\n",
            "[Epoch 1/200] [Batch 701/3166] [D loss: 0.613641] [G loss: 12.729641] [gradient_penalty: 0.007082]\n",
            "[Epoch 1/200] [Batch 702/3166] [D loss: 0.818374] [G loss: 12.475309] [gradient_penalty: 0.008646]\n",
            "[Epoch 1/200] [Batch 703/3166] [D loss: 0.805774] [G loss: 12.803153] [gradient_penalty: 0.007926]\n",
            "[Epoch 1/200] [Batch 704/3166] [D loss: 1.060741] [G loss: 12.377293] [gradient_penalty: 0.007129]\n",
            "[Epoch 1/200] [Batch 705/3166] [D loss: 1.706581] [G loss: 13.220170] [gradient_penalty: 0.011930]\n",
            "[Epoch 1/200] [Batch 706/3166] [D loss: 1.096815] [G loss: 12.460922] [gradient_penalty: 0.010409]\n",
            "[Epoch 1/200] [Batch 707/3166] [D loss: 1.243476] [G loss: 11.645563] [gradient_penalty: 0.008591]\n",
            "[Epoch 1/200] [Batch 708/3166] [D loss: 0.660205] [G loss: 11.694525] [gradient_penalty: 0.004950]\n",
            "[Epoch 1/200] [Batch 709/3166] [D loss: 1.940941] [G loss: 12.277089] [gradient_penalty: 0.008036]\n",
            "[Epoch 1/200] [Batch 710/3166] [D loss: -0.273303] [G loss: 12.239819] [gradient_penalty: 0.006344]\n",
            "[Epoch 1/200] [Batch 711/3166] [D loss: 0.871880] [G loss: 11.898424] [gradient_penalty: 0.009597]\n",
            "[Epoch 1/200] [Batch 712/3166] [D loss: 1.491836] [G loss: 12.540225] [gradient_penalty: 0.007038]\n",
            "[Epoch 1/200] [Batch 713/3166] [D loss: 0.776760] [G loss: 11.795080] [gradient_penalty: 0.008452]\n",
            "[Epoch 1/200] [Batch 714/3166] [D loss: 0.382877] [G loss: 11.171726] [gradient_penalty: 0.007484]\n",
            "[Epoch 1/200] [Batch 715/3166] [D loss: 0.570826] [G loss: 12.767092] [gradient_penalty: 0.005306]\n",
            "[Epoch 1/200] [Batch 716/3166] [D loss: -0.290688] [G loss: 12.025791] [gradient_penalty: 0.008928]\n",
            "[Epoch 1/200] [Batch 717/3166] [D loss: 1.438312] [G loss: 11.403869] [gradient_penalty: 0.006139]\n",
            "[Epoch 1/200] [Batch 718/3166] [D loss: 1.603473] [G loss: 11.625713] [gradient_penalty: 0.007239]\n",
            "[Epoch 1/200] [Batch 719/3166] [D loss: 0.590793] [G loss: 11.736418] [gradient_penalty: 0.008206]\n",
            "[Epoch 1/200] [Batch 720/3166] [D loss: 1.182849] [G loss: 11.356582] [gradient_penalty: 0.006765]\n",
            "[Epoch 1/200] [Batch 721/3166] [D loss: -0.327456] [G loss: 11.102306] [gradient_penalty: 0.005591]\n",
            "[Epoch 1/200] [Batch 722/3166] [D loss: 1.111236] [G loss: 11.389076] [gradient_penalty: 0.008677]\n",
            "[Epoch 1/200] [Batch 723/3166] [D loss: 1.104454] [G loss: 12.181728] [gradient_penalty: 0.006406]\n",
            "[Epoch 1/200] [Batch 724/3166] [D loss: 1.104804] [G loss: 11.221334] [gradient_penalty: 0.006206]\n",
            "[Epoch 1/200] [Batch 725/3166] [D loss: 1.072922] [G loss: 11.579210] [gradient_penalty: 0.008735]\n",
            "[Epoch 1/200] [Batch 726/3166] [D loss: 0.152893] [G loss: 12.340425] [gradient_penalty: 0.007564]\n",
            "[Epoch 1/200] [Batch 727/3166] [D loss: 0.945846] [G loss: 12.778290] [gradient_penalty: 0.009225]\n",
            "[Epoch 1/200] [Batch 728/3166] [D loss: 0.752821] [G loss: 12.220559] [gradient_penalty: 0.006977]\n",
            "[Epoch 1/200] [Batch 729/3166] [D loss: 1.133050] [G loss: 12.250858] [gradient_penalty: 0.010033]\n",
            "[Epoch 1/200] [Batch 730/3166] [D loss: 0.162769] [G loss: 12.772886] [gradient_penalty: 0.009515]\n",
            "[Epoch 1/200] [Batch 731/3166] [D loss: 0.726658] [G loss: 13.048656] [gradient_penalty: 0.009218]\n",
            "[Epoch 1/200] [Batch 732/3166] [D loss: 0.710307] [G loss: 12.766394] [gradient_penalty: 0.010227]\n",
            "[Epoch 1/200] [Batch 733/3166] [D loss: 1.586856] [G loss: 13.005157] [gradient_penalty: 0.007644]\n",
            "[Epoch 1/200] [Batch 734/3166] [D loss: 0.628042] [G loss: 12.130836] [gradient_penalty: 0.008959]\n",
            "log\n",
            "[Epoch 1/200] [Batch 735/3166] [D loss: 1.388656] [G loss: 12.795050] [gradient_penalty: 0.008780]\n",
            "[Epoch 1/200] [Batch 736/3166] [D loss: 1.519550] [G loss: 12.049105] [gradient_penalty: 0.011556]\n",
            "[Epoch 1/200] [Batch 737/3166] [D loss: 2.433507] [G loss: 12.751874] [gradient_penalty: 0.008887]\n",
            "[Epoch 1/200] [Batch 738/3166] [D loss: 0.166348] [G loss: 12.098457] [gradient_penalty: 0.007327]\n",
            "[Epoch 1/200] [Batch 739/3166] [D loss: 1.536327] [G loss: 12.663349] [gradient_penalty: 0.008937]\n",
            "[Epoch 1/200] [Batch 740/3166] [D loss: 2.361393] [G loss: 11.937758] [gradient_penalty: 0.006865]\n",
            "[Epoch 1/200] [Batch 741/3166] [D loss: 1.025156] [G loss: 12.640632] [gradient_penalty: 0.009135]\n",
            "[Epoch 1/200] [Batch 742/3166] [D loss: 1.070618] [G loss: 12.063928] [gradient_penalty: 0.005417]\n",
            "[Epoch 1/200] [Batch 743/3166] [D loss: 0.605126] [G loss: 11.897993] [gradient_penalty: 0.010733]\n",
            "[Epoch 1/200] [Batch 744/3166] [D loss: 1.107964] [G loss: 11.728246] [gradient_penalty: 0.008164]\n",
            "[Epoch 1/200] [Batch 745/3166] [D loss: 1.441630] [G loss: 12.072044] [gradient_penalty: 0.008171]\n",
            "[Epoch 1/200] [Batch 746/3166] [D loss: 0.978566] [G loss: 11.895628] [gradient_penalty: 0.009491]\n",
            "[Epoch 1/200] [Batch 747/3166] [D loss: 1.573101] [G loss: 12.045427] [gradient_penalty: 0.010267]\n",
            "[Epoch 1/200] [Batch 748/3166] [D loss: 0.367724] [G loss: 11.803479] [gradient_penalty: 0.008121]\n",
            "[Epoch 1/200] [Batch 749/3166] [D loss: -0.005914] [G loss: 11.679347] [gradient_penalty: 0.007995]\n",
            "[Epoch 1/200] [Batch 750/3166] [D loss: 0.109000] [G loss: 11.181276] [gradient_penalty: 0.006637]\n",
            "[Epoch 1/200] [Batch 751/3166] [D loss: 1.168704] [G loss: 12.449934] [gradient_penalty: 0.006792]\n",
            "[Epoch 1/200] [Batch 752/3166] [D loss: 0.580035] [G loss: 11.359957] [gradient_penalty: 0.007722]\n",
            "[Epoch 1/200] [Batch 753/3166] [D loss: 0.025667] [G loss: 12.504608] [gradient_penalty: 0.009191]\n",
            "[Epoch 1/200] [Batch 754/3166] [D loss: 0.648100] [G loss: 11.510517] [gradient_penalty: 0.008874]\n",
            "[Epoch 1/200] [Batch 755/3166] [D loss: 0.923215] [G loss: 12.371272] [gradient_penalty: 0.007330]\n",
            "[Epoch 1/200] [Batch 756/3166] [D loss: 0.760639] [G loss: 12.441905] [gradient_penalty: 0.006634]\n",
            "[Epoch 1/200] [Batch 757/3166] [D loss: 0.344490] [G loss: 12.494203] [gradient_penalty: 0.006195]\n",
            "[Epoch 1/200] [Batch 758/3166] [D loss: 1.561890] [G loss: 12.263824] [gradient_penalty: 0.007803]\n",
            "[Epoch 1/200] [Batch 759/3166] [D loss: 0.451248] [G loss: 12.025290] [gradient_penalty: 0.006658]\n",
            "[Epoch 1/200] [Batch 760/3166] [D loss: 0.986874] [G loss: 11.912260] [gradient_penalty: 0.006582]\n",
            "[Epoch 1/200] [Batch 761/3166] [D loss: 2.204353] [G loss: 12.266258] [gradient_penalty: 0.008501]\n",
            "[Epoch 1/200] [Batch 762/3166] [D loss: 1.090812] [G loss: 12.767569] [gradient_penalty: 0.010340]\n",
            "[Epoch 1/200] [Batch 763/3166] [D loss: 0.219887] [G loss: 12.005400] [gradient_penalty: 0.006183]\n",
            "[Epoch 1/200] [Batch 764/3166] [D loss: 2.365772] [G loss: 11.832457] [gradient_penalty: 0.006356]\n",
            "[Epoch 1/200] [Batch 765/3166] [D loss: 0.679118] [G loss: 11.865031] [gradient_penalty: 0.008711]\n",
            "[Epoch 1/200] [Batch 766/3166] [D loss: 0.781440] [G loss: 12.052088] [gradient_penalty: 0.011631]\n",
            "[Epoch 1/200] [Batch 767/3166] [D loss: 0.904174] [G loss: 12.179262] [gradient_penalty: 0.007980]\n",
            "[Epoch 1/200] [Batch 768/3166] [D loss: 1.212560] [G loss: 12.498004] [gradient_penalty: 0.008312]\n",
            "[Epoch 1/200] [Batch 769/3166] [D loss: 2.138296] [G loss: 11.953180] [gradient_penalty: 0.009279]\n",
            "[Epoch 1/200] [Batch 770/3166] [D loss: 0.532671] [G loss: 11.540423] [gradient_penalty: 0.006900]\n",
            "[Epoch 1/200] [Batch 771/3166] [D loss: 1.357215] [G loss: 11.819362] [gradient_penalty: 0.010934]\n",
            "[Epoch 1/200] [Batch 772/3166] [D loss: 2.093343] [G loss: 11.995618] [gradient_penalty: 0.007879]\n",
            "[Epoch 1/200] [Batch 773/3166] [D loss: 0.684617] [G loss: 12.107905] [gradient_penalty: 0.006767]\n",
            "[Epoch 1/200] [Batch 774/3166] [D loss: 1.016722] [G loss: 12.416557] [gradient_penalty: 0.007568]\n",
            "[Epoch 1/200] [Batch 775/3166] [D loss: 0.917567] [G loss: 12.230249] [gradient_penalty: 0.007177]\n",
            "[Epoch 1/200] [Batch 776/3166] [D loss: 0.219620] [G loss: 12.081377] [gradient_penalty: 0.008878]\n",
            "[Epoch 1/200] [Batch 777/3166] [D loss: 0.088023] [G loss: 12.419029] [gradient_penalty: 0.007417]\n",
            "[Epoch 1/200] [Batch 778/3166] [D loss: 1.371790] [G loss: 12.221057] [gradient_penalty: 0.007961]\n",
            "[Epoch 1/200] [Batch 779/3166] [D loss: 0.983353] [G loss: 12.340710] [gradient_penalty: 0.007553]\n",
            "[Epoch 1/200] [Batch 780/3166] [D loss: 0.236761] [G loss: 12.779548] [gradient_penalty: 0.006229]\n",
            "[Epoch 1/200] [Batch 781/3166] [D loss: 0.754505] [G loss: 12.197104] [gradient_penalty: 0.009020]\n",
            "[Epoch 1/200] [Batch 782/3166] [D loss: 1.578449] [G loss: 12.490656] [gradient_penalty: 0.009323]\n",
            "[Epoch 1/200] [Batch 783/3166] [D loss: 1.461516] [G loss: 12.228186] [gradient_penalty: 0.008965]\n",
            "[Epoch 1/200] [Batch 784/3166] [D loss: 2.458831] [G loss: 12.211460] [gradient_penalty: 0.007469]\n",
            "[Epoch 1/200] [Batch 785/3166] [D loss: 1.002552] [G loss: 12.312778] [gradient_penalty: 0.009234]\n",
            "[Epoch 1/200] [Batch 786/3166] [D loss: 1.726131] [G loss: 12.202031] [gradient_penalty: 0.007004]\n",
            "[Epoch 1/200] [Batch 787/3166] [D loss: 0.684685] [G loss: 13.121634] [gradient_penalty: 0.008021]\n",
            "[Epoch 1/200] [Batch 788/3166] [D loss: 0.717185] [G loss: 12.823654] [gradient_penalty: 0.008392]\n",
            "[Epoch 1/200] [Batch 789/3166] [D loss: 0.313178] [G loss: 12.884155] [gradient_penalty: 0.008560]\n",
            "[Epoch 1/200] [Batch 790/3166] [D loss: 0.999191] [G loss: 12.327242] [gradient_penalty: 0.009496]\n",
            "[Epoch 1/200] [Batch 791/3166] [D loss: 1.149449] [G loss: 12.662211] [gradient_penalty: 0.008359]\n",
            "[Epoch 1/200] [Batch 792/3166] [D loss: -0.307048] [G loss: 12.559009] [gradient_penalty: 0.008486]\n",
            "[Epoch 1/200] [Batch 793/3166] [D loss: 2.257225] [G loss: 13.826423] [gradient_penalty: 0.008370]\n",
            "[Epoch 1/200] [Batch 794/3166] [D loss: 1.179259] [G loss: 13.474178] [gradient_penalty: 0.009861]\n",
            "[Epoch 1/200] [Batch 795/3166] [D loss: 1.288912] [G loss: 12.997522] [gradient_penalty: 0.007365]\n",
            "[Epoch 1/200] [Batch 796/3166] [D loss: 1.880928] [G loss: 12.737160] [gradient_penalty: 0.009194]\n",
            "[Epoch 1/200] [Batch 797/3166] [D loss: 0.349097] [G loss: 12.681542] [gradient_penalty: 0.009347]\n",
            "[Epoch 1/200] [Batch 798/3166] [D loss: 1.791129] [G loss: 12.054891] [gradient_penalty: 0.007768]\n",
            "[Epoch 1/200] [Batch 799/3166] [D loss: 0.555621] [G loss: 12.762428] [gradient_penalty: 0.006371]\n",
            "[Epoch 1/200] [Batch 800/3166] [D loss: 0.803829] [G loss: 12.257025] [gradient_penalty: 0.007960]\n",
            "[Epoch 1/200] [Batch 801/3166] [D loss: 0.978408] [G loss: 13.114090] [gradient_penalty: 0.009164]\n",
            "[Epoch 1/200] [Batch 802/3166] [D loss: 1.932895] [G loss: 13.204451] [gradient_penalty: 0.008153]\n",
            "[Epoch 1/200] [Batch 803/3166] [D loss: 0.805638] [G loss: 12.260364] [gradient_penalty: 0.009317]\n",
            "[Epoch 1/200] [Batch 804/3166] [D loss: 1.352737] [G loss: 12.789206] [gradient_penalty: 0.011911]\n",
            "[Epoch 1/200] [Batch 805/3166] [D loss: 1.104770] [G loss: 12.113993] [gradient_penalty: 0.007793]\n",
            "[Epoch 1/200] [Batch 806/3166] [D loss: -0.237603] [G loss: 12.110368] [gradient_penalty: 0.007617]\n",
            "[Epoch 1/200] [Batch 807/3166] [D loss: 1.205222] [G loss: 13.207096] [gradient_penalty: 0.009365]\n",
            "[Epoch 1/200] [Batch 808/3166] [D loss: 0.507794] [G loss: 12.574150] [gradient_penalty: 0.009707]\n",
            "[Epoch 1/200] [Batch 809/3166] [D loss: 0.671799] [G loss: 12.750931] [gradient_penalty: 0.008453]\n",
            "[Epoch 1/200] [Batch 810/3166] [D loss: 1.519892] [G loss: 12.607971] [gradient_penalty: 0.006442]\n",
            "[Epoch 1/200] [Batch 811/3166] [D loss: 0.249892] [G loss: 12.145485] [gradient_penalty: 0.007117]\n",
            "[Epoch 1/200] [Batch 812/3166] [D loss: 0.966784] [G loss: 12.237583] [gradient_penalty: 0.008760]\n",
            "[Epoch 1/200] [Batch 813/3166] [D loss: 0.597091] [G loss: 12.394119] [gradient_penalty: 0.008892]\n",
            "[Epoch 1/200] [Batch 814/3166] [D loss: 2.445259] [G loss: 12.688839] [gradient_penalty: 0.008522]\n",
            "[Epoch 1/200] [Batch 815/3166] [D loss: 1.611658] [G loss: 11.907739] [gradient_penalty: 0.009395]\n",
            "[Epoch 1/200] [Batch 816/3166] [D loss: 0.790288] [G loss: 12.093045] [gradient_penalty: 0.009496]\n",
            "[Epoch 1/200] [Batch 817/3166] [D loss: 2.096347] [G loss: 12.797171] [gradient_penalty: 0.008087]\n",
            "[Epoch 1/200] [Batch 818/3166] [D loss: 1.353676] [G loss: 12.469597] [gradient_penalty: 0.006878]\n",
            "[Epoch 1/200] [Batch 819/3166] [D loss: 0.399934] [G loss: 11.844282] [gradient_penalty: 0.008869]\n",
            "[Epoch 1/200] [Batch 820/3166] [D loss: 1.490250] [G loss: 13.701921] [gradient_penalty: 0.006977]\n",
            "[Epoch 1/200] [Batch 821/3166] [D loss: 1.922121] [G loss: 13.019423] [gradient_penalty: 0.007017]\n",
            "[Epoch 1/200] [Batch 822/3166] [D loss: 0.509384] [G loss: 12.702953] [gradient_penalty: 0.005759]\n",
            "[Epoch 1/200] [Batch 823/3166] [D loss: 1.336460] [G loss: 12.986850] [gradient_penalty: 0.006831]\n",
            "[Epoch 1/200] [Batch 824/3166] [D loss: 0.282943] [G loss: 13.200394] [gradient_penalty: 0.010900]\n",
            "[Epoch 1/200] [Batch 825/3166] [D loss: 0.686488] [G loss: 12.695660] [gradient_penalty: 0.008852]\n",
            "[Epoch 1/200] [Batch 826/3166] [D loss: 1.745329] [G loss: 13.521668] [gradient_penalty: 0.010475]\n",
            "[Epoch 1/200] [Batch 827/3166] [D loss: 1.270724] [G loss: 13.125263] [gradient_penalty: 0.008924]\n",
            "[Epoch 1/200] [Batch 828/3166] [D loss: 0.955516] [G loss: 13.159067] [gradient_penalty: 0.007138]\n",
            "[Epoch 1/200] [Batch 829/3166] [D loss: 1.195890] [G loss: 12.602069] [gradient_penalty: 0.008020]\n",
            "[Epoch 1/200] [Batch 830/3166] [D loss: 1.416126] [G loss: 12.121099] [gradient_penalty: 0.009188]\n",
            "[Epoch 1/200] [Batch 831/3166] [D loss: 2.230829] [G loss: 12.826500] [gradient_penalty: 0.011549]\n",
            "[Epoch 1/200] [Batch 832/3166] [D loss: 0.810071] [G loss: 12.294237] [gradient_penalty: 0.007660]\n",
            "[Epoch 1/200] [Batch 833/3166] [D loss: 0.889184] [G loss: 12.321815] [gradient_penalty: 0.009297]\n",
            "[Epoch 1/200] [Batch 834/3166] [D loss: 0.689449] [G loss: 12.452154] [gradient_penalty: 0.010895]\n",
            "log\n",
            "[Epoch 1/200] [Batch 835/3166] [D loss: 1.436686] [G loss: 12.104612] [gradient_penalty: 0.007066]\n",
            "[Epoch 1/200] [Batch 836/3166] [D loss: 1.240465] [G loss: 12.182663] [gradient_penalty: 0.007651]\n",
            "[Epoch 1/200] [Batch 837/3166] [D loss: 1.308040] [G loss: 12.222078] [gradient_penalty: 0.006671]\n",
            "[Epoch 1/200] [Batch 838/3166] [D loss: 1.370549] [G loss: 12.393494] [gradient_penalty: 0.010270]\n",
            "[Epoch 1/200] [Batch 839/3166] [D loss: 1.626269] [G loss: 12.976995] [gradient_penalty: 0.005214]\n",
            "[Epoch 1/200] [Batch 840/3166] [D loss: 0.308819] [G loss: 13.349684] [gradient_penalty: 0.006807]\n",
            "[Epoch 1/200] [Batch 841/3166] [D loss: 1.174788] [G loss: 12.294624] [gradient_penalty: 0.007988]\n",
            "[Epoch 1/200] [Batch 842/3166] [D loss: 0.143989] [G loss: 12.348911] [gradient_penalty: 0.007477]\n",
            "[Epoch 1/200] [Batch 843/3166] [D loss: 0.678986] [G loss: 12.828926] [gradient_penalty: 0.008732]\n",
            "[Epoch 1/200] [Batch 844/3166] [D loss: 0.480040] [G loss: 12.919463] [gradient_penalty: 0.010112]\n",
            "[Epoch 1/200] [Batch 845/3166] [D loss: 0.305349] [G loss: 11.865263] [gradient_penalty: 0.008201]\n",
            "[Epoch 1/200] [Batch 846/3166] [D loss: 1.726518] [G loss: 11.914596] [gradient_penalty: 0.006126]\n",
            "[Epoch 1/200] [Batch 847/3166] [D loss: 1.786459] [G loss: 12.832201] [gradient_penalty: 0.007783]\n",
            "[Epoch 1/200] [Batch 848/3166] [D loss: 0.612838] [G loss: 12.343349] [gradient_penalty: 0.009464]\n",
            "[Epoch 1/200] [Batch 849/3166] [D loss: 1.554060] [G loss: 12.301167] [gradient_penalty: 0.006895]\n",
            "[Epoch 1/200] [Batch 850/3166] [D loss: 2.036788] [G loss: 12.443174] [gradient_penalty: 0.008781]\n",
            "[Epoch 1/200] [Batch 851/3166] [D loss: -0.386242] [G loss: 12.307367] [gradient_penalty: 0.009097]\n",
            "[Epoch 1/200] [Batch 852/3166] [D loss: 1.059666] [G loss: 12.507352] [gradient_penalty: 0.007134]\n",
            "[Epoch 1/200] [Batch 853/3166] [D loss: 0.013380] [G loss: 12.690065] [gradient_penalty: 0.006186]\n",
            "[Epoch 1/200] [Batch 854/3166] [D loss: 2.158392] [G loss: 13.121864] [gradient_penalty: 0.008370]\n",
            "[Epoch 1/200] [Batch 855/3166] [D loss: 0.098714] [G loss: 12.312290] [gradient_penalty: 0.008756]\n",
            "[Epoch 1/200] [Batch 856/3166] [D loss: 1.514634] [G loss: 13.413432] [gradient_penalty: 0.009351]\n",
            "[Epoch 1/200] [Batch 857/3166] [D loss: 1.155600] [G loss: 13.114646] [gradient_penalty: 0.011263]\n",
            "[Epoch 1/200] [Batch 858/3166] [D loss: 1.009429] [G loss: 12.178939] [gradient_penalty: 0.007791]\n",
            "[Epoch 1/200] [Batch 859/3166] [D loss: 1.369891] [G loss: 12.218361] [gradient_penalty: 0.009084]\n",
            "[Epoch 1/200] [Batch 860/3166] [D loss: 0.673098] [G loss: 11.930149] [gradient_penalty: 0.011016]\n",
            "[Epoch 1/200] [Batch 861/3166] [D loss: 1.282409] [G loss: 12.626213] [gradient_penalty: 0.007351]\n",
            "[Epoch 1/200] [Batch 862/3166] [D loss: 2.532881] [G loss: 12.580473] [gradient_penalty: 0.011540]\n",
            "[Epoch 1/200] [Batch 863/3166] [D loss: 0.505492] [G loss: 12.759605] [gradient_penalty: 0.011072]\n",
            "[Epoch 1/200] [Batch 864/3166] [D loss: 0.786958] [G loss: 12.471171] [gradient_penalty: 0.006662]\n",
            "[Epoch 1/200] [Batch 865/3166] [D loss: 1.295405] [G loss: 12.068668] [gradient_penalty: 0.009648]\n",
            "[Epoch 1/200] [Batch 866/3166] [D loss: 1.035638] [G loss: 11.924688] [gradient_penalty: 0.005466]\n",
            "[Epoch 1/200] [Batch 867/3166] [D loss: 1.213169] [G loss: 12.352478] [gradient_penalty: 0.006094]\n",
            "[Epoch 1/200] [Batch 868/3166] [D loss: 1.327374] [G loss: 12.448145] [gradient_penalty: 0.006221]\n",
            "[Epoch 1/200] [Batch 869/3166] [D loss: 1.012539] [G loss: 11.975810] [gradient_penalty: 0.007170]\n",
            "[Epoch 1/200] [Batch 870/3166] [D loss: 1.104206] [G loss: 12.551950] [gradient_penalty: 0.007604]\n",
            "[Epoch 1/200] [Batch 871/3166] [D loss: 0.640219] [G loss: 12.225567] [gradient_penalty: 0.008011]\n",
            "[Epoch 1/200] [Batch 872/3166] [D loss: 1.326695] [G loss: 12.184319] [gradient_penalty: 0.010750]\n",
            "[Epoch 1/200] [Batch 873/3166] [D loss: 1.059744] [G loss: 12.681740] [gradient_penalty: 0.005705]\n",
            "[Epoch 1/200] [Batch 874/3166] [D loss: -0.127981] [G loss: 11.636599] [gradient_penalty: 0.008025]\n",
            "[Epoch 1/200] [Batch 875/3166] [D loss: 2.323629] [G loss: 11.928997] [gradient_penalty: 0.008313]\n",
            "[Epoch 1/200] [Batch 876/3166] [D loss: 1.527088] [G loss: 11.896985] [gradient_penalty: 0.007731]\n",
            "[Epoch 1/200] [Batch 877/3166] [D loss: 1.052425] [G loss: 11.861872] [gradient_penalty: 0.005832]\n",
            "[Epoch 1/200] [Batch 878/3166] [D loss: 0.857149] [G loss: 12.084492] [gradient_penalty: 0.006469]\n",
            "[Epoch 1/200] [Batch 879/3166] [D loss: 0.113394] [G loss: 11.901686] [gradient_penalty: 0.008503]\n",
            "[Epoch 1/200] [Batch 880/3166] [D loss: 1.810445] [G loss: 12.741880] [gradient_penalty: 0.008312]\n",
            "[Epoch 1/200] [Batch 881/3166] [D loss: 0.326889] [G loss: 12.741001] [gradient_penalty: 0.009570]\n",
            "[Epoch 1/200] [Batch 882/3166] [D loss: 0.478131] [G loss: 12.092453] [gradient_penalty: 0.009594]\n",
            "[Epoch 1/200] [Batch 883/3166] [D loss: 0.242626] [G loss: 12.704465] [gradient_penalty: 0.008240]\n",
            "[Epoch 1/200] [Batch 884/3166] [D loss: 1.621288] [G loss: 13.244686] [gradient_penalty: 0.010543]\n",
            "[Epoch 1/200] [Batch 885/3166] [D loss: 0.963636] [G loss: 13.552308] [gradient_penalty: 0.010479]\n",
            "[Epoch 1/200] [Batch 886/3166] [D loss: 0.531274] [G loss: 13.893909] [gradient_penalty: 0.006680]\n",
            "[Epoch 1/200] [Batch 887/3166] [D loss: 0.851369] [G loss: 13.205212] [gradient_penalty: 0.010673]\n",
            "[Epoch 1/200] [Batch 888/3166] [D loss: 0.938591] [G loss: 13.546562] [gradient_penalty: 0.008247]\n",
            "[Epoch 1/200] [Batch 889/3166] [D loss: 0.903969] [G loss: 13.216983] [gradient_penalty: 0.008094]\n",
            "[Epoch 1/200] [Batch 890/3166] [D loss: 0.094995] [G loss: 13.331450] [gradient_penalty: 0.005097]\n",
            "[Epoch 1/200] [Batch 891/3166] [D loss: 0.843383] [G loss: 13.518478] [gradient_penalty: 0.009048]\n",
            "[Epoch 1/200] [Batch 892/3166] [D loss: 0.628545] [G loss: 12.982258] [gradient_penalty: 0.010408]\n",
            "[Epoch 1/200] [Batch 893/3166] [D loss: 1.710860] [G loss: 13.289173] [gradient_penalty: 0.008813]\n",
            "[Epoch 1/200] [Batch 894/3166] [D loss: 1.211768] [G loss: 13.149284] [gradient_penalty: 0.009162]\n",
            "[Epoch 1/200] [Batch 895/3166] [D loss: 1.015609] [G loss: 12.385469] [gradient_penalty: 0.006631]\n",
            "[Epoch 1/200] [Batch 896/3166] [D loss: 1.267304] [G loss: 12.538429] [gradient_penalty: 0.009134]\n",
            "[Epoch 1/200] [Batch 897/3166] [D loss: 0.994734] [G loss: 12.428690] [gradient_penalty: 0.006468]\n",
            "[Epoch 1/200] [Batch 898/3166] [D loss: 0.583201] [G loss: 12.529750] [gradient_penalty: 0.010104]\n",
            "[Epoch 1/200] [Batch 899/3166] [D loss: 1.578592] [G loss: 12.217937] [gradient_penalty: 0.009495]\n",
            "[Epoch 1/200] [Batch 900/3166] [D loss: 0.529299] [G loss: 12.158346] [gradient_penalty: 0.007611]\n",
            "[Epoch 1/200] [Batch 901/3166] [D loss: 0.697092] [G loss: 12.997822] [gradient_penalty: 0.008357]\n",
            "[Epoch 1/200] [Batch 902/3166] [D loss: 1.414298] [G loss: 11.677095] [gradient_penalty: 0.006530]\n",
            "[Epoch 1/200] [Batch 903/3166] [D loss: 0.687265] [G loss: 12.542439] [gradient_penalty: 0.007174]\n",
            "[Epoch 1/200] [Batch 904/3166] [D loss: 0.523635] [G loss: 12.237345] [gradient_penalty: 0.007001]\n",
            "[Epoch 1/200] [Batch 905/3166] [D loss: -0.183118] [G loss: 12.127285] [gradient_penalty: 0.006457]\n",
            "[Epoch 1/200] [Batch 906/3166] [D loss: 0.749423] [G loss: 11.799280] [gradient_penalty: 0.007314]\n",
            "[Epoch 1/200] [Batch 907/3166] [D loss: 2.290671] [G loss: 11.939506] [gradient_penalty: 0.008791]\n",
            "[Epoch 1/200] [Batch 908/3166] [D loss: 0.444250] [G loss: 11.849293] [gradient_penalty: 0.007808]\n",
            "[Epoch 1/200] [Batch 909/3166] [D loss: 0.933223] [G loss: 12.689379] [gradient_penalty: 0.009406]\n",
            "[Epoch 1/200] [Batch 910/3166] [D loss: 0.887486] [G loss: 12.570610] [gradient_penalty: 0.007101]\n",
            "[Epoch 1/200] [Batch 911/3166] [D loss: 1.775125] [G loss: 13.164968] [gradient_penalty: 0.009616]\n",
            "[Epoch 1/200] [Batch 912/3166] [D loss: 0.515175] [G loss: 12.293888] [gradient_penalty: 0.009132]\n",
            "[Epoch 1/200] [Batch 913/3166] [D loss: 1.422543] [G loss: 12.070129] [gradient_penalty: 0.009258]\n",
            "[Epoch 1/200] [Batch 914/3166] [D loss: 0.520599] [G loss: 13.180061] [gradient_penalty: 0.009535]\n",
            "[Epoch 1/200] [Batch 915/3166] [D loss: 0.975109] [G loss: 13.445114] [gradient_penalty: 0.008461]\n",
            "[Epoch 1/200] [Batch 916/3166] [D loss: 1.532515] [G loss: 12.745935] [gradient_penalty: 0.006486]\n",
            "[Epoch 1/200] [Batch 917/3166] [D loss: 0.247744] [G loss: 13.903261] [gradient_penalty: 0.007188]\n",
            "[Epoch 1/200] [Batch 918/3166] [D loss: 0.640434] [G loss: 12.833551] [gradient_penalty: 0.008471]\n",
            "[Epoch 1/200] [Batch 919/3166] [D loss: -0.297632] [G loss: 13.621700] [gradient_penalty: 0.006438]\n",
            "[Epoch 1/200] [Batch 920/3166] [D loss: 0.374786] [G loss: 14.141446] [gradient_penalty: 0.006809]\n",
            "[Epoch 1/200] [Batch 921/3166] [D loss: 1.235042] [G loss: 13.361365] [gradient_penalty: 0.008023]\n",
            "[Epoch 1/200] [Batch 922/3166] [D loss: 0.047372] [G loss: 14.143860] [gradient_penalty: 0.010895]\n",
            "[Epoch 1/200] [Batch 923/3166] [D loss: 2.428691] [G loss: 13.079514] [gradient_penalty: 0.009064]\n",
            "[Epoch 1/200] [Batch 924/3166] [D loss: 1.105466] [G loss: 12.562135] [gradient_penalty: 0.007832]\n",
            "[Epoch 1/200] [Batch 925/3166] [D loss: 1.570753] [G loss: 12.555537] [gradient_penalty: 0.007653]\n",
            "[Epoch 1/200] [Batch 926/3166] [D loss: 1.082032] [G loss: 12.946587] [gradient_penalty: 0.006577]\n",
            "[Epoch 1/200] [Batch 927/3166] [D loss: 1.695508] [G loss: 12.946061] [gradient_penalty: 0.007850]\n",
            "[Epoch 1/200] [Batch 928/3166] [D loss: 1.945799] [G loss: 13.172003] [gradient_penalty: 0.007139]\n",
            "[Epoch 1/200] [Batch 929/3166] [D loss: 0.509576] [G loss: 12.336074] [gradient_penalty: 0.007520]\n",
            "[Epoch 1/200] [Batch 930/3166] [D loss: 1.685519] [G loss: 12.567879] [gradient_penalty: 0.006154]\n",
            "[Epoch 1/200] [Batch 931/3166] [D loss: 0.127555] [G loss: 13.004786] [gradient_penalty: 0.008737]\n",
            "[Epoch 1/200] [Batch 932/3166] [D loss: 1.050505] [G loss: 13.203112] [gradient_penalty: 0.009397]\n",
            "[Epoch 1/200] [Batch 933/3166] [D loss: 0.391031] [G loss: 13.231478] [gradient_penalty: 0.007630]\n",
            "[Epoch 1/200] [Batch 934/3166] [D loss: 0.384614] [G loss: 12.809670] [gradient_penalty: 0.008089]\n",
            "log\n",
            "[Epoch 1/200] [Batch 935/3166] [D loss: -0.563483] [G loss: 13.664722] [gradient_penalty: 0.007750]\n",
            "[Epoch 1/200] [Batch 936/3166] [D loss: 1.820204] [G loss: 12.592470] [gradient_penalty: 0.008584]\n",
            "[Epoch 1/200] [Batch 937/3166] [D loss: 1.002676] [G loss: 11.807016] [gradient_penalty: 0.009671]\n",
            "[Epoch 1/200] [Batch 938/3166] [D loss: 0.902080] [G loss: 12.989360] [gradient_penalty: 0.010995]\n",
            "[Epoch 1/200] [Batch 939/3166] [D loss: 0.361586] [G loss: 12.942603] [gradient_penalty: 0.007150]\n",
            "[Epoch 1/200] [Batch 940/3166] [D loss: 0.470289] [G loss: 13.815054] [gradient_penalty: 0.009988]\n",
            "[Epoch 1/200] [Batch 941/3166] [D loss: 1.389698] [G loss: 12.711529] [gradient_penalty: 0.008561]\n",
            "[Epoch 1/200] [Batch 942/3166] [D loss: 0.720736] [G loss: 13.017683] [gradient_penalty: 0.008084]\n",
            "[Epoch 1/200] [Batch 943/3166] [D loss: 0.150441] [G loss: 13.738564] [gradient_penalty: 0.008275]\n",
            "[Epoch 1/200] [Batch 944/3166] [D loss: 0.981218] [G loss: 13.157228] [gradient_penalty: 0.009675]\n",
            "[Epoch 1/200] [Batch 945/3166] [D loss: 1.327688] [G loss: 14.035072] [gradient_penalty: 0.007998]\n",
            "[Epoch 1/200] [Batch 946/3166] [D loss: 1.267713] [G loss: 13.019710] [gradient_penalty: 0.009502]\n",
            "[Epoch 1/200] [Batch 947/3166] [D loss: 1.573509] [G loss: 12.998390] [gradient_penalty: 0.008397]\n",
            "[Epoch 1/200] [Batch 948/3166] [D loss: 0.981575] [G loss: 13.182890] [gradient_penalty: 0.007521]\n",
            "[Epoch 1/200] [Batch 949/3166] [D loss: 0.321598] [G loss: 13.163822] [gradient_penalty: 0.009889]\n",
            "[Epoch 1/200] [Batch 950/3166] [D loss: 1.186635] [G loss: 13.878118] [gradient_penalty: 0.009621]\n",
            "[Epoch 1/200] [Batch 951/3166] [D loss: 1.289472] [G loss: 13.353771] [gradient_penalty: 0.006188]\n",
            "[Epoch 1/200] [Batch 952/3166] [D loss: 0.645201] [G loss: 13.648157] [gradient_penalty: 0.009484]\n",
            "[Epoch 1/200] [Batch 953/3166] [D loss: 0.001321] [G loss: 13.973265] [gradient_penalty: 0.006392]\n",
            "[Epoch 1/200] [Batch 954/3166] [D loss: 0.943208] [G loss: 13.330164] [gradient_penalty: 0.007376]\n",
            "[Epoch 1/200] [Batch 955/3166] [D loss: 1.264439] [G loss: 13.268578] [gradient_penalty: 0.010657]\n",
            "[Epoch 1/200] [Batch 956/3166] [D loss: 1.314402] [G loss: 13.009941] [gradient_penalty: 0.007791]\n",
            "[Epoch 1/200] [Batch 957/3166] [D loss: 2.138876] [G loss: 13.488372] [gradient_penalty: 0.005958]\n",
            "[Epoch 1/200] [Batch 958/3166] [D loss: 0.565226] [G loss: 14.335713] [gradient_penalty: 0.007378]\n",
            "[Epoch 1/200] [Batch 959/3166] [D loss: 1.997390] [G loss: 13.555014] [gradient_penalty: 0.008290]\n",
            "[Epoch 1/200] [Batch 960/3166] [D loss: 1.418013] [G loss: 13.487821] [gradient_penalty: 0.010021]\n",
            "[Epoch 1/200] [Batch 961/3166] [D loss: 0.254878] [G loss: 14.170822] [gradient_penalty: 0.008485]\n",
            "[Epoch 1/200] [Batch 962/3166] [D loss: 0.613391] [G loss: 13.283336] [gradient_penalty: 0.008704]\n",
            "[Epoch 1/200] [Batch 963/3166] [D loss: 1.683257] [G loss: 12.798933] [gradient_penalty: 0.006744]\n",
            "[Epoch 1/200] [Batch 964/3166] [D loss: 1.489362] [G loss: 12.973924] [gradient_penalty: 0.009438]\n",
            "[Epoch 1/200] [Batch 965/3166] [D loss: 2.135699] [G loss: 13.184919] [gradient_penalty: 0.007570]\n",
            "[Epoch 1/200] [Batch 966/3166] [D loss: 1.403454] [G loss: 13.897307] [gradient_penalty: 0.008243]\n",
            "[Epoch 1/200] [Batch 967/3166] [D loss: 1.097877] [G loss: 12.407152] [gradient_penalty: 0.008717]\n",
            "[Epoch 1/200] [Batch 968/3166] [D loss: 0.386632] [G loss: 13.281420] [gradient_penalty: 0.006233]\n",
            "[Epoch 1/200] [Batch 969/3166] [D loss: 0.195369] [G loss: 13.473938] [gradient_penalty: 0.005764]\n",
            "[Epoch 1/200] [Batch 970/3166] [D loss: 0.851683] [G loss: 13.281696] [gradient_penalty: 0.009106]\n",
            "[Epoch 1/200] [Batch 971/3166] [D loss: 1.299088] [G loss: 13.076511] [gradient_penalty: 0.009676]\n",
            "[Epoch 1/200] [Batch 972/3166] [D loss: 0.334624] [G loss: 13.404902] [gradient_penalty: 0.007416]\n",
            "[Epoch 1/200] [Batch 973/3166] [D loss: 0.490306] [G loss: 12.636976] [gradient_penalty: 0.009359]\n",
            "[Epoch 1/200] [Batch 974/3166] [D loss: 2.249449] [G loss: 12.826118] [gradient_penalty: 0.008475]\n",
            "[Epoch 1/200] [Batch 975/3166] [D loss: 1.019074] [G loss: 13.220846] [gradient_penalty: 0.009395]\n",
            "[Epoch 1/200] [Batch 976/3166] [D loss: 0.950217] [G loss: 13.807375] [gradient_penalty: 0.007824]\n",
            "[Epoch 1/200] [Batch 977/3166] [D loss: 0.767125] [G loss: 13.388150] [gradient_penalty: 0.008563]\n",
            "[Epoch 1/200] [Batch 978/3166] [D loss: 0.051947] [G loss: 13.633474] [gradient_penalty: 0.007487]\n",
            "[Epoch 1/200] [Batch 979/3166] [D loss: 1.331869] [G loss: 13.615354] [gradient_penalty: 0.007859]\n",
            "[Epoch 1/200] [Batch 980/3166] [D loss: 1.383374] [G loss: 13.219561] [gradient_penalty: 0.008756]\n",
            "[Epoch 1/200] [Batch 981/3166] [D loss: 1.323117] [G loss: 14.171246] [gradient_penalty: 0.007181]\n",
            "[Epoch 1/200] [Batch 982/3166] [D loss: 0.935662] [G loss: 13.871137] [gradient_penalty: 0.006039]\n",
            "[Epoch 1/200] [Batch 983/3166] [D loss: 0.588224] [G loss: 13.105367] [gradient_penalty: 0.009933]\n",
            "[Epoch 1/200] [Batch 984/3166] [D loss: 0.572538] [G loss: 13.965522] [gradient_penalty: 0.009235]\n",
            "[Epoch 1/200] [Batch 985/3166] [D loss: 1.416831] [G loss: 13.221361] [gradient_penalty: 0.008577]\n",
            "[Epoch 1/200] [Batch 986/3166] [D loss: 0.980659] [G loss: 13.740007] [gradient_penalty: 0.006919]\n",
            "[Epoch 1/200] [Batch 987/3166] [D loss: 2.131436] [G loss: 13.728942] [gradient_penalty: 0.008421]\n",
            "[Epoch 1/200] [Batch 988/3166] [D loss: 0.554192] [G loss: 13.069725] [gradient_penalty: 0.007005]\n",
            "[Epoch 1/200] [Batch 989/3166] [D loss: 1.736371] [G loss: 13.444405] [gradient_penalty: 0.005044]\n",
            "[Epoch 1/200] [Batch 990/3166] [D loss: 1.067806] [G loss: 14.147251] [gradient_penalty: 0.007264]\n",
            "[Epoch 1/200] [Batch 991/3166] [D loss: 0.873688] [G loss: 13.255453] [gradient_penalty: 0.008692]\n",
            "[Epoch 1/200] [Batch 992/3166] [D loss: 0.661772] [G loss: 12.707734] [gradient_penalty: 0.006816]\n",
            "[Epoch 1/200] [Batch 993/3166] [D loss: 1.018092] [G loss: 13.054548] [gradient_penalty: 0.006524]\n",
            "[Epoch 1/200] [Batch 994/3166] [D loss: 0.464221] [G loss: 13.478713] [gradient_penalty: 0.008999]\n",
            "[Epoch 1/200] [Batch 995/3166] [D loss: 0.888868] [G loss: 13.359676] [gradient_penalty: 0.006109]\n",
            "[Epoch 1/200] [Batch 996/3166] [D loss: 1.015833] [G loss: 13.161973] [gradient_penalty: 0.008746]\n",
            "[Epoch 1/200] [Batch 997/3166] [D loss: 0.933517] [G loss: 12.815046] [gradient_penalty: 0.009635]\n",
            "[Epoch 1/200] [Batch 998/3166] [D loss: 1.543589] [G loss: 12.529139] [gradient_penalty: 0.008081]\n",
            "[Epoch 1/200] [Batch 999/3166] [D loss: 0.945321] [G loss: 13.184694] [gradient_penalty: 0.006822]\n",
            "[Epoch 1/200] [Batch 1000/3166] [D loss: -0.099576] [G loss: 13.151693] [gradient_penalty: 0.008595]\n",
            "[Epoch 1/200] [Batch 1001/3166] [D loss: 0.666439] [G loss: 13.060204] [gradient_penalty: 0.007780]\n",
            "[Epoch 1/200] [Batch 1002/3166] [D loss: 2.039584] [G loss: 13.337521] [gradient_penalty: 0.009440]\n",
            "[Epoch 1/200] [Batch 1003/3166] [D loss: 1.111907] [G loss: 13.128526] [gradient_penalty: 0.008479]\n",
            "[Epoch 1/200] [Batch 1004/3166] [D loss: 2.162098] [G loss: 13.544108] [gradient_penalty: 0.011234]\n",
            "[Epoch 1/200] [Batch 1005/3166] [D loss: 1.392368] [G loss: 13.046368] [gradient_penalty: 0.010913]\n",
            "[Epoch 1/200] [Batch 1006/3166] [D loss: 0.553733] [G loss: 13.733135] [gradient_penalty: 0.008357]\n",
            "[Epoch 1/200] [Batch 1007/3166] [D loss: 1.208200] [G loss: 12.982695] [gradient_penalty: 0.006938]\n",
            "[Epoch 1/200] [Batch 1008/3166] [D loss: 0.919881] [G loss: 12.468722] [gradient_penalty: 0.008138]\n",
            "[Epoch 1/200] [Batch 1009/3166] [D loss: -0.291717] [G loss: 12.642794] [gradient_penalty: 0.009938]\n",
            "[Epoch 1/200] [Batch 1010/3166] [D loss: 1.239603] [G loss: 12.429266] [gradient_penalty: 0.006211]\n",
            "[Epoch 1/200] [Batch 1011/3166] [D loss: 1.810729] [G loss: 12.425756] [gradient_penalty: 0.006988]\n",
            "[Epoch 1/200] [Batch 1012/3166] [D loss: 0.674636] [G loss: 12.351959] [gradient_penalty: 0.006204]\n",
            "[Epoch 1/200] [Batch 1013/3166] [D loss: 1.500899] [G loss: 12.469086] [gradient_penalty: 0.008887]\n",
            "[Epoch 1/200] [Batch 1014/3166] [D loss: 0.284650] [G loss: 12.391723] [gradient_penalty: 0.009485]\n",
            "[Epoch 1/200] [Batch 1015/3166] [D loss: 1.344988] [G loss: 12.658626] [gradient_penalty: 0.007473]\n",
            "[Epoch 1/200] [Batch 1016/3166] [D loss: 0.730682] [G loss: 12.901702] [gradient_penalty: 0.006939]\n",
            "[Epoch 1/200] [Batch 1017/3166] [D loss: 0.361684] [G loss: 12.957184] [gradient_penalty: 0.008347]\n",
            "[Epoch 1/200] [Batch 1018/3166] [D loss: 0.440581] [G loss: 12.848186] [gradient_penalty: 0.008757]\n",
            "[Epoch 1/200] [Batch 1019/3166] [D loss: -0.070414] [G loss: 13.321274] [gradient_penalty: 0.009116]\n",
            "[Epoch 1/200] [Batch 1020/3166] [D loss: 0.426822] [G loss: 13.811203] [gradient_penalty: 0.007424]\n",
            "[Epoch 1/200] [Batch 1021/3166] [D loss: 1.171351] [G loss: 12.947725] [gradient_penalty: 0.009704]\n",
            "[Epoch 1/200] [Batch 1022/3166] [D loss: 2.013567] [G loss: 12.730343] [gradient_penalty: 0.008283]\n",
            "[Epoch 1/200] [Batch 1023/3166] [D loss: 1.004459] [G loss: 12.412007] [gradient_penalty: 0.009741]\n",
            "[Epoch 1/200] [Batch 1024/3166] [D loss: 0.866224] [G loss: 13.418247] [gradient_penalty: 0.008426]\n",
            "[Epoch 1/200] [Batch 1025/3166] [D loss: 0.822498] [G loss: 12.944103] [gradient_penalty: 0.009359]\n",
            "[Epoch 1/200] [Batch 1026/3166] [D loss: 1.051030] [G loss: 12.015623] [gradient_penalty: 0.007761]\n",
            "[Epoch 1/200] [Batch 1027/3166] [D loss: 1.062215] [G loss: 13.030676] [gradient_penalty: 0.005781]\n",
            "[Epoch 1/200] [Batch 1028/3166] [D loss: 1.490790] [G loss: 12.650578] [gradient_penalty: 0.006265]\n",
            "[Epoch 1/200] [Batch 1029/3166] [D loss: 1.666286] [G loss: 13.838715] [gradient_penalty: 0.008034]\n",
            "[Epoch 1/200] [Batch 1030/3166] [D loss: 0.761058] [G loss: 13.321827] [gradient_penalty: 0.009860]\n",
            "[Epoch 1/200] [Batch 1031/3166] [D loss: 0.920583] [G loss: 13.400551] [gradient_penalty: 0.007086]\n",
            "[Epoch 1/200] [Batch 1032/3166] [D loss: 1.041442] [G loss: 13.181578] [gradient_penalty: 0.007095]\n",
            "[Epoch 1/200] [Batch 1033/3166] [D loss: 1.002900] [G loss: 13.411707] [gradient_penalty: 0.007739]\n",
            "[Epoch 1/200] [Batch 1034/3166] [D loss: 0.328663] [G loss: 13.302475] [gradient_penalty: 0.007110]\n",
            "log\n",
            "[Epoch 1/200] [Batch 1035/3166] [D loss: 0.883169] [G loss: 11.968107] [gradient_penalty: 0.008780]\n",
            "[Epoch 1/200] [Batch 1036/3166] [D loss: -0.394521] [G loss: 12.976140] [gradient_penalty: 0.010072]\n",
            "[Epoch 1/200] [Batch 1037/3166] [D loss: 0.855228] [G loss: 13.946739] [gradient_penalty: 0.009174]\n",
            "[Epoch 1/200] [Batch 1038/3166] [D loss: 1.374496] [G loss: 14.040234] [gradient_penalty: 0.009239]\n",
            "[Epoch 1/200] [Batch 1039/3166] [D loss: 1.124801] [G loss: 13.894888] [gradient_penalty: 0.012607]\n",
            "[Epoch 1/200] [Batch 1040/3166] [D loss: 0.474064] [G loss: 14.021101] [gradient_penalty: 0.009444]\n",
            "[Epoch 1/200] [Batch 1041/3166] [D loss: 1.389890] [G loss: 13.162064] [gradient_penalty: 0.007702]\n",
            "[Epoch 1/200] [Batch 1042/3166] [D loss: 0.464518] [G loss: 13.728052] [gradient_penalty: 0.006563]\n",
            "[Epoch 1/200] [Batch 1043/3166] [D loss: 1.038699] [G loss: 13.433126] [gradient_penalty: 0.008304]\n",
            "[Epoch 1/200] [Batch 1044/3166] [D loss: 1.010618] [G loss: 12.891085] [gradient_penalty: 0.005873]\n",
            "[Epoch 1/200] [Batch 1045/3166] [D loss: 0.014600] [G loss: 13.040288] [gradient_penalty: 0.009074]\n",
            "[Epoch 1/200] [Batch 1046/3166] [D loss: 1.519191] [G loss: 12.874414] [gradient_penalty: 0.008489]\n",
            "[Epoch 1/200] [Batch 1047/3166] [D loss: 1.095168] [G loss: 12.923372] [gradient_penalty: 0.006231]\n",
            "[Epoch 1/200] [Batch 1048/3166] [D loss: 0.235467] [G loss: 13.454548] [gradient_penalty: 0.006763]\n",
            "[Epoch 1/200] [Batch 1049/3166] [D loss: 1.150939] [G loss: 13.318562] [gradient_penalty: 0.009202]\n",
            "[Epoch 1/200] [Batch 1050/3166] [D loss: 0.827613] [G loss: 12.446236] [gradient_penalty: 0.010106]\n",
            "[Epoch 1/200] [Batch 1051/3166] [D loss: 0.425603] [G loss: 13.677835] [gradient_penalty: 0.008859]\n",
            "[Epoch 1/200] [Batch 1052/3166] [D loss: 1.744232] [G loss: 13.358683] [gradient_penalty: 0.007881]\n",
            "[Epoch 1/200] [Batch 1053/3166] [D loss: 1.665758] [G loss: 13.597151] [gradient_penalty: 0.008745]\n",
            "[Epoch 1/200] [Batch 1054/3166] [D loss: 0.334935] [G loss: 13.929565] [gradient_penalty: 0.007504]\n",
            "[Epoch 1/200] [Batch 1055/3166] [D loss: 1.314851] [G loss: 13.038249] [gradient_penalty: 0.009043]\n",
            "[Epoch 1/200] [Batch 1056/3166] [D loss: 0.407688] [G loss: 13.623854] [gradient_penalty: 0.008906]\n",
            "[Epoch 1/200] [Batch 1057/3166] [D loss: 0.624988] [G loss: 13.292093] [gradient_penalty: 0.008695]\n",
            "[Epoch 1/200] [Batch 1058/3166] [D loss: 1.157365] [G loss: 13.894927] [gradient_penalty: 0.008815]\n",
            "[Epoch 1/200] [Batch 1059/3166] [D loss: 1.026490] [G loss: 13.641057] [gradient_penalty: 0.008574]\n",
            "[Epoch 1/200] [Batch 1060/3166] [D loss: 0.998472] [G loss: 13.727591] [gradient_penalty: 0.010109]\n",
            "[Epoch 1/200] [Batch 1061/3166] [D loss: 0.731008] [G loss: 13.149778] [gradient_penalty: 0.009351]\n",
            "[Epoch 1/200] [Batch 1062/3166] [D loss: 1.628981] [G loss: 14.385540] [gradient_penalty: 0.009757]\n",
            "[Epoch 1/200] [Batch 1063/3166] [D loss: 0.849409] [G loss: 13.588758] [gradient_penalty: 0.006776]\n",
            "[Epoch 1/200] [Batch 1064/3166] [D loss: 2.199602] [G loss: 14.435555] [gradient_penalty: 0.006873]\n",
            "[Epoch 1/200] [Batch 1065/3166] [D loss: 0.616272] [G loss: 14.284464] [gradient_penalty: 0.009756]\n",
            "[Epoch 1/200] [Batch 1066/3166] [D loss: 0.608890] [G loss: 13.230392] [gradient_penalty: 0.010183]\n",
            "[Epoch 1/200] [Batch 1067/3166] [D loss: 0.297677] [G loss: 14.230321] [gradient_penalty: 0.007445]\n",
            "[Epoch 1/200] [Batch 1068/3166] [D loss: 0.966683] [G loss: 14.062535] [gradient_penalty: 0.007699]\n",
            "[Epoch 1/200] [Batch 1069/3166] [D loss: 0.480729] [G loss: 12.880072] [gradient_penalty: 0.007728]\n",
            "[Epoch 1/200] [Batch 1070/3166] [D loss: 1.241736] [G loss: 13.233696] [gradient_penalty: 0.010531]\n",
            "[Epoch 1/200] [Batch 1071/3166] [D loss: 1.179434] [G loss: 13.174652] [gradient_penalty: 0.008640]\n",
            "[Epoch 1/200] [Batch 1072/3166] [D loss: 1.268039] [G loss: 12.715652] [gradient_penalty: 0.009058]\n",
            "[Epoch 1/200] [Batch 1073/3166] [D loss: 1.084305] [G loss: 13.512353] [gradient_penalty: 0.006443]\n",
            "[Epoch 1/200] [Batch 1074/3166] [D loss: 0.021303] [G loss: 13.099241] [gradient_penalty: 0.007037]\n",
            "[Epoch 1/200] [Batch 1075/3166] [D loss: 0.004789] [G loss: 12.923476] [gradient_penalty: 0.008693]\n",
            "[Epoch 1/200] [Batch 1076/3166] [D loss: 0.626271] [G loss: 12.906623] [gradient_penalty: 0.007845]\n",
            "[Epoch 1/200] [Batch 1077/3166] [D loss: 0.753918] [G loss: 12.906481] [gradient_penalty: 0.006422]\n",
            "[Epoch 1/200] [Batch 1078/3166] [D loss: 0.891065] [G loss: 13.285469] [gradient_penalty: 0.007133]\n",
            "[Epoch 1/200] [Batch 1079/3166] [D loss: 0.407372] [G loss: 12.478924] [gradient_penalty: 0.006463]\n",
            "[Epoch 1/200] [Batch 1080/3166] [D loss: 0.917843] [G loss: 12.625573] [gradient_penalty: 0.008940]\n",
            "[Epoch 1/200] [Batch 1081/3166] [D loss: 0.697188] [G loss: 13.322277] [gradient_penalty: 0.008661]\n",
            "[Epoch 1/200] [Batch 1082/3166] [D loss: 0.943190] [G loss: 13.524099] [gradient_penalty: 0.007603]\n",
            "[Epoch 1/200] [Batch 1083/3166] [D loss: 0.134421] [G loss: 12.842438] [gradient_penalty: 0.006984]\n",
            "[Epoch 1/200] [Batch 1084/3166] [D loss: 1.048162] [G loss: 12.976068] [gradient_penalty: 0.007743]\n",
            "[Epoch 1/200] [Batch 1085/3166] [D loss: 0.830418] [G loss: 13.202528] [gradient_penalty: 0.006591]\n",
            "[Epoch 1/200] [Batch 1086/3166] [D loss: 2.146196] [G loss: 13.454949] [gradient_penalty: 0.008299]\n",
            "[Epoch 1/200] [Batch 1087/3166] [D loss: 0.905826] [G loss: 14.119869] [gradient_penalty: 0.008766]\n",
            "[Epoch 1/200] [Batch 1088/3166] [D loss: 1.883716] [G loss: 11.969380] [gradient_penalty: 0.008582]\n",
            "[Epoch 1/200] [Batch 1089/3166] [D loss: 1.411952] [G loss: 12.841873] [gradient_penalty: 0.007250]\n",
            "[Epoch 1/200] [Batch 1090/3166] [D loss: 1.363988] [G loss: 13.484318] [gradient_penalty: 0.009200]\n",
            "[Epoch 1/200] [Batch 1091/3166] [D loss: -0.336031] [G loss: 13.289360] [gradient_penalty: 0.006620]\n",
            "[Epoch 1/200] [Batch 1092/3166] [D loss: 1.177463] [G loss: 13.080686] [gradient_penalty: 0.009625]\n",
            "[Epoch 1/200] [Batch 1093/3166] [D loss: 0.912572] [G loss: 13.808865] [gradient_penalty: 0.009645]\n",
            "[Epoch 1/200] [Batch 1094/3166] [D loss: 0.103874] [G loss: 13.507927] [gradient_penalty: 0.011923]\n",
            "[Epoch 1/200] [Batch 1095/3166] [D loss: 0.690993] [G loss: 11.984270] [gradient_penalty: 0.008277]\n",
            "[Epoch 1/200] [Batch 1096/3166] [D loss: 0.506722] [G loss: 13.459840] [gradient_penalty: 0.007391]\n",
            "[Epoch 1/200] [Batch 1097/3166] [D loss: 1.345833] [G loss: 12.265237] [gradient_penalty: 0.006484]\n",
            "[Epoch 1/200] [Batch 1098/3166] [D loss: 0.691275] [G loss: 12.958401] [gradient_penalty: 0.006032]\n",
            "[Epoch 1/200] [Batch 1099/3166] [D loss: 0.567959] [G loss: 13.378045] [gradient_penalty: 0.011020]\n",
            "[Epoch 1/200] [Batch 1100/3166] [D loss: 1.712559] [G loss: 12.389380] [gradient_penalty: 0.009217]\n",
            "[Epoch 1/200] [Batch 1101/3166] [D loss: 0.986425] [G loss: 13.159761] [gradient_penalty: 0.005711]\n",
            "[Epoch 1/200] [Batch 1102/3166] [D loss: 1.713757] [G loss: 13.436987] [gradient_penalty: 0.009554]\n",
            "[Epoch 1/200] [Batch 1103/3166] [D loss: 0.447506] [G loss: 14.071065] [gradient_penalty: 0.006548]\n",
            "[Epoch 1/200] [Batch 1104/3166] [D loss: 0.029502] [G loss: 13.083036] [gradient_penalty: 0.007824]\n",
            "[Epoch 1/200] [Batch 1105/3166] [D loss: 0.348763] [G loss: 13.578482] [gradient_penalty: 0.009062]\n",
            "[Epoch 1/200] [Batch 1106/3166] [D loss: 0.372562] [G loss: 13.266543] [gradient_penalty: 0.009140]\n",
            "[Epoch 1/200] [Batch 1107/3166] [D loss: 1.205658] [G loss: 13.257669] [gradient_penalty: 0.007574]\n",
            "[Epoch 1/200] [Batch 1108/3166] [D loss: 0.303572] [G loss: 14.307371] [gradient_penalty: 0.007641]\n",
            "[Epoch 1/200] [Batch 1109/3166] [D loss: 0.806543] [G loss: 14.186256] [gradient_penalty: 0.008000]\n",
            "[Epoch 1/200] [Batch 1110/3166] [D loss: 0.065205] [G loss: 12.627500] [gradient_penalty: 0.008951]\n",
            "[Epoch 1/200] [Batch 1111/3166] [D loss: 0.753234] [G loss: 13.098532] [gradient_penalty: 0.008706]\n",
            "[Epoch 1/200] [Batch 1112/3166] [D loss: 1.242426] [G loss: 12.650450] [gradient_penalty: 0.007045]\n",
            "[Epoch 1/200] [Batch 1113/3166] [D loss: 1.455708] [G loss: 13.490322] [gradient_penalty: 0.008631]\n",
            "[Epoch 1/200] [Batch 1114/3166] [D loss: 1.283738] [G loss: 11.825754] [gradient_penalty: 0.008117]\n",
            "[Epoch 1/200] [Batch 1115/3166] [D loss: 0.500794] [G loss: 12.712665] [gradient_penalty: 0.007167]\n",
            "[Epoch 1/200] [Batch 1116/3166] [D loss: 0.915102] [G loss: 13.473966] [gradient_penalty: 0.007207]\n",
            "[Epoch 1/200] [Batch 1117/3166] [D loss: 0.663310] [G loss: 13.169848] [gradient_penalty: 0.010369]\n",
            "[Epoch 1/200] [Batch 1118/3166] [D loss: 1.073490] [G loss: 12.968416] [gradient_penalty: 0.010147]\n",
            "[Epoch 1/200] [Batch 1119/3166] [D loss: 0.715471] [G loss: 13.261869] [gradient_penalty: 0.009725]\n",
            "[Epoch 1/200] [Batch 1120/3166] [D loss: 0.245360] [G loss: 13.008879] [gradient_penalty: 0.006689]\n",
            "[Epoch 1/200] [Batch 1121/3166] [D loss: 1.859452] [G loss: 13.386512] [gradient_penalty: 0.008487]\n",
            "[Epoch 1/200] [Batch 1122/3166] [D loss: 0.443165] [G loss: 13.295643] [gradient_penalty: 0.008594]\n",
            "[Epoch 1/200] [Batch 1123/3166] [D loss: 0.288491] [G loss: 13.417976] [gradient_penalty: 0.008173]\n",
            "[Epoch 1/200] [Batch 1124/3166] [D loss: -0.352814] [G loss: 14.345451] [gradient_penalty: 0.009793]\n",
            "[Epoch 1/200] [Batch 1125/3166] [D loss: -0.071681] [G loss: 13.895756] [gradient_penalty: 0.010827]\n",
            "[Epoch 1/200] [Batch 1126/3166] [D loss: 0.017526] [G loss: 13.013556] [gradient_penalty: 0.009532]\n",
            "[Epoch 1/200] [Batch 1127/3166] [D loss: 0.016999] [G loss: 13.486019] [gradient_penalty: 0.010376]\n",
            "[Epoch 1/200] [Batch 1128/3166] [D loss: 1.349609] [G loss: 13.584170] [gradient_penalty: 0.010564]\n",
            "[Epoch 1/200] [Batch 1129/3166] [D loss: 1.545099] [G loss: 11.952748] [gradient_penalty: 0.009389]\n",
            "[Epoch 1/200] [Batch 1130/3166] [D loss: 1.285964] [G loss: 12.665865] [gradient_penalty: 0.008762]\n",
            "[Epoch 1/200] [Batch 1131/3166] [D loss: 1.108343] [G loss: 12.164227] [gradient_penalty: 0.007260]\n",
            "[Epoch 1/200] [Batch 1132/3166] [D loss: 1.759495] [G loss: 12.228254] [gradient_penalty: 0.009062]\n",
            "[Epoch 1/200] [Batch 1133/3166] [D loss: -0.217705] [G loss: 13.997028] [gradient_penalty: 0.007791]\n",
            "[Epoch 1/200] [Batch 1134/3166] [D loss: 0.885252] [G loss: 12.809283] [gradient_penalty: 0.008757]\n",
            "log\n",
            "[Epoch 1/200] [Batch 1135/3166] [D loss: 1.245379] [G loss: 12.863817] [gradient_penalty: 0.010312]\n",
            "[Epoch 1/200] [Batch 1136/3166] [D loss: 1.437099] [G loss: 12.930725] [gradient_penalty: 0.008575]\n",
            "[Epoch 1/200] [Batch 1137/3166] [D loss: 1.125037] [G loss: 12.912242] [gradient_penalty: 0.006663]\n",
            "[Epoch 1/200] [Batch 1138/3166] [D loss: 1.611776] [G loss: 12.522776] [gradient_penalty: 0.009266]\n",
            "[Epoch 1/200] [Batch 1139/3166] [D loss: 0.565548] [G loss: 11.489719] [gradient_penalty: 0.006192]\n",
            "[Epoch 1/200] [Batch 1140/3166] [D loss: 0.584722] [G loss: 11.707544] [gradient_penalty: 0.009996]\n",
            "[Epoch 1/200] [Batch 1141/3166] [D loss: 1.026533] [G loss: 11.779385] [gradient_penalty: 0.007833]\n",
            "[Epoch 1/200] [Batch 1142/3166] [D loss: 0.834662] [G loss: 12.144794] [gradient_penalty: 0.011582]\n",
            "[Epoch 1/200] [Batch 1143/3166] [D loss: 1.128666] [G loss: 11.805990] [gradient_penalty: 0.006504]\n",
            "[Epoch 1/200] [Batch 1144/3166] [D loss: 2.093090] [G loss: 12.238680] [gradient_penalty: 0.007733]\n",
            "[Epoch 1/200] [Batch 1145/3166] [D loss: 0.386908] [G loss: 11.909604] [gradient_penalty: 0.009953]\n",
            "[Epoch 1/200] [Batch 1146/3166] [D loss: 0.809293] [G loss: 11.672194] [gradient_penalty: 0.006581]\n",
            "[Epoch 1/200] [Batch 1147/3166] [D loss: 1.093693] [G loss: 11.848593] [gradient_penalty: 0.009906]\n",
            "[Epoch 1/200] [Batch 1148/3166] [D loss: 1.553865] [G loss: 11.214275] [gradient_penalty: 0.007715]\n",
            "[Epoch 1/200] [Batch 1149/3166] [D loss: 1.786464] [G loss: 11.522356] [gradient_penalty: 0.009499]\n",
            "[Epoch 1/200] [Batch 1150/3166] [D loss: 0.691235] [G loss: 12.682288] [gradient_penalty: 0.009042]\n",
            "[Epoch 1/200] [Batch 1151/3166] [D loss: 0.582836] [G loss: 13.167456] [gradient_penalty: 0.010706]\n",
            "[Epoch 1/200] [Batch 1152/3166] [D loss: 2.113697] [G loss: 13.223590] [gradient_penalty: 0.010155]\n",
            "[Epoch 1/200] [Batch 1153/3166] [D loss: 0.795980] [G loss: 12.121955] [gradient_penalty: 0.010427]\n",
            "[Epoch 1/200] [Batch 1154/3166] [D loss: -0.304587] [G loss: 13.366699] [gradient_penalty: 0.011233]\n",
            "[Epoch 1/200] [Batch 1155/3166] [D loss: 1.186121] [G loss: 13.168206] [gradient_penalty: 0.012852]\n",
            "[Epoch 1/200] [Batch 1156/3166] [D loss: 2.086150] [G loss: 12.778259] [gradient_penalty: 0.010903]\n",
            "[Epoch 1/200] [Batch 1157/3166] [D loss: 1.117860] [G loss: 11.969515] [gradient_penalty: 0.008950]\n",
            "[Epoch 1/200] [Batch 1158/3166] [D loss: 1.586223] [G loss: 12.855745] [gradient_penalty: 0.008232]\n",
            "[Epoch 1/200] [Batch 1159/3166] [D loss: 1.596005] [G loss: 12.526066] [gradient_penalty: 0.011698]\n",
            "[Epoch 1/200] [Batch 1160/3166] [D loss: 2.201242] [G loss: 13.046086] [gradient_penalty: 0.006951]\n",
            "[Epoch 1/200] [Batch 1161/3166] [D loss: 1.865353] [G loss: 10.859289] [gradient_penalty: 0.008528]\n",
            "[Epoch 1/200] [Batch 1162/3166] [D loss: 0.709639] [G loss: 12.284456] [gradient_penalty: 0.010783]\n",
            "[Epoch 1/200] [Batch 1163/3166] [D loss: 1.328922] [G loss: 11.530504] [gradient_penalty: 0.008083]\n",
            "[Epoch 1/200] [Batch 1164/3166] [D loss: 0.640276] [G loss: 11.442883] [gradient_penalty: 0.008435]\n",
            "[Epoch 1/200] [Batch 1165/3166] [D loss: 0.514953] [G loss: 12.838996] [gradient_penalty: 0.007833]\n",
            "[Epoch 1/200] [Batch 1166/3166] [D loss: 1.300133] [G loss: 12.001547] [gradient_penalty: 0.008821]\n",
            "[Epoch 1/200] [Batch 1167/3166] [D loss: 0.933442] [G loss: 11.815145] [gradient_penalty: 0.006357]\n",
            "[Epoch 1/200] [Batch 1168/3166] [D loss: 0.308120] [G loss: 11.828411] [gradient_penalty: 0.005833]\n",
            "[Epoch 1/200] [Batch 1169/3166] [D loss: 0.927293] [G loss: 12.371288] [gradient_penalty: 0.012237]\n",
            "[Epoch 1/200] [Batch 1170/3166] [D loss: 0.336889] [G loss: 12.542665] [gradient_penalty: 0.011310]\n",
            "[Epoch 1/200] [Batch 1171/3166] [D loss: 1.812396] [G loss: 12.648533] [gradient_penalty: 0.008042]\n",
            "[Epoch 1/200] [Batch 1172/3166] [D loss: 1.098063] [G loss: 12.061171] [gradient_penalty: 0.009553]\n",
            "[Epoch 1/200] [Batch 1173/3166] [D loss: 2.129714] [G loss: 11.714768] [gradient_penalty: 0.007270]\n",
            "[Epoch 1/200] [Batch 1174/3166] [D loss: 2.346701] [G loss: 11.797216] [gradient_penalty: 0.008504]\n",
            "[Epoch 1/200] [Batch 1175/3166] [D loss: -0.214621] [G loss: 11.334114] [gradient_penalty: 0.006848]\n",
            "[Epoch 1/200] [Batch 1176/3166] [D loss: 1.230806] [G loss: 11.682522] [gradient_penalty: 0.008852]\n",
            "[Epoch 1/200] [Batch 1177/3166] [D loss: 1.107697] [G loss: 12.092050] [gradient_penalty: 0.009185]\n",
            "[Epoch 1/200] [Batch 1178/3166] [D loss: 2.140177] [G loss: 11.091351] [gradient_penalty: 0.009682]\n",
            "[Epoch 1/200] [Batch 1179/3166] [D loss: 0.722074] [G loss: 12.073807] [gradient_penalty: 0.008310]\n",
            "[Epoch 1/200] [Batch 1180/3166] [D loss: 2.024480] [G loss: 12.553171] [gradient_penalty: 0.006085]\n",
            "[Epoch 1/200] [Batch 1181/3166] [D loss: 1.665071] [G loss: 12.030939] [gradient_penalty: 0.005965]\n",
            "[Epoch 1/200] [Batch 1182/3166] [D loss: 0.751338] [G loss: 12.400531] [gradient_penalty: 0.007301]\n",
            "[Epoch 1/200] [Batch 1183/3166] [D loss: 0.552376] [G loss: 12.668321] [gradient_penalty: 0.008593]\n",
            "[Epoch 1/200] [Batch 1184/3166] [D loss: 0.497689] [G loss: 12.000824] [gradient_penalty: 0.007861]\n",
            "[Epoch 1/200] [Batch 1185/3166] [D loss: 0.125366] [G loss: 12.820795] [gradient_penalty: 0.008461]\n",
            "[Epoch 1/200] [Batch 1186/3166] [D loss: 0.582441] [G loss: 13.137109] [gradient_penalty: 0.005575]\n",
            "[Epoch 1/200] [Batch 1187/3166] [D loss: 1.744161] [G loss: 12.895376] [gradient_penalty: 0.010062]\n",
            "[Epoch 1/200] [Batch 1188/3166] [D loss: 0.702002] [G loss: 12.334135] [gradient_penalty: 0.010276]\n",
            "[Epoch 1/200] [Batch 1189/3166] [D loss: 1.414107] [G loss: 12.987940] [gradient_penalty: 0.009838]\n",
            "[Epoch 1/200] [Batch 1190/3166] [D loss: 1.663377] [G loss: 11.983671] [gradient_penalty: 0.009889]\n",
            "[Epoch 1/200] [Batch 1191/3166] [D loss: 1.956329] [G loss: 11.601683] [gradient_penalty: 0.009086]\n",
            "[Epoch 1/200] [Batch 1192/3166] [D loss: 0.613918] [G loss: 11.476492] [gradient_penalty: 0.007671]\n",
            "[Epoch 1/200] [Batch 1193/3166] [D loss: 1.685696] [G loss: 12.330618] [gradient_penalty: 0.009467]\n",
            "[Epoch 1/200] [Batch 1194/3166] [D loss: 0.731695] [G loss: 11.509237] [gradient_penalty: 0.007488]\n",
            "[Epoch 1/200] [Batch 1195/3166] [D loss: 0.877917] [G loss: 12.186222] [gradient_penalty: 0.009268]\n",
            "[Epoch 1/200] [Batch 1196/3166] [D loss: 1.923555] [G loss: 12.095245] [gradient_penalty: 0.009713]\n",
            "[Epoch 1/200] [Batch 1197/3166] [D loss: 0.873315] [G loss: 11.427176] [gradient_penalty: 0.007867]\n",
            "[Epoch 1/200] [Batch 1198/3166] [D loss: 2.076153] [G loss: 11.887595] [gradient_penalty: 0.007541]\n",
            "[Epoch 1/200] [Batch 1199/3166] [D loss: 0.591264] [G loss: 12.107281] [gradient_penalty: 0.007930]\n",
            "[Epoch 1/200] [Batch 1200/3166] [D loss: 0.472997] [G loss: 11.407387] [gradient_penalty: 0.006738]\n",
            "[Epoch 1/200] [Batch 1201/3166] [D loss: 0.763163] [G loss: 11.209726] [gradient_penalty: 0.006038]\n",
            "[Epoch 1/200] [Batch 1202/3166] [D loss: 0.592834] [G loss: 10.768206] [gradient_penalty: 0.006498]\n",
            "[Epoch 1/200] [Batch 1203/3166] [D loss: 0.468531] [G loss: 11.383862] [gradient_penalty: 0.009831]\n",
            "[Epoch 1/200] [Batch 1204/3166] [D loss: 0.734123] [G loss: 11.185066] [gradient_penalty: 0.007942]\n",
            "[Epoch 1/200] [Batch 1205/3166] [D loss: 0.305266] [G loss: 11.636515] [gradient_penalty: 0.011976]\n",
            "[Epoch 1/200] [Batch 1206/3166] [D loss: 0.190879] [G loss: 11.233827] [gradient_penalty: 0.007074]\n",
            "[Epoch 1/200] [Batch 1207/3166] [D loss: 2.473136] [G loss: 11.013387] [gradient_penalty: 0.008991]\n",
            "[Epoch 1/200] [Batch 1208/3166] [D loss: 0.159048] [G loss: 10.668598] [gradient_penalty: 0.006630]\n",
            "[Epoch 1/200] [Batch 1209/3166] [D loss: 0.144388] [G loss: 11.360575] [gradient_penalty: 0.010049]\n",
            "[Epoch 1/200] [Batch 1210/3166] [D loss: 0.458012] [G loss: 11.301542] [gradient_penalty: 0.008746]\n",
            "[Epoch 1/200] [Batch 1211/3166] [D loss: 2.444765] [G loss: 11.337223] [gradient_penalty: 0.008016]\n",
            "[Epoch 1/200] [Batch 1212/3166] [D loss: 1.614659] [G loss: 12.109022] [gradient_penalty: 0.006787]\n",
            "[Epoch 1/200] [Batch 1213/3166] [D loss: 1.532131] [G loss: 11.375623] [gradient_penalty: 0.008529]\n",
            "[Epoch 1/200] [Batch 1214/3166] [D loss: 0.613949] [G loss: 11.753706] [gradient_penalty: 0.012189]\n",
            "[Epoch 1/200] [Batch 1215/3166] [D loss: 0.874192] [G loss: 11.928598] [gradient_penalty: 0.009332]\n",
            "[Epoch 1/200] [Batch 1216/3166] [D loss: 0.824230] [G loss: 12.384905] [gradient_penalty: 0.007844]\n",
            "[Epoch 1/200] [Batch 1217/3166] [D loss: 0.214995] [G loss: 11.890607] [gradient_penalty: 0.009679]\n",
            "[Epoch 1/200] [Batch 1218/3166] [D loss: 1.445926] [G loss: 11.914185] [gradient_penalty: 0.010661]\n",
            "[Epoch 1/200] [Batch 1219/3166] [D loss: 0.375057] [G loss: 11.758545] [gradient_penalty: 0.007169]\n",
            "[Epoch 1/200] [Batch 1220/3166] [D loss: 1.251508] [G loss: 11.804560] [gradient_penalty: 0.006696]\n",
            "[Epoch 1/200] [Batch 1221/3166] [D loss: 1.151596] [G loss: 11.059445] [gradient_penalty: 0.005467]\n",
            "[Epoch 1/200] [Batch 1222/3166] [D loss: 1.422658] [G loss: 12.221201] [gradient_penalty: 0.007751]\n",
            "[Epoch 1/200] [Batch 1223/3166] [D loss: 1.091348] [G loss: 11.246038] [gradient_penalty: 0.007785]\n",
            "[Epoch 1/200] [Batch 1224/3166] [D loss: 1.011393] [G loss: 11.753058] [gradient_penalty: 0.006781]\n",
            "[Epoch 1/200] [Batch 1225/3166] [D loss: 1.305184] [G loss: 11.290051] [gradient_penalty: 0.009196]\n",
            "[Epoch 1/200] [Batch 1226/3166] [D loss: 1.088646] [G loss: 11.244623] [gradient_penalty: 0.006813]\n",
            "[Epoch 1/200] [Batch 1227/3166] [D loss: 0.494109] [G loss: 11.030962] [gradient_penalty: 0.007301]\n",
            "[Epoch 1/200] [Batch 1228/3166] [D loss: 0.008863] [G loss: 10.761470] [gradient_penalty: 0.007920]\n",
            "[Epoch 1/200] [Batch 1229/3166] [D loss: 1.448525] [G loss: 11.065303] [gradient_penalty: 0.006556]\n",
            "[Epoch 1/200] [Batch 1230/3166] [D loss: 0.536433] [G loss: 11.845280] [gradient_penalty: 0.011414]\n",
            "[Epoch 1/200] [Batch 1231/3166] [D loss: 0.344183] [G loss: 11.672417] [gradient_penalty: 0.008279]\n",
            "[Epoch 1/200] [Batch 1232/3166] [D loss: 1.506894] [G loss: 11.340124] [gradient_penalty: 0.008376]\n",
            "[Epoch 1/200] [Batch 1233/3166] [D loss: 0.832539] [G loss: 11.201763] [gradient_penalty: 0.009676]\n",
            "[Epoch 1/200] [Batch 1234/3166] [D loss: 2.849800] [G loss: 11.104869] [gradient_penalty: 0.009906]\n",
            "log\n",
            "[Epoch 1/200] [Batch 1235/3166] [D loss: 0.724939] [G loss: 10.035456] [gradient_penalty: 0.006441]\n",
            "[Epoch 1/200] [Batch 1236/3166] [D loss: 1.195632] [G loss: 10.731031] [gradient_penalty: 0.010243]\n",
            "[Epoch 1/200] [Batch 1237/3166] [D loss: 1.401248] [G loss: 11.852026] [gradient_penalty: 0.008229]\n",
            "[Epoch 1/200] [Batch 1238/3166] [D loss: 1.668890] [G loss: 10.842831] [gradient_penalty: 0.006780]\n",
            "[Epoch 1/200] [Batch 1239/3166] [D loss: 1.173775] [G loss: 10.868196] [gradient_penalty: 0.005817]\n",
            "[Epoch 1/200] [Batch 1240/3166] [D loss: 0.905067] [G loss: 11.130016] [gradient_penalty: 0.008704]\n",
            "[Epoch 1/200] [Batch 1241/3166] [D loss: 0.371175] [G loss: 11.611429] [gradient_penalty: 0.010309]\n",
            "[Epoch 1/200] [Batch 1242/3166] [D loss: 1.490737] [G loss: 11.105776] [gradient_penalty: 0.007753]\n",
            "[Epoch 1/200] [Batch 1243/3166] [D loss: 1.219332] [G loss: 11.057084] [gradient_penalty: 0.007002]\n",
            "[Epoch 1/200] [Batch 1244/3166] [D loss: 0.954068] [G loss: 12.306704] [gradient_penalty: 0.008022]\n",
            "[Epoch 1/200] [Batch 1245/3166] [D loss: 1.585194] [G loss: 11.957014] [gradient_penalty: 0.016567]\n",
            "[Epoch 1/200] [Batch 1246/3166] [D loss: 0.493527] [G loss: 12.657810] [gradient_penalty: 0.008696]\n",
            "[Epoch 1/200] [Batch 1247/3166] [D loss: -0.635850] [G loss: 11.591351] [gradient_penalty: 0.007131]\n",
            "[Epoch 1/200] [Batch 1248/3166] [D loss: 0.828150] [G loss: 11.925338] [gradient_penalty: 0.010179]\n",
            "[Epoch 1/200] [Batch 1249/3166] [D loss: 0.152679] [G loss: 11.727725] [gradient_penalty: 0.010069]\n",
            "[Epoch 1/200] [Batch 1250/3166] [D loss: 0.909756] [G loss: 11.666330] [gradient_penalty: 0.007277]\n",
            "[Epoch 1/200] [Batch 1251/3166] [D loss: 1.627722] [G loss: 11.410261] [gradient_penalty: 0.010971]\n",
            "[Epoch 1/200] [Batch 1252/3166] [D loss: 0.351648] [G loss: 11.261725] [gradient_penalty: 0.007451]\n",
            "[Epoch 1/200] [Batch 1253/3166] [D loss: 1.068818] [G loss: 12.263541] [gradient_penalty: 0.008203]\n",
            "[Epoch 1/200] [Batch 1254/3166] [D loss: 2.448814] [G loss: 11.991409] [gradient_penalty: 0.009870]\n",
            "[Epoch 1/200] [Batch 1255/3166] [D loss: 1.305672] [G loss: 11.638315] [gradient_penalty: 0.007260]\n",
            "[Epoch 1/200] [Batch 1256/3166] [D loss: 0.813764] [G loss: 11.495043] [gradient_penalty: 0.007899]\n",
            "[Epoch 1/200] [Batch 1257/3166] [D loss: 1.206703] [G loss: 11.189971] [gradient_penalty: 0.008871]\n",
            "[Epoch 1/200] [Batch 1258/3166] [D loss: 0.428940] [G loss: 11.203770] [gradient_penalty: 0.007945]\n",
            "[Epoch 1/200] [Batch 1259/3166] [D loss: 1.052052] [G loss: 11.069195] [gradient_penalty: 0.009275]\n",
            "[Epoch 1/200] [Batch 1260/3166] [D loss: 1.619961] [G loss: 11.294719] [gradient_penalty: 0.010183]\n",
            "[Epoch 1/200] [Batch 1261/3166] [D loss: 0.272996] [G loss: 11.581600] [gradient_penalty: 0.007977]\n",
            "[Epoch 1/200] [Batch 1262/3166] [D loss: 1.319693] [G loss: 10.776414] [gradient_penalty: 0.008644]\n",
            "[Epoch 1/200] [Batch 1263/3166] [D loss: 0.241940] [G loss: 10.309337] [gradient_penalty: 0.006776]\n",
            "[Epoch 1/200] [Batch 1264/3166] [D loss: 0.717947] [G loss: 11.994778] [gradient_penalty: 0.006298]\n",
            "[Epoch 1/200] [Batch 1265/3166] [D loss: 0.291571] [G loss: 10.219759] [gradient_penalty: 0.006603]\n",
            "[Epoch 1/200] [Batch 1266/3166] [D loss: 0.309366] [G loss: 10.889357] [gradient_penalty: 0.005541]\n",
            "[Epoch 1/200] [Batch 1267/3166] [D loss: 1.315111] [G loss: 10.847904] [gradient_penalty: 0.009223]\n",
            "[Epoch 1/200] [Batch 1268/3166] [D loss: 0.959459] [G loss: 10.272137] [gradient_penalty: 0.009750]\n",
            "[Epoch 1/200] [Batch 1269/3166] [D loss: 0.431474] [G loss: 10.567485] [gradient_penalty: 0.007187]\n",
            "[Epoch 1/200] [Batch 1270/3166] [D loss: 1.461386] [G loss: 10.732187] [gradient_penalty: 0.006368]\n",
            "[Epoch 1/200] [Batch 1271/3166] [D loss: 1.563450] [G loss: 11.214678] [gradient_penalty: 0.007671]\n",
            "[Epoch 1/200] [Batch 1272/3166] [D loss: 1.454892] [G loss: 11.376563] [gradient_penalty: 0.009549]\n",
            "[Epoch 1/200] [Batch 1273/3166] [D loss: 0.758685] [G loss: 11.664938] [gradient_penalty: 0.008417]\n",
            "[Epoch 1/200] [Batch 1274/3166] [D loss: 0.402107] [G loss: 11.442041] [gradient_penalty: 0.008281]\n",
            "[Epoch 1/200] [Batch 1275/3166] [D loss: 0.537355] [G loss: 11.774823] [gradient_penalty: 0.009272]\n",
            "[Epoch 1/200] [Batch 1276/3166] [D loss: 0.200536] [G loss: 11.393141] [gradient_penalty: 0.006709]\n",
            "[Epoch 1/200] [Batch 1277/3166] [D loss: -0.624293] [G loss: 11.556471] [gradient_penalty: 0.007566]\n",
            "[Epoch 1/200] [Batch 1278/3166] [D loss: 1.312044] [G loss: 11.210684] [gradient_penalty: 0.008797]\n",
            "[Epoch 1/200] [Batch 1279/3166] [D loss: 0.052380] [G loss: 11.699252] [gradient_penalty: 0.011352]\n",
            "[Epoch 1/200] [Batch 1280/3166] [D loss: -0.136326] [G loss: 11.990931] [gradient_penalty: 0.011032]\n",
            "[Epoch 1/200] [Batch 1281/3166] [D loss: 1.429532] [G loss: 11.608811] [gradient_penalty: 0.009588]\n",
            "[Epoch 1/200] [Batch 1282/3166] [D loss: 1.776445] [G loss: 11.510530] [gradient_penalty: 0.011819]\n",
            "[Epoch 1/200] [Batch 1283/3166] [D loss: 0.497421] [G loss: 11.372686] [gradient_penalty: 0.010515]\n",
            "[Epoch 1/200] [Batch 1284/3166] [D loss: 1.397610] [G loss: 11.648642] [gradient_penalty: 0.009950]\n",
            "[Epoch 1/200] [Batch 1285/3166] [D loss: 0.061771] [G loss: 11.813256] [gradient_penalty: 0.010086]\n",
            "[Epoch 1/200] [Batch 1286/3166] [D loss: 0.407868] [G loss: 10.938406] [gradient_penalty: 0.009141]\n",
            "[Epoch 1/200] [Batch 1287/3166] [D loss: 1.388845] [G loss: 11.109106] [gradient_penalty: 0.010934]\n",
            "[Epoch 1/200] [Batch 1288/3166] [D loss: 1.156501] [G loss: 11.403501] [gradient_penalty: 0.007287]\n",
            "[Epoch 1/200] [Batch 1289/3166] [D loss: 0.828359] [G loss: 11.300171] [gradient_penalty: 0.007150]\n",
            "[Epoch 1/200] [Batch 1290/3166] [D loss: 1.694496] [G loss: 11.473179] [gradient_penalty: 0.009324]\n",
            "[Epoch 1/200] [Batch 1291/3166] [D loss: 0.945072] [G loss: 11.783068] [gradient_penalty: 0.007887]\n",
            "[Epoch 1/200] [Batch 1292/3166] [D loss: 2.422659] [G loss: 11.500340] [gradient_penalty: 0.010296]\n",
            "[Epoch 1/200] [Batch 1293/3166] [D loss: 1.118015] [G loss: 11.228184] [gradient_penalty: 0.008347]\n",
            "[Epoch 1/200] [Batch 1294/3166] [D loss: 1.281948] [G loss: 10.676027] [gradient_penalty: 0.006649]\n",
            "[Epoch 1/200] [Batch 1295/3166] [D loss: 1.407545] [G loss: 11.506432] [gradient_penalty: 0.007601]\n",
            "[Epoch 1/200] [Batch 1296/3166] [D loss: -0.863666] [G loss: 10.657195] [gradient_penalty: 0.010937]\n",
            "[Epoch 1/200] [Batch 1297/3166] [D loss: 1.717811] [G loss: 12.103821] [gradient_penalty: 0.009734]\n",
            "[Epoch 1/200] [Batch 1298/3166] [D loss: 0.410108] [G loss: 10.555431] [gradient_penalty: 0.006137]\n",
            "[Epoch 1/200] [Batch 1299/3166] [D loss: 0.658719] [G loss: 11.832342] [gradient_penalty: 0.010012]\n",
            "[Epoch 1/200] [Batch 1300/3166] [D loss: 1.405235] [G loss: 11.499423] [gradient_penalty: 0.010063]\n",
            "[Epoch 1/200] [Batch 1301/3166] [D loss: -0.070130] [G loss: 10.501262] [gradient_penalty: 0.008432]\n",
            "[Epoch 1/200] [Batch 1302/3166] [D loss: 1.871149] [G loss: 10.868634] [gradient_penalty: 0.009339]\n",
            "[Epoch 1/200] [Batch 1303/3166] [D loss: 1.263246] [G loss: 10.986115] [gradient_penalty: 0.007857]\n",
            "[Epoch 1/200] [Batch 1304/3166] [D loss: 0.723824] [G loss: 10.849760] [gradient_penalty: 0.007252]\n",
            "[Epoch 1/200] [Batch 1305/3166] [D loss: 0.910058] [G loss: 12.073317] [gradient_penalty: 0.007241]\n",
            "[Epoch 1/200] [Batch 1306/3166] [D loss: 1.149090] [G loss: 11.285508] [gradient_penalty: 0.010047]\n",
            "[Epoch 1/200] [Batch 1307/3166] [D loss: 0.623775] [G loss: 11.636748] [gradient_penalty: 0.007748]\n",
            "[Epoch 1/200] [Batch 1308/3166] [D loss: 0.745206] [G loss: 12.363411] [gradient_penalty: 0.008513]\n",
            "[Epoch 1/200] [Batch 1309/3166] [D loss: -0.027677] [G loss: 12.623272] [gradient_penalty: 0.008539]\n",
            "[Epoch 1/200] [Batch 1310/3166] [D loss: 0.971319] [G loss: 12.343719] [gradient_penalty: 0.007532]\n",
            "[Epoch 1/200] [Batch 1311/3166] [D loss: 1.230047] [G loss: 12.176729] [gradient_penalty: 0.009281]\n",
            "[Epoch 1/200] [Batch 1312/3166] [D loss: 1.024627] [G loss: 11.767679] [gradient_penalty: 0.008845]\n",
            "[Epoch 1/200] [Batch 1313/3166] [D loss: 0.370279] [G loss: 12.932768] [gradient_penalty: 0.009029]\n",
            "[Epoch 1/200] [Batch 1314/3166] [D loss: 0.648737] [G loss: 11.825066] [gradient_penalty: 0.011739]\n",
            "[Epoch 1/200] [Batch 1315/3166] [D loss: 1.511775] [G loss: 11.606144] [gradient_penalty: 0.007658]\n",
            "[Epoch 1/200] [Batch 1316/3166] [D loss: 2.249148] [G loss: 11.469727] [gradient_penalty: 0.008237]\n",
            "[Epoch 1/200] [Batch 1317/3166] [D loss: 1.563907] [G loss: 11.021912] [gradient_penalty: 0.009716]\n",
            "[Epoch 1/200] [Batch 1318/3166] [D loss: 1.430433] [G loss: 10.861476] [gradient_penalty: 0.010849]\n",
            "[Epoch 1/200] [Batch 1319/3166] [D loss: 1.628186] [G loss: 10.826872] [gradient_penalty: 0.009327]\n",
            "[Epoch 1/200] [Batch 1320/3166] [D loss: 1.396358] [G loss: 11.951629] [gradient_penalty: 0.010649]\n",
            "[Epoch 1/200] [Batch 1321/3166] [D loss: 1.106760] [G loss: 11.177071] [gradient_penalty: 0.009127]\n",
            "[Epoch 1/200] [Batch 1322/3166] [D loss: 1.430401] [G loss: 11.522503] [gradient_penalty: 0.008623]\n",
            "[Epoch 1/200] [Batch 1323/3166] [D loss: 0.949973] [G loss: 11.589916] [gradient_penalty: 0.011047]\n",
            "[Epoch 1/200] [Batch 1324/3166] [D loss: 0.615322] [G loss: 11.228901] [gradient_penalty: 0.008392]\n",
            "[Epoch 1/200] [Batch 1325/3166] [D loss: 0.577578] [G loss: 11.858452] [gradient_penalty: 0.010442]\n",
            "[Epoch 1/200] [Batch 1326/3166] [D loss: 0.811775] [G loss: 11.333680] [gradient_penalty: 0.011170]\n",
            "[Epoch 1/200] [Batch 1327/3166] [D loss: 1.084588] [G loss: 11.011497] [gradient_penalty: 0.007548]\n",
            "[Epoch 1/200] [Batch 1328/3166] [D loss: 0.229540] [G loss: 10.544884] [gradient_penalty: 0.007855]\n",
            "[Epoch 1/200] [Batch 1329/3166] [D loss: 1.499596] [G loss: 10.125044] [gradient_penalty: 0.007028]\n",
            "[Epoch 1/200] [Batch 1330/3166] [D loss: 1.013609] [G loss: 10.685890] [gradient_penalty: 0.013642]\n",
            "[Epoch 1/200] [Batch 1331/3166] [D loss: 1.272267] [G loss: 11.028446] [gradient_penalty: 0.006330]\n",
            "[Epoch 1/200] [Batch 1332/3166] [D loss: -0.377046] [G loss: 9.891118] [gradient_penalty: 0.007558]\n",
            "[Epoch 1/200] [Batch 1333/3166] [D loss: 2.324236] [G loss: 10.951994] [gradient_penalty: 0.006507]\n",
            "[Epoch 1/200] [Batch 1334/3166] [D loss: 0.837529] [G loss: 10.703444] [gradient_penalty: 0.007336]\n",
            "log\n",
            "[Epoch 1/200] [Batch 1335/3166] [D loss: 0.647520] [G loss: 10.519274] [gradient_penalty: 0.007519]\n",
            "[Epoch 1/200] [Batch 1336/3166] [D loss: 0.086381] [G loss: 9.883855] [gradient_penalty: 0.006801]\n",
            "[Epoch 1/200] [Batch 1337/3166] [D loss: 0.095920] [G loss: 11.720804] [gradient_penalty: 0.010487]\n",
            "[Epoch 1/200] [Batch 1338/3166] [D loss: 0.877020] [G loss: 11.049240] [gradient_penalty: 0.007908]\n",
            "[Epoch 1/200] [Batch 1339/3166] [D loss: 0.932710] [G loss: 10.918911] [gradient_penalty: 0.007193]\n",
            "[Epoch 1/200] [Batch 1340/3166] [D loss: 1.360384] [G loss: 12.316187] [gradient_penalty: 0.010152]\n",
            "[Epoch 1/200] [Batch 1341/3166] [D loss: 0.596408] [G loss: 11.596578] [gradient_penalty: 0.007080]\n",
            "[Epoch 1/200] [Batch 1342/3166] [D loss: 0.990787] [G loss: 11.669325] [gradient_penalty: 0.011118]\n",
            "[Epoch 1/200] [Batch 1343/3166] [D loss: 0.638548] [G loss: 11.025617] [gradient_penalty: 0.009998]\n",
            "[Epoch 1/200] [Batch 1344/3166] [D loss: 0.431536] [G loss: 12.052202] [gradient_penalty: 0.008989]\n",
            "[Epoch 1/200] [Batch 1345/3166] [D loss: 1.067992] [G loss: 11.799486] [gradient_penalty: 0.010773]\n",
            "[Epoch 1/200] [Batch 1346/3166] [D loss: 0.093495] [G loss: 12.090693] [gradient_penalty: 0.009057]\n",
            "[Epoch 1/200] [Batch 1347/3166] [D loss: 0.693646] [G loss: 12.139532] [gradient_penalty: 0.008203]\n",
            "[Epoch 1/200] [Batch 1348/3166] [D loss: 1.317602] [G loss: 11.305709] [gradient_penalty: 0.012171]\n",
            "[Epoch 1/200] [Batch 1349/3166] [D loss: 0.864678] [G loss: 12.043289] [gradient_penalty: 0.008671]\n",
            "[Epoch 1/200] [Batch 1350/3166] [D loss: -0.112582] [G loss: 11.810449] [gradient_penalty: 0.007275]\n",
            "[Epoch 1/200] [Batch 1351/3166] [D loss: 1.184108] [G loss: 11.938721] [gradient_penalty: 0.008410]\n",
            "[Epoch 1/200] [Batch 1352/3166] [D loss: 2.206773] [G loss: 11.559048] [gradient_penalty: 0.011511]\n",
            "[Epoch 1/200] [Batch 1353/3166] [D loss: 0.727023] [G loss: 11.948088] [gradient_penalty: 0.008574]\n",
            "[Epoch 1/200] [Batch 1354/3166] [D loss: 0.972773] [G loss: 11.342188] [gradient_penalty: 0.009651]\n",
            "[Epoch 1/200] [Batch 1355/3166] [D loss: 1.379419] [G loss: 11.319467] [gradient_penalty: 0.007506]\n",
            "[Epoch 1/200] [Batch 1356/3166] [D loss: 1.034315] [G loss: 11.970819] [gradient_penalty: 0.007203]\n",
            "[Epoch 1/200] [Batch 1357/3166] [D loss: 0.660016] [G loss: 11.634563] [gradient_penalty: 0.007366]\n",
            "[Epoch 1/200] [Batch 1358/3166] [D loss: -0.201774] [G loss: 11.870447] [gradient_penalty: 0.010634]\n",
            "[Epoch 1/200] [Batch 1359/3166] [D loss: 1.034217] [G loss: 11.730778] [gradient_penalty: 0.009423]\n",
            "[Epoch 1/200] [Batch 1360/3166] [D loss: 0.268190] [G loss: 12.230822] [gradient_penalty: 0.008975]\n",
            "[Epoch 1/200] [Batch 1361/3166] [D loss: 0.453493] [G loss: 12.241613] [gradient_penalty: 0.009423]\n",
            "[Epoch 1/200] [Batch 1362/3166] [D loss: 0.922625] [G loss: 11.414843] [gradient_penalty: 0.007783]\n",
            "[Epoch 1/200] [Batch 1363/3166] [D loss: 1.350694] [G loss: 11.326313] [gradient_penalty: 0.006848]\n",
            "[Epoch 1/200] [Batch 1364/3166] [D loss: 0.301996] [G loss: 10.674215] [gradient_penalty: 0.008519]\n",
            "[Epoch 1/200] [Batch 1365/3166] [D loss: 1.837505] [G loss: 11.737171] [gradient_penalty: 0.007818]\n",
            "[Epoch 1/200] [Batch 1366/3166] [D loss: 0.246353] [G loss: 10.861739] [gradient_penalty: 0.009475]\n",
            "[Epoch 1/200] [Batch 1367/3166] [D loss: 0.776582] [G loss: 11.253552] [gradient_penalty: 0.009345]\n",
            "[Epoch 1/200] [Batch 1368/3166] [D loss: 2.038177] [G loss: 10.206206] [gradient_penalty: 0.008904]\n",
            "[Epoch 1/200] [Batch 1369/3166] [D loss: 1.547221] [G loss: 11.169862] [gradient_penalty: 0.008255]\n",
            "[Epoch 1/200] [Batch 1370/3166] [D loss: 1.961943] [G loss: 11.114219] [gradient_penalty: 0.010169]\n",
            "[Epoch 1/200] [Batch 1371/3166] [D loss: 0.872818] [G loss: 10.691056] [gradient_penalty: 0.008038]\n",
            "[Epoch 1/200] [Batch 1372/3166] [D loss: 1.141712] [G loss: 11.301990] [gradient_penalty: 0.006642]\n",
            "[Epoch 1/200] [Batch 1373/3166] [D loss: 0.759578] [G loss: 11.819569] [gradient_penalty: 0.009271]\n",
            "[Epoch 1/200] [Batch 1374/3166] [D loss: 1.303295] [G loss: 11.386995] [gradient_penalty: 0.008702]\n",
            "[Epoch 1/200] [Batch 1375/3166] [D loss: 0.109846] [G loss: 11.942959] [gradient_penalty: 0.009048]\n",
            "[Epoch 1/200] [Batch 1376/3166] [D loss: 0.653238] [G loss: 11.895103] [gradient_penalty: 0.007014]\n",
            "[Epoch 1/200] [Batch 1377/3166] [D loss: 0.271709] [G loss: 11.872641] [gradient_penalty: 0.008917]\n",
            "[Epoch 1/200] [Batch 1378/3166] [D loss: 1.437836] [G loss: 13.699773] [gradient_penalty: 0.010461]\n",
            "[Epoch 1/200] [Batch 1379/3166] [D loss: 0.542830] [G loss: 12.041260] [gradient_penalty: 0.013519]\n",
            "[Epoch 1/200] [Batch 1380/3166] [D loss: 0.253462] [G loss: 13.140428] [gradient_penalty: 0.010166]\n",
            "[Epoch 1/200] [Batch 1381/3166] [D loss: 0.439878] [G loss: 12.540827] [gradient_penalty: 0.010764]\n",
            "[Epoch 1/200] [Batch 1382/3166] [D loss: -0.218098] [G loss: 12.590877] [gradient_penalty: 0.009252]\n",
            "[Epoch 1/200] [Batch 1383/3166] [D loss: 0.211053] [G loss: 12.273430] [gradient_penalty: 0.011616]\n",
            "[Epoch 1/200] [Batch 1384/3166] [D loss: 2.125053] [G loss: 12.182394] [gradient_penalty: 0.010361]\n",
            "[Epoch 1/200] [Batch 1385/3166] [D loss: 1.289943] [G loss: 12.365117] [gradient_penalty: 0.011157]\n",
            "[Epoch 1/200] [Batch 1386/3166] [D loss: 1.405451] [G loss: 11.400448] [gradient_penalty: 0.005984]\n",
            "[Epoch 1/200] [Batch 1387/3166] [D loss: 2.209366] [G loss: 12.164207] [gradient_penalty: 0.008564]\n",
            "[Epoch 1/200] [Batch 1388/3166] [D loss: 0.748582] [G loss: 11.202419] [gradient_penalty: 0.008563]\n",
            "[Epoch 1/200] [Batch 1389/3166] [D loss: 0.974415] [G loss: 11.669118] [gradient_penalty: 0.009861]\n",
            "[Epoch 1/200] [Batch 1390/3166] [D loss: 0.804708] [G loss: 10.834814] [gradient_penalty: 0.010371]\n",
            "[Epoch 1/200] [Batch 1391/3166] [D loss: 0.908210] [G loss: 11.942203] [gradient_penalty: 0.007431]\n",
            "[Epoch 1/200] [Batch 1392/3166] [D loss: 0.256859] [G loss: 11.761285] [gradient_penalty: 0.008307]\n",
            "[Epoch 1/200] [Batch 1393/3166] [D loss: 0.120335] [G loss: 11.808516] [gradient_penalty: 0.005978]\n",
            "[Epoch 1/200] [Batch 1394/3166] [D loss: 0.552043] [G loss: 12.606207] [gradient_penalty: 0.007626]\n",
            "[Epoch 1/200] [Batch 1395/3166] [D loss: 0.455265] [G loss: 12.435478] [gradient_penalty: 0.006147]\n",
            "[Epoch 1/200] [Batch 1396/3166] [D loss: 0.393483] [G loss: 12.203325] [gradient_penalty: 0.008949]\n",
            "[Epoch 1/200] [Batch 1397/3166] [D loss: 1.056719] [G loss: 12.374945] [gradient_penalty: 0.006332]\n",
            "[Epoch 1/200] [Batch 1398/3166] [D loss: -0.124069] [G loss: 11.294287] [gradient_penalty: 0.007930]\n",
            "[Epoch 1/200] [Batch 1399/3166] [D loss: 2.028914] [G loss: 11.504212] [gradient_penalty: 0.009466]\n",
            "[Epoch 1/200] [Batch 1400/3166] [D loss: 2.037371] [G loss: 11.515148] [gradient_penalty: 0.010960]\n",
            "[Epoch 1/200] [Batch 1401/3166] [D loss: 1.622254] [G loss: 11.303267] [gradient_penalty: 0.007112]\n",
            "[Epoch 1/200] [Batch 1402/3166] [D loss: 2.063536] [G loss: 10.822359] [gradient_penalty: 0.008650]\n",
            "[Epoch 1/200] [Batch 1403/3166] [D loss: 2.338313] [G loss: 11.412916] [gradient_penalty: 0.007835]\n",
            "[Epoch 1/200] [Batch 1404/3166] [D loss: 0.918231] [G loss: 11.130150] [gradient_penalty: 0.006838]\n",
            "[Epoch 1/200] [Batch 1405/3166] [D loss: 1.005783] [G loss: 10.994982] [gradient_penalty: 0.008731]\n",
            "[Epoch 1/200] [Batch 1406/3166] [D loss: 1.627273] [G loss: 11.952282] [gradient_penalty: 0.008022]\n",
            "[Epoch 1/200] [Batch 1407/3166] [D loss: 0.407002] [G loss: 11.579323] [gradient_penalty: 0.008473]\n",
            "[Epoch 1/200] [Batch 1408/3166] [D loss: 1.069720] [G loss: 11.527201] [gradient_penalty: 0.008241]\n",
            "[Epoch 1/200] [Batch 1409/3166] [D loss: -0.442851] [G loss: 11.633814] [gradient_penalty: 0.007708]\n",
            "[Epoch 1/200] [Batch 1410/3166] [D loss: -0.027589] [G loss: 11.898470] [gradient_penalty: 0.009490]\n",
            "[Epoch 1/200] [Batch 1411/3166] [D loss: 0.991810] [G loss: 12.137845] [gradient_penalty: 0.008721]\n",
            "[Epoch 1/200] [Batch 1412/3166] [D loss: -0.799337] [G loss: 13.052073] [gradient_penalty: 0.009399]\n",
            "[Epoch 1/200] [Batch 1413/3166] [D loss: 1.584071] [G loss: 12.518019] [gradient_penalty: 0.008388]\n",
            "[Epoch 1/200] [Batch 1414/3166] [D loss: -0.271088] [G loss: 12.366516] [gradient_penalty: 0.010839]\n",
            "[Epoch 1/200] [Batch 1415/3166] [D loss: 0.245244] [G loss: 12.365286] [gradient_penalty: 0.010087]\n",
            "[Epoch 1/200] [Batch 1416/3166] [D loss: 1.524446] [G loss: 11.632277] [gradient_penalty: 0.010782]\n",
            "[Epoch 1/200] [Batch 1417/3166] [D loss: 0.570378] [G loss: 12.936784] [gradient_penalty: 0.006524]\n",
            "[Epoch 1/200] [Batch 1418/3166] [D loss: 1.721302] [G loss: 12.146423] [gradient_penalty: 0.010403]\n",
            "[Epoch 1/200] [Batch 1419/3166] [D loss: 0.828902] [G loss: 12.435051] [gradient_penalty: 0.007361]\n",
            "[Epoch 1/200] [Batch 1420/3166] [D loss: 1.219707] [G loss: 11.679161] [gradient_penalty: 0.007641]\n",
            "[Epoch 1/200] [Batch 1421/3166] [D loss: 2.068956] [G loss: 11.595549] [gradient_penalty: 0.009745]\n",
            "[Epoch 1/200] [Batch 1422/3166] [D loss: -0.246615] [G loss: 11.067466] [gradient_penalty: 0.008421]\n",
            "[Epoch 1/200] [Batch 1423/3166] [D loss: 1.360545] [G loss: 11.486007] [gradient_penalty: 0.009600]\n",
            "[Epoch 1/200] [Batch 1424/3166] [D loss: 0.956807] [G loss: 10.752278] [gradient_penalty: 0.006494]\n",
            "[Epoch 1/200] [Batch 1425/3166] [D loss: 0.660329] [G loss: 11.594007] [gradient_penalty: 0.011284]\n",
            "[Epoch 1/200] [Batch 1426/3166] [D loss: 0.750261] [G loss: 12.421043] [gradient_penalty: 0.005133]\n",
            "[Epoch 1/200] [Batch 1427/3166] [D loss: 0.949937] [G loss: 11.798109] [gradient_penalty: 0.008518]\n",
            "[Epoch 1/200] [Batch 1428/3166] [D loss: 0.663957] [G loss: 12.457867] [gradient_penalty: 0.007760]\n",
            "[Epoch 1/200] [Batch 1429/3166] [D loss: -0.472633] [G loss: 12.133711] [gradient_penalty: 0.007134]\n",
            "[Epoch 1/200] [Batch 1430/3166] [D loss: 1.299853] [G loss: 11.935034] [gradient_penalty: 0.010019]\n",
            "[Epoch 1/200] [Batch 1431/3166] [D loss: 0.029369] [G loss: 12.929842] [gradient_penalty: 0.007245]\n",
            "[Epoch 1/200] [Batch 1432/3166] [D loss: 1.654649] [G loss: 11.929450] [gradient_penalty: 0.008274]\n",
            "[Epoch 1/200] [Batch 1433/3166] [D loss: 0.668992] [G loss: 12.475372] [gradient_penalty: 0.007130]\n",
            "[Epoch 1/200] [Batch 1434/3166] [D loss: -0.179098] [G loss: 11.780077] [gradient_penalty: 0.009538]\n",
            "log\n",
            "[Epoch 1/200] [Batch 1435/3166] [D loss: 1.102446] [G loss: 11.307432] [gradient_penalty: 0.007870]\n",
            "[Epoch 1/200] [Batch 1436/3166] [D loss: 0.821412] [G loss: 11.661219] [gradient_penalty: 0.009249]\n",
            "[Epoch 1/200] [Batch 1437/3166] [D loss: 1.449036] [G loss: 11.019229] [gradient_penalty: 0.006321]\n",
            "[Epoch 1/200] [Batch 1438/3166] [D loss: 1.227345] [G loss: 11.274706] [gradient_penalty: 0.007602]\n",
            "[Epoch 1/200] [Batch 1439/3166] [D loss: 0.101659] [G loss: 11.524813] [gradient_penalty: 0.008565]\n",
            "[Epoch 1/200] [Batch 1440/3166] [D loss: 1.780565] [G loss: 12.323944] [gradient_penalty: 0.008912]\n",
            "[Epoch 1/200] [Batch 1441/3166] [D loss: 1.557751] [G loss: 12.220572] [gradient_penalty: 0.008146]\n",
            "[Epoch 1/200] [Batch 1442/3166] [D loss: 0.590211] [G loss: 11.818956] [gradient_penalty: 0.010966]\n",
            "[Epoch 1/200] [Batch 1443/3166] [D loss: 0.225443] [G loss: 11.871370] [gradient_penalty: 0.008468]\n",
            "[Epoch 1/200] [Batch 1444/3166] [D loss: 0.151592] [G loss: 12.708842] [gradient_penalty: 0.008446]\n",
            "[Epoch 1/200] [Batch 1445/3166] [D loss: -0.012474] [G loss: 13.477413] [gradient_penalty: 0.008915]\n",
            "[Epoch 1/200] [Batch 1446/3166] [D loss: -0.098676] [G loss: 14.118461] [gradient_penalty: 0.010273]\n",
            "[Epoch 1/200] [Batch 1447/3166] [D loss: 0.207926] [G loss: 13.553066] [gradient_penalty: 0.010035]\n",
            "[Epoch 1/200] [Batch 1448/3166] [D loss: 0.599016] [G loss: 13.634545] [gradient_penalty: 0.007885]\n",
            "[Epoch 1/200] [Batch 1449/3166] [D loss: 0.483385] [G loss: 15.195547] [gradient_penalty: 0.007476]\n",
            "[Epoch 1/200] [Batch 1450/3166] [D loss: 0.849693] [G loss: 14.136864] [gradient_penalty: 0.010013]\n",
            "[Epoch 1/200] [Batch 1451/3166] [D loss: 0.680979] [G loss: 13.747486] [gradient_penalty: 0.009592]\n",
            "[Epoch 1/200] [Batch 1452/3166] [D loss: 0.789739] [G loss: 12.856191] [gradient_penalty: 0.009505]\n",
            "[Epoch 1/200] [Batch 1453/3166] [D loss: 2.244768] [G loss: 13.430576] [gradient_penalty: 0.012959]\n",
            "[Epoch 1/200] [Batch 1454/3166] [D loss: 1.626441] [G loss: 13.929996] [gradient_penalty: 0.006439]\n",
            "[Epoch 1/200] [Batch 1455/3166] [D loss: 2.248321] [G loss: 13.242791] [gradient_penalty: 0.008952]\n",
            "[Epoch 1/200] [Batch 1456/3166] [D loss: 0.952208] [G loss: 13.362135] [gradient_penalty: 0.007381]\n",
            "[Epoch 1/200] [Batch 1457/3166] [D loss: -0.162349] [G loss: 12.406771] [gradient_penalty: 0.008326]\n",
            "[Epoch 1/200] [Batch 1458/3166] [D loss: 0.482277] [G loss: 13.164666] [gradient_penalty: 0.007405]\n",
            "[Epoch 1/200] [Batch 1459/3166] [D loss: 0.481395] [G loss: 12.235639] [gradient_penalty: 0.006792]\n",
            "[Epoch 1/200] [Batch 1460/3166] [D loss: 0.526051] [G loss: 12.706993] [gradient_penalty: 0.012398]\n",
            "[Epoch 1/200] [Batch 1461/3166] [D loss: 1.733858] [G loss: 12.077869] [gradient_penalty: 0.009589]\n",
            "[Epoch 1/200] [Batch 1462/3166] [D loss: 1.049793] [G loss: 12.339954] [gradient_penalty: 0.011528]\n",
            "[Epoch 1/200] [Batch 1463/3166] [D loss: 0.281557] [G loss: 12.788951] [gradient_penalty: 0.008314]\n",
            "[Epoch 1/200] [Batch 1464/3166] [D loss: 1.610885] [G loss: 11.896237] [gradient_penalty: 0.008483]\n",
            "[Epoch 1/200] [Batch 1465/3166] [D loss: 0.534712] [G loss: 11.789453] [gradient_penalty: 0.006332]\n",
            "[Epoch 1/200] [Batch 1466/3166] [D loss: 0.861896] [G loss: 13.487719] [gradient_penalty: 0.007622]\n",
            "[Epoch 1/200] [Batch 1467/3166] [D loss: 1.693887] [G loss: 12.332107] [gradient_penalty: 0.006560]\n",
            "[Epoch 1/200] [Batch 1468/3166] [D loss: 1.115102] [G loss: 12.907509] [gradient_penalty: 0.007378]\n",
            "[Epoch 1/200] [Batch 1469/3166] [D loss: 0.768228] [G loss: 12.438890] [gradient_penalty: 0.011112]\n",
            "[Epoch 1/200] [Batch 1470/3166] [D loss: 1.014411] [G loss: 12.132959] [gradient_penalty: 0.008671]\n",
            "[Epoch 1/200] [Batch 1471/3166] [D loss: 1.189506] [G loss: 13.118823] [gradient_penalty: 0.006257]\n",
            "[Epoch 1/200] [Batch 1472/3166] [D loss: 0.732035] [G loss: 13.168835] [gradient_penalty: 0.007958]\n",
            "[Epoch 1/200] [Batch 1473/3166] [D loss: 0.503334] [G loss: 13.230404] [gradient_penalty: 0.010615]\n",
            "[Epoch 1/200] [Batch 1474/3166] [D loss: 1.805400] [G loss: 11.888759] [gradient_penalty: 0.008507]\n",
            "[Epoch 1/200] [Batch 1475/3166] [D loss: 0.931498] [G loss: 12.027417] [gradient_penalty: 0.007767]\n",
            "[Epoch 1/200] [Batch 1476/3166] [D loss: 0.733380] [G loss: 12.375976] [gradient_penalty: 0.010217]\n",
            "[Epoch 1/200] [Batch 1477/3166] [D loss: 0.326847] [G loss: 12.862429] [gradient_penalty: 0.006551]\n",
            "[Epoch 1/200] [Batch 1478/3166] [D loss: 0.839122] [G loss: 12.013029] [gradient_penalty: 0.007706]\n",
            "[Epoch 1/200] [Batch 1479/3166] [D loss: 1.067955] [G loss: 12.952598] [gradient_penalty: 0.007875]\n",
            "[Epoch 1/200] [Batch 1480/3166] [D loss: 1.101025] [G loss: 12.466074] [gradient_penalty: 0.010272]\n",
            "[Epoch 1/200] [Batch 1481/3166] [D loss: 0.800928] [G loss: 12.585157] [gradient_penalty: 0.009669]\n",
            "[Epoch 1/200] [Batch 1482/3166] [D loss: 0.999975] [G loss: 12.759151] [gradient_penalty: 0.009101]\n",
            "[Epoch 1/200] [Batch 1483/3166] [D loss: 1.086935] [G loss: 12.178602] [gradient_penalty: 0.008747]\n",
            "[Epoch 1/200] [Batch 1484/3166] [D loss: 1.740798] [G loss: 12.266253] [gradient_penalty: 0.007826]\n",
            "[Epoch 1/200] [Batch 1485/3166] [D loss: 0.788411] [G loss: 13.170607] [gradient_penalty: 0.010245]\n",
            "[Epoch 1/200] [Batch 1486/3166] [D loss: 1.114206] [G loss: 11.980447] [gradient_penalty: 0.009202]\n",
            "[Epoch 1/200] [Batch 1487/3166] [D loss: -0.394010] [G loss: 12.731282] [gradient_penalty: 0.008552]\n",
            "[Epoch 1/200] [Batch 1488/3166] [D loss: 1.016859] [G loss: 11.895607] [gradient_penalty: 0.008662]\n",
            "[Epoch 1/200] [Batch 1489/3166] [D loss: 1.097307] [G loss: 12.404879] [gradient_penalty: 0.011227]\n",
            "[Epoch 1/200] [Batch 1490/3166] [D loss: 1.685819] [G loss: 12.401983] [gradient_penalty: 0.011172]\n",
            "[Epoch 1/200] [Batch 1491/3166] [D loss: 0.137437] [G loss: 11.909947] [gradient_penalty: 0.008901]\n",
            "[Epoch 1/200] [Batch 1492/3166] [D loss: 0.228682] [G loss: 11.915985] [gradient_penalty: 0.006890]\n",
            "[Epoch 1/200] [Batch 1493/3166] [D loss: 0.693876] [G loss: 12.153521] [gradient_penalty: 0.008650]\n",
            "[Epoch 1/200] [Batch 1494/3166] [D loss: 0.945219] [G loss: 12.623369] [gradient_penalty: 0.009213]\n",
            "[Epoch 1/200] [Batch 1495/3166] [D loss: -0.157277] [G loss: 12.115488] [gradient_penalty: 0.007818]\n",
            "[Epoch 1/200] [Batch 1496/3166] [D loss: -0.475662] [G loss: 12.616655] [gradient_penalty: 0.006769]\n",
            "[Epoch 1/200] [Batch 1497/3166] [D loss: -0.158779] [G loss: 12.434893] [gradient_penalty: 0.007741]\n",
            "[Epoch 1/200] [Batch 1498/3166] [D loss: 0.891593] [G loss: 12.488926] [gradient_penalty: 0.009416]\n",
            "[Epoch 1/200] [Batch 1499/3166] [D loss: 2.404258] [G loss: 12.674893] [gradient_penalty: 0.008872]\n",
            "[Epoch 1/200] [Batch 1500/3166] [D loss: 1.149397] [G loss: 12.643583] [gradient_penalty: 0.005390]\n",
            "[Epoch 1/200] [Batch 1501/3166] [D loss: 1.249467] [G loss: 11.953415] [gradient_penalty: 0.008788]\n",
            "[Epoch 1/200] [Batch 1502/3166] [D loss: 0.481141] [G loss: 11.863905] [gradient_penalty: 0.007647]\n",
            "[Epoch 1/200] [Batch 1503/3166] [D loss: 0.665872] [G loss: 12.072378] [gradient_penalty: 0.007898]\n",
            "[Epoch 1/200] [Batch 1504/3166] [D loss: 1.097112] [G loss: 11.994329] [gradient_penalty: 0.008035]\n",
            "[Epoch 1/200] [Batch 1505/3166] [D loss: 0.919212] [G loss: 12.145598] [gradient_penalty: 0.008547]\n",
            "[Epoch 1/200] [Batch 1506/3166] [D loss: 0.281618] [G loss: 10.960025] [gradient_penalty: 0.008781]\n",
            "[Epoch 1/200] [Batch 1507/3166] [D loss: 0.232019] [G loss: 11.090235] [gradient_penalty: 0.008174]\n",
            "[Epoch 1/200] [Batch 1508/3166] [D loss: 0.945074] [G loss: 11.853064] [gradient_penalty: 0.011120]\n",
            "[Epoch 1/200] [Batch 1509/3166] [D loss: 0.584194] [G loss: 12.724615] [gradient_penalty: 0.012601]\n",
            "[Epoch 1/200] [Batch 1510/3166] [D loss: 0.891999] [G loss: 12.154057] [gradient_penalty: 0.008526]\n",
            "[Epoch 1/200] [Batch 1511/3166] [D loss: 0.717549] [G loss: 12.517409] [gradient_penalty: 0.006796]\n",
            "[Epoch 1/200] [Batch 1512/3166] [D loss: 1.164459] [G loss: 12.455967] [gradient_penalty: 0.006268]\n",
            "[Epoch 1/200] [Batch 1513/3166] [D loss: 1.091826] [G loss: 12.323389] [gradient_penalty: 0.007975]\n",
            "[Epoch 1/200] [Batch 1514/3166] [D loss: 0.521189] [G loss: 13.240907] [gradient_penalty: 0.008130]\n",
            "[Epoch 1/200] [Batch 1515/3166] [D loss: 0.709407] [G loss: 12.111124] [gradient_penalty: 0.007450]\n",
            "[Epoch 1/200] [Batch 1516/3166] [D loss: 0.963571] [G loss: 12.355804] [gradient_penalty: 0.010437]\n",
            "[Epoch 1/200] [Batch 1517/3166] [D loss: 1.943556] [G loss: 12.677955] [gradient_penalty: 0.006947]\n",
            "[Epoch 1/200] [Batch 1518/3166] [D loss: 1.636990] [G loss: 12.922539] [gradient_penalty: 0.008796]\n",
            "[Epoch 1/200] [Batch 1519/3166] [D loss: 1.320496] [G loss: 11.428842] [gradient_penalty: 0.009388]\n",
            "[Epoch 1/200] [Batch 1520/3166] [D loss: 1.275345] [G loss: 11.637014] [gradient_penalty: 0.010372]\n",
            "[Epoch 1/200] [Batch 1521/3166] [D loss: 0.462488] [G loss: 12.461906] [gradient_penalty: 0.009458]\n",
            "[Epoch 1/200] [Batch 1522/3166] [D loss: 0.702828] [G loss: 12.034899] [gradient_penalty: 0.010427]\n",
            "[Epoch 1/200] [Batch 1523/3166] [D loss: 1.639783] [G loss: 12.643822] [gradient_penalty: 0.009880]\n",
            "[Epoch 1/200] [Batch 1524/3166] [D loss: 0.967099] [G loss: 11.399881] [gradient_penalty: 0.006552]\n",
            "[Epoch 1/200] [Batch 1525/3166] [D loss: 2.067355] [G loss: 11.356703] [gradient_penalty: 0.008672]\n",
            "[Epoch 1/200] [Batch 1526/3166] [D loss: 1.398097] [G loss: 11.894745] [gradient_penalty: 0.008183]\n",
            "[Epoch 1/200] [Batch 1527/3166] [D loss: 0.346200] [G loss: 11.685755] [gradient_penalty: 0.009457]\n",
            "[Epoch 1/200] [Batch 1528/3166] [D loss: 2.467372] [G loss: 12.024487] [gradient_penalty: 0.006784]\n",
            "[Epoch 1/200] [Batch 1529/3166] [D loss: 0.871133] [G loss: 11.121988] [gradient_penalty: 0.007701]\n",
            "[Epoch 1/200] [Batch 1530/3166] [D loss: 0.040691] [G loss: 10.790257] [gradient_penalty: 0.010882]\n",
            "[Epoch 1/200] [Batch 1531/3166] [D loss: 0.350640] [G loss: 10.754902] [gradient_penalty: 0.010784]\n",
            "[Epoch 1/200] [Batch 1532/3166] [D loss: 0.991862] [G loss: 11.483092] [gradient_penalty: 0.008494]\n",
            "[Epoch 1/200] [Batch 1533/3166] [D loss: 0.327795] [G loss: 11.660381] [gradient_penalty: 0.006886]\n",
            "[Epoch 1/200] [Batch 1534/3166] [D loss: 0.182099] [G loss: 11.645764] [gradient_penalty: 0.007039]\n",
            "log\n",
            "[Epoch 1/200] [Batch 1535/3166] [D loss: 0.019055] [G loss: 11.217027] [gradient_penalty: 0.005055]\n",
            "[Epoch 1/200] [Batch 1536/3166] [D loss: 0.348313] [G loss: 11.264359] [gradient_penalty: 0.007971]\n",
            "[Epoch 1/200] [Batch 1537/3166] [D loss: 0.923446] [G loss: 11.581793] [gradient_penalty: 0.006623]\n",
            "[Epoch 1/200] [Batch 1538/3166] [D loss: 0.445916] [G loss: 10.955193] [gradient_penalty: 0.006654]\n",
            "[Epoch 1/200] [Batch 1539/3166] [D loss: -0.068980] [G loss: 11.471665] [gradient_penalty: 0.006999]\n",
            "[Epoch 1/200] [Batch 1540/3166] [D loss: -0.658809] [G loss: 11.689142] [gradient_penalty: 0.008130]\n",
            "[Epoch 1/200] [Batch 1541/3166] [D loss: -0.346922] [G loss: 11.412024] [gradient_penalty: 0.009441]\n",
            "[Epoch 1/200] [Batch 1542/3166] [D loss: 2.101857] [G loss: 11.247707] [gradient_penalty: 0.007567]\n",
            "[Epoch 1/200] [Batch 1543/3166] [D loss: 0.922809] [G loss: 10.784997] [gradient_penalty: 0.006915]\n",
            "[Epoch 1/200] [Batch 1544/3166] [D loss: 0.536478] [G loss: 11.964756] [gradient_penalty: 0.009642]\n",
            "[Epoch 1/200] [Batch 1545/3166] [D loss: 0.153600] [G loss: 11.219796] [gradient_penalty: 0.009514]\n",
            "[Epoch 1/200] [Batch 1546/3166] [D loss: 0.938785] [G loss: 11.162556] [gradient_penalty: 0.007757]\n",
            "[Epoch 1/200] [Batch 1547/3166] [D loss: 0.108799] [G loss: 11.589506] [gradient_penalty: 0.005768]\n",
            "[Epoch 1/200] [Batch 1548/3166] [D loss: 1.793120] [G loss: 11.441557] [gradient_penalty: 0.007336]\n",
            "[Epoch 1/200] [Batch 1549/3166] [D loss: 0.108319] [G loss: 11.020208] [gradient_penalty: 0.005817]\n",
            "[Epoch 1/200] [Batch 1550/3166] [D loss: 1.822568] [G loss: 11.050410] [gradient_penalty: 0.006293]\n",
            "[Epoch 1/200] [Batch 1551/3166] [D loss: 0.547446] [G loss: 11.318421] [gradient_penalty: 0.008369]\n",
            "[Epoch 1/200] [Batch 1552/3166] [D loss: 0.604402] [G loss: 12.743706] [gradient_penalty: 0.007899]\n",
            "[Epoch 1/200] [Batch 1553/3166] [D loss: 0.803993] [G loss: 12.257434] [gradient_penalty: 0.008776]\n",
            "[Epoch 1/200] [Batch 1554/3166] [D loss: 0.621258] [G loss: 12.657768] [gradient_penalty: 0.006885]\n",
            "[Epoch 1/200] [Batch 1555/3166] [D loss: 0.500025] [G loss: 12.860840] [gradient_penalty: 0.009290]\n",
            "[Epoch 1/200] [Batch 1556/3166] [D loss: 1.957789] [G loss: 12.807680] [gradient_penalty: 0.012735]\n",
            "[Epoch 1/200] [Batch 1557/3166] [D loss: 0.422798] [G loss: 12.836279] [gradient_penalty: 0.008386]\n",
            "[Epoch 1/200] [Batch 1558/3166] [D loss: 0.856831] [G loss: 12.718086] [gradient_penalty: 0.008665]\n",
            "[Epoch 1/200] [Batch 1559/3166] [D loss: 0.650027] [G loss: 12.459155] [gradient_penalty: 0.010917]\n",
            "[Epoch 1/200] [Batch 1560/3166] [D loss: 0.488053] [G loss: 12.666220] [gradient_penalty: 0.011637]\n",
            "[Epoch 1/200] [Batch 1561/3166] [D loss: 0.236556] [G loss: 12.134024] [gradient_penalty: 0.006881]\n",
            "[Epoch 1/200] [Batch 1562/3166] [D loss: 1.076556] [G loss: 11.447872] [gradient_penalty: 0.009034]\n",
            "[Epoch 1/200] [Batch 1563/3166] [D loss: 1.066113] [G loss: 11.955863] [gradient_penalty: 0.008107]\n",
            "[Epoch 1/200] [Batch 1564/3166] [D loss: 0.591993] [G loss: 12.263432] [gradient_penalty: 0.007567]\n",
            "[Epoch 1/200] [Batch 1565/3166] [D loss: 0.886054] [G loss: 11.840685] [gradient_penalty: 0.009573]\n",
            "[Epoch 1/200] [Batch 1566/3166] [D loss: 1.163866] [G loss: 11.602015] [gradient_penalty: 0.008244]\n",
            "[Epoch 1/200] [Batch 1567/3166] [D loss: 0.212498] [G loss: 12.320072] [gradient_penalty: 0.007923]\n",
            "[Epoch 1/200] [Batch 1568/3166] [D loss: 1.329904] [G loss: 12.163019] [gradient_penalty: 0.009782]\n",
            "[Epoch 1/200] [Batch 1569/3166] [D loss: 0.427328] [G loss: 11.890137] [gradient_penalty: 0.008960]\n",
            "[Epoch 1/200] [Batch 1570/3166] [D loss: -0.142054] [G loss: 11.893839] [gradient_penalty: 0.009952]\n",
            "[Epoch 1/200] [Batch 1571/3166] [D loss: 0.683187] [G loss: 11.484560] [gradient_penalty: 0.008482]\n",
            "[Epoch 1/200] [Batch 1572/3166] [D loss: 0.611168] [G loss: 11.499639] [gradient_penalty: 0.007314]\n",
            "[Epoch 1/200] [Batch 1573/3166] [D loss: -0.258976] [G loss: 11.252378] [gradient_penalty: 0.008461]\n",
            "[Epoch 1/200] [Batch 1574/3166] [D loss: 0.131236] [G loss: 11.614265] [gradient_penalty: 0.009612]\n",
            "[Epoch 1/200] [Batch 1575/3166] [D loss: 0.986808] [G loss: 10.688263] [gradient_penalty: 0.007386]\n",
            "[Epoch 1/200] [Batch 1576/3166] [D loss: 1.572309] [G loss: 11.638384] [gradient_penalty: 0.007287]\n",
            "[Epoch 1/200] [Batch 1577/3166] [D loss: 1.759494] [G loss: 11.700268] [gradient_penalty: 0.009999]\n",
            "[Epoch 1/200] [Batch 1578/3166] [D loss: 1.467529] [G loss: 11.675834] [gradient_penalty: 0.008987]\n",
            "[Epoch 1/200] [Batch 1579/3166] [D loss: 1.510594] [G loss: 11.266133] [gradient_penalty: 0.009471]\n",
            "[Epoch 1/200] [Batch 1580/3166] [D loss: 1.737071] [G loss: 11.115764] [gradient_penalty: 0.010899]\n",
            "[Epoch 1/200] [Batch 1581/3166] [D loss: -0.326751] [G loss: 11.513201] [gradient_penalty: 0.007112]\n",
            "[Epoch 1/200] [Batch 1582/3166] [D loss: 0.536622] [G loss: 11.654789] [gradient_penalty: 0.009250]\n",
            "[Epoch 1/200] [Batch 1583/3166] [D loss: 1.274537] [G loss: 11.516594] [gradient_penalty: 0.010767]\n",
            "[Epoch 1/200] [Batch 1584/3166] [D loss: 1.999740] [G loss: 11.153157] [gradient_penalty: 0.011972]\n",
            "[Epoch 1/200] [Batch 1585/3166] [D loss: 0.781067] [G loss: 11.334919] [gradient_penalty: 0.006635]\n",
            "[Epoch 1/200] [Batch 1586/3166] [D loss: 0.852911] [G loss: 11.731101] [gradient_penalty: 0.011069]\n",
            "[Epoch 1/200] [Batch 1587/3166] [D loss: 1.224878] [G loss: 11.851465] [gradient_penalty: 0.011553]\n",
            "[Epoch 1/200] [Batch 1588/3166] [D loss: 0.502376] [G loss: 12.070896] [gradient_penalty: 0.008692]\n",
            "[Epoch 1/200] [Batch 1589/3166] [D loss: 0.047117] [G loss: 12.053030] [gradient_penalty: 0.010228]\n",
            "[Epoch 1/200] [Batch 1590/3166] [D loss: 0.915617] [G loss: 11.931711] [gradient_penalty: 0.009431]\n",
            "[Epoch 1/200] [Batch 1591/3166] [D loss: 0.231911] [G loss: 12.514125] [gradient_penalty: 0.009639]\n",
            "[Epoch 1/200] [Batch 1592/3166] [D loss: 0.386262] [G loss: 11.487301] [gradient_penalty: 0.005967]\n",
            "[Epoch 1/200] [Batch 1593/3166] [D loss: 0.609748] [G loss: 12.435061] [gradient_penalty: 0.008290]\n",
            "[Epoch 1/200] [Batch 1594/3166] [D loss: 0.839964] [G loss: 12.612403] [gradient_penalty: 0.011750]\n",
            "[Epoch 1/200] [Batch 1595/3166] [D loss: 0.430945] [G loss: 11.425756] [gradient_penalty: 0.009498]\n",
            "[Epoch 1/200] [Batch 1596/3166] [D loss: -0.106206] [G loss: 11.473845] [gradient_penalty: 0.007514]\n",
            "[Epoch 1/200] [Batch 1597/3166] [D loss: 1.315849] [G loss: 12.134689] [gradient_penalty: 0.008037]\n",
            "[Epoch 1/200] [Batch 1598/3166] [D loss: 1.210948] [G loss: 11.883060] [gradient_penalty: 0.010673]\n",
            "[Epoch 1/200] [Batch 1599/3166] [D loss: 1.110856] [G loss: 11.721667] [gradient_penalty: 0.011395]\n",
            "[Epoch 1/200] [Batch 1600/3166] [D loss: 1.278066] [G loss: 12.228056] [gradient_penalty: 0.010237]\n",
            "[Epoch 1/200] [Batch 1601/3166] [D loss: 0.275556] [G loss: 12.045998] [gradient_penalty: 0.009743]\n",
            "[Epoch 1/200] [Batch 1602/3166] [D loss: 1.170843] [G loss: 12.104008] [gradient_penalty: 0.005935]\n",
            "[Epoch 1/200] [Batch 1603/3166] [D loss: 0.806732] [G loss: 12.049730] [gradient_penalty: 0.009186]\n",
            "[Epoch 1/200] [Batch 1604/3166] [D loss: 0.849961] [G loss: 11.885183] [gradient_penalty: 0.008225]\n",
            "[Epoch 1/200] [Batch 1605/3166] [D loss: 0.947415] [G loss: 12.043838] [gradient_penalty: 0.011346]\n",
            "[Epoch 1/200] [Batch 1606/3166] [D loss: 1.040431] [G loss: 11.358593] [gradient_penalty: 0.006483]\n",
            "[Epoch 1/200] [Batch 1607/3166] [D loss: -0.076756] [G loss: 11.881231] [gradient_penalty: 0.008431]\n",
            "[Epoch 1/200] [Batch 1608/3166] [D loss: 0.198136] [G loss: 12.926243] [gradient_penalty: 0.009787]\n",
            "[Epoch 1/200] [Batch 1609/3166] [D loss: 1.427350] [G loss: 11.494289] [gradient_penalty: 0.010188]\n",
            "[Epoch 1/200] [Batch 1610/3166] [D loss: -0.075055] [G loss: 11.531831] [gradient_penalty: 0.005850]\n",
            "[Epoch 1/200] [Batch 1611/3166] [D loss: 0.967959] [G loss: 12.660159] [gradient_penalty: 0.009134]\n",
            "[Epoch 1/200] [Batch 1612/3166] [D loss: 1.013210] [G loss: 11.265390] [gradient_penalty: 0.008554]\n",
            "[Epoch 1/200] [Batch 1613/3166] [D loss: 1.280922] [G loss: 11.565767] [gradient_penalty: 0.009784]\n",
            "[Epoch 1/200] [Batch 1614/3166] [D loss: 1.059296] [G loss: 11.989034] [gradient_penalty: 0.005630]\n",
            "[Epoch 1/200] [Batch 1615/3166] [D loss: 0.937403] [G loss: 12.179016] [gradient_penalty: 0.007931]\n",
            "[Epoch 1/200] [Batch 1616/3166] [D loss: 1.053705] [G loss: 11.772057] [gradient_penalty: 0.006462]\n",
            "[Epoch 1/200] [Batch 1617/3166] [D loss: 0.577777] [G loss: 12.871567] [gradient_penalty: 0.007141]\n",
            "[Epoch 1/200] [Batch 1618/3166] [D loss: 0.786891] [G loss: 11.381397] [gradient_penalty: 0.008456]\n",
            "[Epoch 1/200] [Batch 1619/3166] [D loss: -0.467820] [G loss: 12.910068] [gradient_penalty: 0.009425]\n",
            "[Epoch 1/200] [Batch 1620/3166] [D loss: -0.014601] [G loss: 12.932527] [gradient_penalty: 0.008706]\n",
            "[Epoch 1/200] [Batch 1621/3166] [D loss: -0.193117] [G loss: 13.313936] [gradient_penalty: 0.009945]\n",
            "[Epoch 1/200] [Batch 1622/3166] [D loss: -0.012645] [G loss: 12.793144] [gradient_penalty: 0.009004]\n",
            "[Epoch 1/200] [Batch 1623/3166] [D loss: 0.675583] [G loss: 12.995031] [gradient_penalty: 0.008992]\n",
            "[Epoch 1/200] [Batch 1624/3166] [D loss: 0.307405] [G loss: 12.813435] [gradient_penalty: 0.009187]\n",
            "[Epoch 1/200] [Batch 1625/3166] [D loss: 0.798207] [G loss: 13.945447] [gradient_penalty: 0.009696]\n",
            "[Epoch 1/200] [Batch 1626/3166] [D loss: 1.604515] [G loss: 13.656971] [gradient_penalty: 0.009073]\n",
            "[Epoch 1/200] [Batch 1627/3166] [D loss: 0.578171] [G loss: 12.818419] [gradient_penalty: 0.008749]\n",
            "[Epoch 1/200] [Batch 1628/3166] [D loss: 2.285835] [G loss: 11.411146] [gradient_penalty: 0.011569]\n",
            "[Epoch 1/200] [Batch 1629/3166] [D loss: 0.834524] [G loss: 12.342522] [gradient_penalty: 0.008577]\n",
            "[Epoch 1/200] [Batch 1630/3166] [D loss: 1.648945] [G loss: 12.373734] [gradient_penalty: 0.007962]\n",
            "[Epoch 1/200] [Batch 1631/3166] [D loss: 1.524895] [G loss: 12.133718] [gradient_penalty: 0.010426]\n",
            "[Epoch 1/200] [Batch 1632/3166] [D loss: 0.717794] [G loss: 11.766619] [gradient_penalty: 0.007591]\n",
            "[Epoch 1/200] [Batch 1633/3166] [D loss: 2.413095] [G loss: 11.916626] [gradient_penalty: 0.010347]\n",
            "[Epoch 1/200] [Batch 1634/3166] [D loss: 0.214209] [G loss: 11.720435] [gradient_penalty: 0.008906]\n",
            "log\n",
            "[Epoch 1/200] [Batch 1635/3166] [D loss: 0.387941] [G loss: 11.611416] [gradient_penalty: 0.007695]\n",
            "[Epoch 1/200] [Batch 1636/3166] [D loss: 0.502861] [G loss: 12.742250] [gradient_penalty: 0.006716]\n",
            "[Epoch 1/200] [Batch 1637/3166] [D loss: 2.106163] [G loss: 11.814157] [gradient_penalty: 0.010770]\n",
            "[Epoch 1/200] [Batch 1638/3166] [D loss: 0.996331] [G loss: 12.633494] [gradient_penalty: 0.006627]\n",
            "[Epoch 1/200] [Batch 1639/3166] [D loss: 0.958619] [G loss: 12.385030] [gradient_penalty: 0.007011]\n",
            "[Epoch 1/200] [Batch 1640/3166] [D loss: 1.448359] [G loss: 11.786055] [gradient_penalty: 0.009943]\n",
            "[Epoch 1/200] [Batch 1641/3166] [D loss: -0.491540] [G loss: 11.576796] [gradient_penalty: 0.008844]\n",
            "[Epoch 1/200] [Batch 1642/3166] [D loss: 0.836427] [G loss: 11.377875] [gradient_penalty: 0.008054]\n",
            "[Epoch 1/200] [Batch 1643/3166] [D loss: 2.701148] [G loss: 11.703982] [gradient_penalty: 0.007878]\n",
            "[Epoch 1/200] [Batch 1644/3166] [D loss: 0.194371] [G loss: 12.303684] [gradient_penalty: 0.012291]\n",
            "[Epoch 1/200] [Batch 1645/3166] [D loss: 1.713294] [G loss: 11.905923] [gradient_penalty: 0.008983]\n",
            "[Epoch 1/200] [Batch 1646/3166] [D loss: 0.838413] [G loss: 11.963535] [gradient_penalty: 0.008943]\n",
            "[Epoch 1/200] [Batch 1647/3166] [D loss: 1.267807] [G loss: 12.291704] [gradient_penalty: 0.007062]\n",
            "[Epoch 1/200] [Batch 1648/3166] [D loss: 0.827655] [G loss: 12.397068] [gradient_penalty: 0.008705]\n",
            "[Epoch 1/200] [Batch 1649/3166] [D loss: 0.503210] [G loss: 11.760045] [gradient_penalty: 0.008708]\n",
            "[Epoch 1/200] [Batch 1650/3166] [D loss: 0.967235] [G loss: 11.365054] [gradient_penalty: 0.006239]\n",
            "[Epoch 1/200] [Batch 1651/3166] [D loss: 1.906661] [G loss: 12.227201] [gradient_penalty: 0.010307]\n",
            "[Epoch 1/200] [Batch 1652/3166] [D loss: 1.799098] [G loss: 11.759647] [gradient_penalty: 0.005642]\n",
            "[Epoch 1/200] [Batch 1653/3166] [D loss: 0.915824] [G loss: 11.854252] [gradient_penalty: 0.010551]\n",
            "[Epoch 1/200] [Batch 1654/3166] [D loss: 0.356112] [G loss: 11.430720] [gradient_penalty: 0.008173]\n",
            "[Epoch 1/200] [Batch 1655/3166] [D loss: 0.796790] [G loss: 11.409007] [gradient_penalty: 0.007818]\n",
            "[Epoch 1/200] [Batch 1656/3166] [D loss: 0.138816] [G loss: 11.823509] [gradient_penalty: 0.007935]\n",
            "[Epoch 1/200] [Batch 1657/3166] [D loss: 0.544113] [G loss: 12.377884] [gradient_penalty: 0.007292]\n",
            "[Epoch 1/200] [Batch 1658/3166] [D loss: 0.632654] [G loss: 12.081131] [gradient_penalty: 0.008237]\n",
            "[Epoch 1/200] [Batch 1659/3166] [D loss: 0.418488] [G loss: 11.894890] [gradient_penalty: 0.007933]\n",
            "[Epoch 1/200] [Batch 1660/3166] [D loss: 1.114073] [G loss: 12.998207] [gradient_penalty: 0.009951]\n",
            "[Epoch 1/200] [Batch 1661/3166] [D loss: 0.314259] [G loss: 12.488881] [gradient_penalty: 0.010967]\n",
            "[Epoch 1/200] [Batch 1662/3166] [D loss: 1.772437] [G loss: 12.467566] [gradient_penalty: 0.008431]\n",
            "[Epoch 1/200] [Batch 1663/3166] [D loss: 1.251158] [G loss: 13.168941] [gradient_penalty: 0.011915]\n",
            "[Epoch 1/200] [Batch 1664/3166] [D loss: 1.114894] [G loss: 12.453825] [gradient_penalty: 0.008349]\n",
            "[Epoch 1/200] [Batch 1665/3166] [D loss: 1.291789] [G loss: 12.458918] [gradient_penalty: 0.011920]\n",
            "[Epoch 1/200] [Batch 1666/3166] [D loss: 1.075243] [G loss: 11.995170] [gradient_penalty: 0.008784]\n",
            "[Epoch 1/200] [Batch 1667/3166] [D loss: 0.996544] [G loss: 12.698294] [gradient_penalty: 0.012603]\n",
            "[Epoch 1/200] [Batch 1668/3166] [D loss: 0.805229] [G loss: 12.331464] [gradient_penalty: 0.006586]\n",
            "[Epoch 1/200] [Batch 1669/3166] [D loss: -0.222374] [G loss: 11.956442] [gradient_penalty: 0.010857]\n",
            "[Epoch 1/200] [Batch 1670/3166] [D loss: 0.613772] [G loss: 11.415646] [gradient_penalty: 0.008782]\n",
            "[Epoch 1/200] [Batch 1671/3166] [D loss: 1.523366] [G loss: 12.749261] [gradient_penalty: 0.010014]\n",
            "[Epoch 1/200] [Batch 1672/3166] [D loss: -0.006120] [G loss: 11.610012] [gradient_penalty: 0.008249]\n",
            "[Epoch 1/200] [Batch 1673/3166] [D loss: -0.220165] [G loss: 13.103978] [gradient_penalty: 0.011394]\n",
            "[Epoch 1/200] [Batch 1674/3166] [D loss: -0.442049] [G loss: 13.526467] [gradient_penalty: 0.007854]\n",
            "[Epoch 1/200] [Batch 1675/3166] [D loss: 0.553707] [G loss: 12.445459] [gradient_penalty: 0.009328]\n",
            "[Epoch 1/200] [Batch 1676/3166] [D loss: 0.092498] [G loss: 11.722333] [gradient_penalty: 0.008281]\n",
            "[Epoch 1/200] [Batch 1677/3166] [D loss: 0.404897] [G loss: 11.789223] [gradient_penalty: 0.011005]\n",
            "[Epoch 1/200] [Batch 1678/3166] [D loss: 0.817155] [G loss: 11.467071] [gradient_penalty: 0.009976]\n",
            "[Epoch 1/200] [Batch 1679/3166] [D loss: 0.300567] [G loss: 11.345751] [gradient_penalty: 0.009958]\n",
            "[Epoch 1/200] [Batch 1680/3166] [D loss: -0.017335] [G loss: 11.848146] [gradient_penalty: 0.008564]\n",
            "[Epoch 1/200] [Batch 1681/3166] [D loss: 0.955449] [G loss: 11.408940] [gradient_penalty: 0.009265]\n",
            "[Epoch 1/200] [Batch 1682/3166] [D loss: 1.330076] [G loss: 11.487283] [gradient_penalty: 0.010951]\n",
            "[Epoch 1/200] [Batch 1683/3166] [D loss: 1.205371] [G loss: 11.517813] [gradient_penalty: 0.006862]\n",
            "[Epoch 1/200] [Batch 1684/3166] [D loss: 1.315370] [G loss: 11.201941] [gradient_penalty: 0.008529]\n",
            "[Epoch 1/200] [Batch 1685/3166] [D loss: 1.309554] [G loss: 12.713078] [gradient_penalty: 0.008398]\n",
            "[Epoch 1/200] [Batch 1686/3166] [D loss: -0.232040] [G loss: 12.218725] [gradient_penalty: 0.011235]\n",
            "[Epoch 1/200] [Batch 1687/3166] [D loss: 0.997910] [G loss: 11.570047] [gradient_penalty: 0.008072]\n",
            "[Epoch 1/200] [Batch 1688/3166] [D loss: -0.024077] [G loss: 12.720084] [gradient_penalty: 0.009771]\n",
            "[Epoch 1/200] [Batch 1689/3166] [D loss: 0.619222] [G loss: 11.810433] [gradient_penalty: 0.006729]\n",
            "[Epoch 1/200] [Batch 1690/3166] [D loss: 0.789863] [G loss: 12.970996] [gradient_penalty: 0.008818]\n",
            "[Epoch 1/200] [Batch 1691/3166] [D loss: 0.161632] [G loss: 12.207979] [gradient_penalty: 0.010675]\n",
            "[Epoch 1/200] [Batch 1692/3166] [D loss: 0.562890] [G loss: 13.231351] [gradient_penalty: 0.008748]\n",
            "[Epoch 1/200] [Batch 1693/3166] [D loss: 0.573238] [G loss: 12.961060] [gradient_penalty: 0.009831]\n",
            "[Epoch 1/200] [Batch 1694/3166] [D loss: 0.998101] [G loss: 12.467881] [gradient_penalty: 0.010134]\n",
            "[Epoch 1/200] [Batch 1695/3166] [D loss: 0.697386] [G loss: 11.674472] [gradient_penalty: 0.012958]\n",
            "[Epoch 1/200] [Batch 1696/3166] [D loss: 2.014220] [G loss: 12.479239] [gradient_penalty: 0.009521]\n",
            "[Epoch 1/200] [Batch 1697/3166] [D loss: 1.040547] [G loss: 12.854874] [gradient_penalty: 0.010622]\n",
            "[Epoch 1/200] [Batch 1698/3166] [D loss: 1.565786] [G loss: 12.934002] [gradient_penalty: 0.009431]\n",
            "[Epoch 1/200] [Batch 1699/3166] [D loss: 1.129238] [G loss: 12.148861] [gradient_penalty: 0.007522]\n",
            "[Epoch 1/200] [Batch 1700/3166] [D loss: 1.451444] [G loss: 11.570509] [gradient_penalty: 0.008319]\n",
            "[Epoch 1/200] [Batch 1701/3166] [D loss: 2.484145] [G loss: 12.039856] [gradient_penalty: 0.009452]\n",
            "[Epoch 1/200] [Batch 1702/3166] [D loss: 1.159531] [G loss: 11.739853] [gradient_penalty: 0.010972]\n",
            "[Epoch 1/200] [Batch 1703/3166] [D loss: 0.656880] [G loss: 12.303508] [gradient_penalty: 0.012660]\n",
            "[Epoch 1/200] [Batch 1704/3166] [D loss: 0.158233] [G loss: 12.118714] [gradient_penalty: 0.010926]\n",
            "[Epoch 1/200] [Batch 1705/3166] [D loss: 1.466911] [G loss: 12.528650] [gradient_penalty: 0.007821]\n",
            "[Epoch 1/200] [Batch 1706/3166] [D loss: 1.660345] [G loss: 12.451564] [gradient_penalty: 0.008712]\n",
            "[Epoch 1/200] [Batch 1707/3166] [D loss: 1.156619] [G loss: 11.739049] [gradient_penalty: 0.009591]\n",
            "[Epoch 1/200] [Batch 1708/3166] [D loss: 0.021149] [G loss: 12.704691] [gradient_penalty: 0.007974]\n",
            "[Epoch 1/200] [Batch 1709/3166] [D loss: -0.025128] [G loss: 12.041147] [gradient_penalty: 0.011726]\n",
            "[Epoch 1/200] [Batch 1710/3166] [D loss: 0.468966] [G loss: 12.669662] [gradient_penalty: 0.007292]\n",
            "[Epoch 1/200] [Batch 1711/3166] [D loss: 0.035241] [G loss: 13.091221] [gradient_penalty: 0.009307]\n",
            "[Epoch 1/200] [Batch 1712/3166] [D loss: 1.399033] [G loss: 13.027416] [gradient_penalty: 0.008955]\n",
            "[Epoch 1/200] [Batch 1713/3166] [D loss: 1.040802] [G loss: 11.315147] [gradient_penalty: 0.011118]\n",
            "[Epoch 1/200] [Batch 1714/3166] [D loss: 0.683099] [G loss: 11.253073] [gradient_penalty: 0.010176]\n",
            "[Epoch 1/200] [Batch 1715/3166] [D loss: 0.202310] [G loss: 11.919837] [gradient_penalty: 0.007803]\n",
            "[Epoch 1/200] [Batch 1716/3166] [D loss: 1.107292] [G loss: 11.236957] [gradient_penalty: 0.009147]\n",
            "[Epoch 1/200] [Batch 1717/3166] [D loss: 0.837123] [G loss: 10.817013] [gradient_penalty: 0.008941]\n",
            "[Epoch 1/200] [Batch 1718/3166] [D loss: 1.491079] [G loss: 10.652206] [gradient_penalty: 0.010811]\n",
            "[Epoch 1/200] [Batch 1719/3166] [D loss: 0.766630] [G loss: 11.197182] [gradient_penalty: 0.008704]\n",
            "[Epoch 1/200] [Batch 1720/3166] [D loss: 2.098054] [G loss: 11.615667] [gradient_penalty: 0.011178]\n",
            "[Epoch 1/200] [Batch 1721/3166] [D loss: 0.217777] [G loss: 11.056070] [gradient_penalty: 0.007775]\n",
            "[Epoch 1/200] [Batch 1722/3166] [D loss: 0.387713] [G loss: 11.941148] [gradient_penalty: 0.005336]\n",
            "[Epoch 1/200] [Batch 1723/3166] [D loss: 0.553018] [G loss: 12.116318] [gradient_penalty: 0.009495]\n",
            "[Epoch 1/200] [Batch 1724/3166] [D loss: 0.719361] [G loss: 12.697813] [gradient_penalty: 0.007922]\n",
            "[Epoch 1/200] [Batch 1725/3166] [D loss: 1.456177] [G loss: 12.674159] [gradient_penalty: 0.009063]\n",
            "[Epoch 1/200] [Batch 1726/3166] [D loss: 0.683593] [G loss: 12.484642] [gradient_penalty: 0.008182]\n",
            "[Epoch 1/200] [Batch 1727/3166] [D loss: -0.212633] [G loss: 13.356195] [gradient_penalty: 0.011166]\n",
            "[Epoch 1/200] [Batch 1728/3166] [D loss: 1.514986] [G loss: 13.181562] [gradient_penalty: 0.013512]\n",
            "[Epoch 1/200] [Batch 1729/3166] [D loss: 0.434408] [G loss: 13.128950] [gradient_penalty: 0.008372]\n",
            "[Epoch 1/200] [Batch 1730/3166] [D loss: 0.521606] [G loss: 14.489377] [gradient_penalty: 0.009018]\n",
            "[Epoch 1/200] [Batch 1731/3166] [D loss: 1.031519] [G loss: 13.706777] [gradient_penalty: 0.011113]\n",
            "[Epoch 1/200] [Batch 1732/3166] [D loss: -0.428818] [G loss: 13.413355] [gradient_penalty: 0.010266]\n",
            "[Epoch 1/200] [Batch 1733/3166] [D loss: 1.285182] [G loss: 13.811178] [gradient_penalty: 0.012316]\n",
            "[Epoch 1/200] [Batch 1734/3166] [D loss: 0.500388] [G loss: 14.551325] [gradient_penalty: 0.010568]\n",
            "log\n",
            "[Epoch 1/200] [Batch 1735/3166] [D loss: 0.786776] [G loss: 14.383158] [gradient_penalty: 0.015543]\n",
            "[Epoch 1/200] [Batch 1736/3166] [D loss: 0.045428] [G loss: 14.377575] [gradient_penalty: 0.010785]\n",
            "[Epoch 1/200] [Batch 1737/3166] [D loss: 0.410115] [G loss: 12.944331] [gradient_penalty: 0.008059]\n",
            "[Epoch 1/200] [Batch 1738/3166] [D loss: 0.145651] [G loss: 13.889364] [gradient_penalty: 0.010605]\n",
            "[Epoch 1/200] [Batch 1739/3166] [D loss: 0.822974] [G loss: 12.517406] [gradient_penalty: 0.009432]\n",
            "[Epoch 1/200] [Batch 1740/3166] [D loss: 1.724314] [G loss: 12.552273] [gradient_penalty: 0.009749]\n",
            "[Epoch 1/200] [Batch 1741/3166] [D loss: 1.746271] [G loss: 13.254722] [gradient_penalty: 0.010633]\n",
            "[Epoch 1/200] [Batch 1742/3166] [D loss: 1.737452] [G loss: 13.131475] [gradient_penalty: 0.012714]\n",
            "[Epoch 1/200] [Batch 1743/3166] [D loss: 0.508741] [G loss: 12.680792] [gradient_penalty: 0.010223]\n",
            "[Epoch 1/200] [Batch 1744/3166] [D loss: 0.332975] [G loss: 12.893489] [gradient_penalty: 0.012827]\n",
            "[Epoch 1/200] [Batch 1745/3166] [D loss: 1.409592] [G loss: 13.689625] [gradient_penalty: 0.010415]\n",
            "[Epoch 1/200] [Batch 1746/3166] [D loss: 0.430615] [G loss: 13.010092] [gradient_penalty: 0.010967]\n",
            "[Epoch 1/200] [Batch 1747/3166] [D loss: -0.302992] [G loss: 13.309542] [gradient_penalty: 0.011489]\n",
            "[Epoch 1/200] [Batch 1748/3166] [D loss: -0.394427] [G loss: 12.512642] [gradient_penalty: 0.008049]\n",
            "[Epoch 1/200] [Batch 1749/3166] [D loss: -1.084497] [G loss: 14.501583] [gradient_penalty: 0.008470]\n",
            "[Epoch 1/200] [Batch 1750/3166] [D loss: -0.965081] [G loss: 13.944856] [gradient_penalty: 0.007738]\n",
            "[Epoch 1/200] [Batch 1751/3166] [D loss: 2.284281] [G loss: 12.704165] [gradient_penalty: 0.014825]\n",
            "[Epoch 1/200] [Batch 1752/3166] [D loss: 0.161696] [G loss: 12.507290] [gradient_penalty: 0.008320]\n",
            "[Epoch 1/200] [Batch 1753/3166] [D loss: 1.463115] [G loss: 12.626307] [gradient_penalty: 0.010499]\n",
            "[Epoch 1/200] [Batch 1754/3166] [D loss: 0.393984] [G loss: 11.856524] [gradient_penalty: 0.010306]\n",
            "[Epoch 1/200] [Batch 1755/3166] [D loss: 2.187100] [G loss: 12.935292] [gradient_penalty: 0.010900]\n",
            "[Epoch 1/200] [Batch 1756/3166] [D loss: 2.126122] [G loss: 11.881905] [gradient_penalty: 0.008348]\n",
            "[Epoch 1/200] [Batch 1757/3166] [D loss: 1.452838] [G loss: 11.677708] [gradient_penalty: 0.008696]\n",
            "[Epoch 1/200] [Batch 1758/3166] [D loss: 1.037305] [G loss: 13.186926] [gradient_penalty: 0.010563]\n",
            "[Epoch 1/200] [Batch 1759/3166] [D loss: 1.623993] [G loss: 12.662306] [gradient_penalty: 0.011195]\n",
            "[Epoch 1/200] [Batch 1760/3166] [D loss: 1.573548] [G loss: 12.925776] [gradient_penalty: 0.008653]\n",
            "[Epoch 1/200] [Batch 1761/3166] [D loss: 0.519368] [G loss: 12.442473] [gradient_penalty: 0.010173]\n",
            "[Epoch 1/200] [Batch 1762/3166] [D loss: 0.796286] [G loss: 12.663666] [gradient_penalty: 0.011550]\n",
            "[Epoch 1/200] [Batch 1763/3166] [D loss: -0.173431] [G loss: 13.025085] [gradient_penalty: 0.012096]\n",
            "[Epoch 1/200] [Batch 1764/3166] [D loss: 0.278309] [G loss: 12.879119] [gradient_penalty: 0.009854]\n",
            "[Epoch 1/200] [Batch 1765/3166] [D loss: 1.278195] [G loss: 13.118950] [gradient_penalty: 0.009134]\n",
            "[Epoch 1/200] [Batch 1766/3166] [D loss: 1.515764] [G loss: 12.937414] [gradient_penalty: 0.013139]\n",
            "[Epoch 1/200] [Batch 1767/3166] [D loss: 1.346640] [G loss: 13.672210] [gradient_penalty: 0.014908]\n",
            "[Epoch 1/200] [Batch 1768/3166] [D loss: -1.584868] [G loss: 13.742434] [gradient_penalty: 0.010680]\n",
            "[Epoch 1/200] [Batch 1769/3166] [D loss: 2.407785] [G loss: 12.855833] [gradient_penalty: 0.010869]\n",
            "[Epoch 1/200] [Batch 1770/3166] [D loss: 1.215481] [G loss: 13.002757] [gradient_penalty: 0.009883]\n",
            "[Epoch 1/200] [Batch 1771/3166] [D loss: 0.471197] [G loss: 12.259262] [gradient_penalty: 0.013309]\n",
            "[Epoch 1/200] [Batch 1772/3166] [D loss: 0.399655] [G loss: 12.987680] [gradient_penalty: 0.013542]\n",
            "[Epoch 1/200] [Batch 1773/3166] [D loss: 1.623291] [G loss: 12.557446] [gradient_penalty: 0.010335]\n",
            "[Epoch 1/200] [Batch 1774/3166] [D loss: 2.191105] [G loss: 11.863159] [gradient_penalty: 0.009586]\n",
            "[Epoch 1/200] [Batch 1775/3166] [D loss: 0.703571] [G loss: 12.121982] [gradient_penalty: 0.008768]\n",
            "[Epoch 1/200] [Batch 1776/3166] [D loss: 1.662224] [G loss: 13.048357] [gradient_penalty: 0.009070]\n",
            "[Epoch 1/200] [Batch 1777/3166] [D loss: 0.840523] [G loss: 12.382048] [gradient_penalty: 0.009146]\n",
            "[Epoch 1/200] [Batch 1778/3166] [D loss: 1.703570] [G loss: 13.302281] [gradient_penalty: 0.009536]\n",
            "[Epoch 1/200] [Batch 1779/3166] [D loss: 0.515670] [G loss: 11.688301] [gradient_penalty: 0.008720]\n",
            "[Epoch 1/200] [Batch 1780/3166] [D loss: 0.558199] [G loss: 11.835617] [gradient_penalty: 0.011171]\n",
            "[Epoch 1/200] [Batch 1781/3166] [D loss: -0.166510] [G loss: 12.256360] [gradient_penalty: 0.015049]\n",
            "[Epoch 1/200] [Batch 1782/3166] [D loss: 0.638095] [G loss: 12.693462] [gradient_penalty: 0.009597]\n",
            "[Epoch 1/200] [Batch 1783/3166] [D loss: 0.325079] [G loss: 12.841618] [gradient_penalty: 0.010152]\n",
            "[Epoch 1/200] [Batch 1784/3166] [D loss: 0.588518] [G loss: 12.160164] [gradient_penalty: 0.009832]\n",
            "[Epoch 1/200] [Batch 1785/3166] [D loss: 0.919308] [G loss: 12.356936] [gradient_penalty: 0.007743]\n",
            "[Epoch 1/200] [Batch 1786/3166] [D loss: 1.122425] [G loss: 11.898746] [gradient_penalty: 0.009862]\n",
            "[Epoch 1/200] [Batch 1787/3166] [D loss: 0.752434] [G loss: 12.432149] [gradient_penalty: 0.010176]\n",
            "[Epoch 1/200] [Batch 1788/3166] [D loss: -0.478118] [G loss: 11.169226] [gradient_penalty: 0.008653]\n",
            "[Epoch 1/200] [Batch 1789/3166] [D loss: 1.202130] [G loss: 12.441195] [gradient_penalty: 0.010639]\n",
            "[Epoch 1/200] [Batch 1790/3166] [D loss: 1.670898] [G loss: 12.463366] [gradient_penalty: 0.008560]\n",
            "[Epoch 1/200] [Batch 1791/3166] [D loss: 1.389597] [G loss: 12.289883] [gradient_penalty: 0.008046]\n",
            "[Epoch 1/200] [Batch 1792/3166] [D loss: 0.533436] [G loss: 11.948771] [gradient_penalty: 0.009695]\n",
            "[Epoch 1/200] [Batch 1793/3166] [D loss: 0.572261] [G loss: 11.892664] [gradient_penalty: 0.010524]\n",
            "[Epoch 1/200] [Batch 1794/3166] [D loss: 0.813638] [G loss: 11.059767] [gradient_penalty: 0.009907]\n",
            "[Epoch 1/200] [Batch 1795/3166] [D loss: 1.743223] [G loss: 11.510132] [gradient_penalty: 0.009952]\n",
            "[Epoch 1/200] [Batch 1796/3166] [D loss: -0.058349] [G loss: 11.540056] [gradient_penalty: 0.008063]\n",
            "[Epoch 1/200] [Batch 1797/3166] [D loss: 0.543297] [G loss: 12.018724] [gradient_penalty: 0.010365]\n",
            "[Epoch 1/200] [Batch 1798/3166] [D loss: 0.994493] [G loss: 11.676247] [gradient_penalty: 0.009495]\n",
            "[Epoch 1/200] [Batch 1799/3166] [D loss: -0.079523] [G loss: 11.815586] [gradient_penalty: 0.008283]\n",
            "[Epoch 1/200] [Batch 1800/3166] [D loss: 1.461659] [G loss: 12.649992] [gradient_penalty: 0.008745]\n",
            "[Epoch 1/200] [Batch 1801/3166] [D loss: 0.270074] [G loss: 12.938792] [gradient_penalty: 0.011751]\n",
            "[Epoch 1/200] [Batch 1802/3166] [D loss: 0.267038] [G loss: 12.455568] [gradient_penalty: 0.010265]\n",
            "[Epoch 1/200] [Batch 1803/3166] [D loss: -0.639427] [G loss: 12.196956] [gradient_penalty: 0.007558]\n",
            "[Epoch 1/200] [Batch 1804/3166] [D loss: 0.007563] [G loss: 12.993472] [gradient_penalty: 0.014221]\n",
            "[Epoch 1/200] [Batch 1805/3166] [D loss: 0.219061] [G loss: 12.396254] [gradient_penalty: 0.011294]\n",
            "[Epoch 1/200] [Batch 1806/3166] [D loss: -0.417717] [G loss: 13.237963] [gradient_penalty: 0.010219]\n",
            "[Epoch 1/200] [Batch 1807/3166] [D loss: 1.589643] [G loss: 11.791074] [gradient_penalty: 0.008728]\n",
            "[Epoch 1/200] [Batch 1808/3166] [D loss: 1.688812] [G loss: 12.698572] [gradient_penalty: 0.023181]\n",
            "[Epoch 1/200] [Batch 1809/3166] [D loss: 1.886194] [G loss: 12.814033] [gradient_penalty: 0.009144]\n",
            "[Epoch 1/200] [Batch 1810/3166] [D loss: 0.192763] [G loss: 12.105206] [gradient_penalty: 0.010354]\n",
            "[Epoch 1/200] [Batch 1811/3166] [D loss: 1.484283] [G loss: 12.856585] [gradient_penalty: 0.011135]\n",
            "[Epoch 1/200] [Batch 1812/3166] [D loss: 0.198125] [G loss: 12.329193] [gradient_penalty: 0.012611]\n",
            "[Epoch 1/200] [Batch 1813/3166] [D loss: 0.075297] [G loss: 12.235239] [gradient_penalty: 0.008979]\n",
            "[Epoch 1/200] [Batch 1814/3166] [D loss: 0.872450] [G loss: 11.939722] [gradient_penalty: 0.010490]\n",
            "[Epoch 1/200] [Batch 1815/3166] [D loss: 0.717212] [G loss: 12.329172] [gradient_penalty: 0.012363]\n",
            "[Epoch 1/200] [Batch 1816/3166] [D loss: 0.469429] [G loss: 12.115572] [gradient_penalty: 0.007353]\n",
            "[Epoch 1/200] [Batch 1817/3166] [D loss: 1.712804] [G loss: 11.708179] [gradient_penalty: 0.008471]\n",
            "[Epoch 1/200] [Batch 1818/3166] [D loss: 1.299502] [G loss: 12.592768] [gradient_penalty: 0.009107]\n",
            "[Epoch 1/200] [Batch 1819/3166] [D loss: -0.766845] [G loss: 11.995504] [gradient_penalty: 0.010084]\n",
            "[Epoch 1/200] [Batch 1820/3166] [D loss: 0.503747] [G loss: 11.142336] [gradient_penalty: 0.009653]\n",
            "[Epoch 1/200] [Batch 1821/3166] [D loss: 0.744129] [G loss: 12.496382] [gradient_penalty: 0.009891]\n",
            "[Epoch 1/200] [Batch 1822/3166] [D loss: 0.933199] [G loss: 12.084789] [gradient_penalty: 0.009608]\n",
            "[Epoch 1/200] [Batch 1823/3166] [D loss: -0.062820] [G loss: 11.546919] [gradient_penalty: 0.007821]\n",
            "[Epoch 1/200] [Batch 1824/3166] [D loss: 1.520536] [G loss: 11.522082] [gradient_penalty: 0.012011]\n",
            "[Epoch 1/200] [Batch 1825/3166] [D loss: 0.477481] [G loss: 11.646502] [gradient_penalty: 0.013195]\n",
            "[Epoch 1/200] [Batch 1826/3166] [D loss: -0.290905] [G loss: 11.317642] [gradient_penalty: 0.007901]\n",
            "[Epoch 1/200] [Batch 1827/3166] [D loss: 1.872150] [G loss: 10.841095] [gradient_penalty: 0.009615]\n",
            "[Epoch 1/200] [Batch 1828/3166] [D loss: 1.040113] [G loss: 11.114588] [gradient_penalty: 0.009928]\n",
            "[Epoch 1/200] [Batch 1829/3166] [D loss: 0.092269] [G loss: 10.943357] [gradient_penalty: 0.010675]\n",
            "[Epoch 1/200] [Batch 1830/3166] [D loss: 1.818546] [G loss: 11.144108] [gradient_penalty: 0.008616]\n",
            "[Epoch 1/200] [Batch 1831/3166] [D loss: 1.266351] [G loss: 11.771815] [gradient_penalty: 0.010022]\n",
            "[Epoch 1/200] [Batch 1832/3166] [D loss: 0.653447] [G loss: 11.898060] [gradient_penalty: 0.008332]\n",
            "[Epoch 1/200] [Batch 1833/3166] [D loss: 0.314558] [G loss: 11.199592] [gradient_penalty: 0.007317]\n",
            "[Epoch 1/200] [Batch 1834/3166] [D loss: 0.859885] [G loss: 11.738859] [gradient_penalty: 0.008880]\n",
            "log\n",
            "[Epoch 1/200] [Batch 1835/3166] [D loss: 0.201846] [G loss: 12.008650] [gradient_penalty: 0.010941]\n",
            "[Epoch 1/200] [Batch 1836/3166] [D loss: 0.994348] [G loss: 12.875122] [gradient_penalty: 0.010241]\n",
            "[Epoch 1/200] [Batch 1837/3166] [D loss: 0.865958] [G loss: 10.931166] [gradient_penalty: 0.009086]\n",
            "[Epoch 1/200] [Batch 1838/3166] [D loss: 1.016002] [G loss: 12.054129] [gradient_penalty: 0.010467]\n",
            "[Epoch 1/200] [Batch 1839/3166] [D loss: 0.599551] [G loss: 12.758087] [gradient_penalty: 0.009075]\n",
            "[Epoch 1/200] [Batch 1840/3166] [D loss: 0.177152] [G loss: 11.456991] [gradient_penalty: 0.012437]\n",
            "[Epoch 1/200] [Batch 1841/3166] [D loss: 2.034136] [G loss: 12.885798] [gradient_penalty: 0.009151]\n",
            "[Epoch 1/200] [Batch 1842/3166] [D loss: 0.592976] [G loss: 11.710324] [gradient_penalty: 0.010734]\n",
            "[Epoch 1/200] [Batch 1843/3166] [D loss: 0.477914] [G loss: 12.985915] [gradient_penalty: 0.009035]\n",
            "[Epoch 1/200] [Batch 1844/3166] [D loss: 0.424313] [G loss: 12.273783] [gradient_penalty: 0.009372]\n",
            "[Epoch 1/200] [Batch 1845/3166] [D loss: 0.474230] [G loss: 12.038555] [gradient_penalty: 0.009762]\n",
            "[Epoch 1/200] [Batch 1846/3166] [D loss: 0.725333] [G loss: 11.572726] [gradient_penalty: 0.009147]\n",
            "[Epoch 1/200] [Batch 1847/3166] [D loss: 0.646395] [G loss: 12.127176] [gradient_penalty: 0.009522]\n",
            "[Epoch 1/200] [Batch 1848/3166] [D loss: 0.696498] [G loss: 12.454645] [gradient_penalty: 0.008441]\n",
            "[Epoch 1/200] [Batch 1849/3166] [D loss: 0.967885] [G loss: 13.134445] [gradient_penalty: 0.011830]\n",
            "[Epoch 1/200] [Batch 1850/3166] [D loss: -0.695699] [G loss: 12.226632] [gradient_penalty: 0.011479]\n",
            "[Epoch 1/200] [Batch 1851/3166] [D loss: 0.822534] [G loss: 12.784852] [gradient_penalty: 0.015628]\n",
            "[Epoch 1/200] [Batch 1852/3166] [D loss: -0.326497] [G loss: 12.455530] [gradient_penalty: 0.007375]\n",
            "[Epoch 1/200] [Batch 1853/3166] [D loss: 1.246743] [G loss: 13.463424] [gradient_penalty: 0.010612]\n",
            "[Epoch 1/200] [Batch 1854/3166] [D loss: 0.327833] [G loss: 12.254881] [gradient_penalty: 0.010278]\n",
            "[Epoch 1/200] [Batch 1855/3166] [D loss: 0.709756] [G loss: 12.291920] [gradient_penalty: 0.011226]\n",
            "[Epoch 1/200] [Batch 1856/3166] [D loss: 0.806342] [G loss: 12.724266] [gradient_penalty: 0.009852]\n",
            "[Epoch 1/200] [Batch 1857/3166] [D loss: 1.762962] [G loss: 13.157337] [gradient_penalty: 0.012218]\n",
            "[Epoch 1/200] [Batch 1858/3166] [D loss: -0.611453] [G loss: 12.805593] [gradient_penalty: 0.011337]\n",
            "[Epoch 1/200] [Batch 1859/3166] [D loss: 1.050222] [G loss: 13.119112] [gradient_penalty: 0.011028]\n",
            "[Epoch 1/200] [Batch 1860/3166] [D loss: 0.559729] [G loss: 12.541897] [gradient_penalty: 0.009632]\n",
            "[Epoch 1/200] [Batch 1861/3166] [D loss: 0.815762] [G loss: 12.719238] [gradient_penalty: 0.009217]\n",
            "[Epoch 1/200] [Batch 1862/3166] [D loss: 0.946398] [G loss: 13.039999] [gradient_penalty: 0.013177]\n",
            "[Epoch 1/200] [Batch 1863/3166] [D loss: 0.404110] [G loss: 13.088762] [gradient_penalty: 0.011482]\n",
            "[Epoch 1/200] [Batch 1864/3166] [D loss: 1.054909] [G loss: 12.641640] [gradient_penalty: 0.010058]\n",
            "[Epoch 1/200] [Batch 1865/3166] [D loss: 0.134819] [G loss: 12.636747] [gradient_penalty: 0.009251]\n",
            "[Epoch 1/200] [Batch 1866/3166] [D loss: 0.654373] [G loss: 12.962324] [gradient_penalty: 0.009214]\n",
            "[Epoch 1/200] [Batch 1867/3166] [D loss: 0.752965] [G loss: 13.101690] [gradient_penalty: 0.009844]\n",
            "[Epoch 1/200] [Batch 1868/3166] [D loss: 0.604869] [G loss: 14.115993] [gradient_penalty: 0.008571]\n",
            "[Epoch 1/200] [Batch 1869/3166] [D loss: 0.787720] [G loss: 13.274736] [gradient_penalty: 0.007349]\n",
            "[Epoch 1/200] [Batch 1870/3166] [D loss: -0.133574] [G loss: 12.445805] [gradient_penalty: 0.008381]\n",
            "[Epoch 1/200] [Batch 1871/3166] [D loss: 0.356204] [G loss: 12.310829] [gradient_penalty: 0.009580]\n",
            "[Epoch 1/200] [Batch 1872/3166] [D loss: 0.891830] [G loss: 12.352697] [gradient_penalty: 0.012743]\n",
            "[Epoch 1/200] [Batch 1873/3166] [D loss: 0.215294] [G loss: 13.462185] [gradient_penalty: 0.009013]\n",
            "[Epoch 1/200] [Batch 1874/3166] [D loss: 0.369582] [G loss: 13.245509] [gradient_penalty: 0.008775]\n",
            "[Epoch 1/200] [Batch 1875/3166] [D loss: -0.480625] [G loss: 12.173103] [gradient_penalty: 0.008606]\n",
            "[Epoch 1/200] [Batch 1876/3166] [D loss: 0.055452] [G loss: 13.413438] [gradient_penalty: 0.013939]\n",
            "[Epoch 1/200] [Batch 1877/3166] [D loss: -0.204894] [G loss: 13.192472] [gradient_penalty: 0.006125]\n",
            "[Epoch 1/200] [Batch 1878/3166] [D loss: 0.273014] [G loss: 12.292314] [gradient_penalty: 0.008452]\n",
            "[Epoch 1/200] [Batch 1879/3166] [D loss: 0.925315] [G loss: 12.117914] [gradient_penalty: 0.011115]\n",
            "[Epoch 1/200] [Batch 1880/3166] [D loss: 1.084813] [G loss: 12.615288] [gradient_penalty: 0.008614]\n",
            "[Epoch 1/200] [Batch 1881/3166] [D loss: -0.645516] [G loss: 11.758910] [gradient_penalty: 0.009432]\n",
            "[Epoch 1/200] [Batch 1882/3166] [D loss: 0.663244] [G loss: 12.063038] [gradient_penalty: 0.009725]\n",
            "[Epoch 1/200] [Batch 1883/3166] [D loss: -0.524108] [G loss: 12.381098] [gradient_penalty: 0.010916]\n",
            "[Epoch 1/200] [Batch 1884/3166] [D loss: 1.096954] [G loss: 12.371590] [gradient_penalty: 0.007673]\n",
            "[Epoch 1/200] [Batch 1885/3166] [D loss: 0.157276] [G loss: 11.563863] [gradient_penalty: 0.011665]\n",
            "[Epoch 1/200] [Batch 1886/3166] [D loss: 0.344752] [G loss: 12.599941] [gradient_penalty: 0.009286]\n",
            "[Epoch 1/200] [Batch 1887/3166] [D loss: 0.461888] [G loss: 13.246726] [gradient_penalty: 0.008940]\n",
            "[Epoch 1/200] [Batch 1888/3166] [D loss: -0.097173] [G loss: 12.311586] [gradient_penalty: 0.008804]\n",
            "[Epoch 1/200] [Batch 1889/3166] [D loss: 1.225055] [G loss: 11.256773] [gradient_penalty: 0.007859]\n",
            "[Epoch 1/200] [Batch 1890/3166] [D loss: -0.997650] [G loss: 11.578973] [gradient_penalty: 0.011806]\n",
            "[Epoch 1/200] [Batch 1891/3166] [D loss: 0.922060] [G loss: 11.666344] [gradient_penalty: 0.012120]\n",
            "[Epoch 1/200] [Batch 1892/3166] [D loss: 0.770096] [G loss: 12.066378] [gradient_penalty: 0.013909]\n",
            "[Epoch 1/200] [Batch 1893/3166] [D loss: 1.021841] [G loss: 12.246564] [gradient_penalty: 0.008380]\n",
            "[Epoch 1/200] [Batch 1894/3166] [D loss: 1.237481] [G loss: 12.918715] [gradient_penalty: 0.009502]\n",
            "[Epoch 1/200] [Batch 1895/3166] [D loss: 1.052387] [G loss: 11.951869] [gradient_penalty: 0.008788]\n",
            "[Epoch 1/200] [Batch 1896/3166] [D loss: 0.696154] [G loss: 11.787347] [gradient_penalty: 0.009666]\n",
            "[Epoch 1/200] [Batch 1897/3166] [D loss: 0.668421] [G loss: 12.260090] [gradient_penalty: 0.010543]\n",
            "[Epoch 1/200] [Batch 1898/3166] [D loss: 2.012513] [G loss: 12.464407] [gradient_penalty: 0.008387]\n",
            "[Epoch 1/200] [Batch 1899/3166] [D loss: 0.818323] [G loss: 11.686483] [gradient_penalty: 0.012865]\n",
            "[Epoch 1/200] [Batch 1900/3166] [D loss: 1.319298] [G loss: 12.366282] [gradient_penalty: 0.012238]\n",
            "[Epoch 1/200] [Batch 1901/3166] [D loss: 0.724668] [G loss: 12.229666] [gradient_penalty: 0.007426]\n",
            "[Epoch 1/200] [Batch 1902/3166] [D loss: -0.507215] [G loss: 12.716204] [gradient_penalty: 0.009574]\n",
            "[Epoch 1/200] [Batch 1903/3166] [D loss: 0.484019] [G loss: 12.678639] [gradient_penalty: 0.009118]\n",
            "[Epoch 1/200] [Batch 1904/3166] [D loss: 1.470223] [G loss: 11.895758] [gradient_penalty: 0.009484]\n",
            "[Epoch 1/200] [Batch 1905/3166] [D loss: 0.977378] [G loss: 12.283804] [gradient_penalty: 0.008512]\n",
            "[Epoch 1/200] [Batch 1906/3166] [D loss: 0.285943] [G loss: 12.882141] [gradient_penalty: 0.006517]\n",
            "[Epoch 1/200] [Batch 1907/3166] [D loss: -0.450893] [G loss: 13.018639] [gradient_penalty: 0.010354]\n",
            "[Epoch 1/200] [Batch 1908/3166] [D loss: 1.422455] [G loss: 12.945794] [gradient_penalty: 0.011422]\n",
            "[Epoch 1/200] [Batch 1909/3166] [D loss: 0.763548] [G loss: 13.347873] [gradient_penalty: 0.010014]\n",
            "[Epoch 1/200] [Batch 1910/3166] [D loss: 1.119039] [G loss: 12.697498] [gradient_penalty: 0.013181]\n",
            "[Epoch 1/200] [Batch 1911/3166] [D loss: 1.885686] [G loss: 11.603632] [gradient_penalty: 0.009079]\n",
            "[Epoch 1/200] [Batch 1912/3166] [D loss: 1.047178] [G loss: 12.244490] [gradient_penalty: 0.010646]\n",
            "[Epoch 1/200] [Batch 1913/3166] [D loss: 1.075575] [G loss: 11.587425] [gradient_penalty: 0.010974]\n",
            "[Epoch 1/200] [Batch 1914/3166] [D loss: -0.472658] [G loss: 11.903898] [gradient_penalty: 0.007952]\n",
            "[Epoch 1/200] [Batch 1915/3166] [D loss: 0.585781] [G loss: 12.028430] [gradient_penalty: 0.009350]\n",
            "[Epoch 1/200] [Batch 1916/3166] [D loss: -0.040425] [G loss: 12.897744] [gradient_penalty: 0.009571]\n",
            "[Epoch 1/200] [Batch 1917/3166] [D loss: 0.375333] [G loss: 12.007524] [gradient_penalty: 0.007236]\n",
            "[Epoch 1/200] [Batch 1918/3166] [D loss: 1.160295] [G loss: 12.144171] [gradient_penalty: 0.009806]\n",
            "[Epoch 1/200] [Batch 1919/3166] [D loss: 0.815136] [G loss: 12.724606] [gradient_penalty: 0.008382]\n",
            "[Epoch 1/200] [Batch 1920/3166] [D loss: 1.341456] [G loss: 12.271831] [gradient_penalty: 0.008121]\n",
            "[Epoch 1/200] [Batch 1921/3166] [D loss: 0.604645] [G loss: 13.330587] [gradient_penalty: 0.006964]\n",
            "[Epoch 1/200] [Batch 1922/3166] [D loss: 0.491529] [G loss: 13.163591] [gradient_penalty: 0.009382]\n",
            "[Epoch 1/200] [Batch 1923/3166] [D loss: 1.197577] [G loss: 13.133871] [gradient_penalty: 0.008460]\n",
            "[Epoch 1/200] [Batch 1924/3166] [D loss: -0.426030] [G loss: 12.951324] [gradient_penalty: 0.010300]\n",
            "[Epoch 1/200] [Batch 1925/3166] [D loss: 1.109538] [G loss: 13.181498] [gradient_penalty: 0.010663]\n",
            "[Epoch 1/200] [Batch 1926/3166] [D loss: 0.020649] [G loss: 13.309248] [gradient_penalty: 0.007848]\n",
            "[Epoch 1/200] [Batch 1927/3166] [D loss: 0.195427] [G loss: 12.943285] [gradient_penalty: 0.007071]\n",
            "[Epoch 1/200] [Batch 1928/3166] [D loss: 0.432361] [G loss: 12.711214] [gradient_penalty: 0.005758]\n",
            "[Epoch 1/200] [Batch 1929/3166] [D loss: -0.249341] [G loss: 12.628358] [gradient_penalty: 0.009246]\n",
            "[Epoch 1/200] [Batch 1930/3166] [D loss: 0.389565] [G loss: 12.085544] [gradient_penalty: 0.008890]\n",
            "[Epoch 1/200] [Batch 1931/3166] [D loss: 1.127044] [G loss: 13.192119] [gradient_penalty: 0.006458]\n",
            "[Epoch 1/200] [Batch 1932/3166] [D loss: 0.124228] [G loss: 12.212766] [gradient_penalty: 0.008499]\n",
            "[Epoch 1/200] [Batch 1933/3166] [D loss: 0.143593] [G loss: 13.678568] [gradient_penalty: 0.008580]\n",
            "[Epoch 1/200] [Batch 1934/3166] [D loss: 0.061181] [G loss: 12.355467] [gradient_penalty: 0.007347]\n",
            "log\n",
            "[Epoch 1/200] [Batch 1935/3166] [D loss: 0.630389] [G loss: 12.985156] [gradient_penalty: 0.008506]\n",
            "[Epoch 1/200] [Batch 1936/3166] [D loss: 0.368442] [G loss: 12.558691] [gradient_penalty: 0.009392]\n",
            "[Epoch 1/200] [Batch 1937/3166] [D loss: 0.893865] [G loss: 12.563898] [gradient_penalty: 0.009194]\n",
            "[Epoch 1/200] [Batch 1938/3166] [D loss: 0.415834] [G loss: 13.287012] [gradient_penalty: 0.009360]\n",
            "[Epoch 1/200] [Batch 1939/3166] [D loss: 1.086190] [G loss: 13.118034] [gradient_penalty: 0.009560]\n",
            "[Epoch 1/200] [Batch 1940/3166] [D loss: -0.556984] [G loss: 12.590768] [gradient_penalty: 0.011075]\n",
            "[Epoch 1/200] [Batch 1941/3166] [D loss: 0.340083] [G loss: 12.392717] [gradient_penalty: 0.010681]\n",
            "[Epoch 1/200] [Batch 1942/3166] [D loss: 0.758772] [G loss: 12.380287] [gradient_penalty: 0.012035]\n",
            "[Epoch 1/200] [Batch 1943/3166] [D loss: 0.626836] [G loss: 12.406389] [gradient_penalty: 0.008168]\n",
            "[Epoch 1/200] [Batch 1944/3166] [D loss: 1.920456] [G loss: 12.675331] [gradient_penalty: 0.010132]\n",
            "[Epoch 1/200] [Batch 1945/3166] [D loss: 2.398067] [G loss: 11.925438] [gradient_penalty: 0.009514]\n",
            "[Epoch 1/200] [Batch 1946/3166] [D loss: 0.224514] [G loss: 11.742588] [gradient_penalty: 0.009936]\n",
            "[Epoch 1/200] [Batch 1947/3166] [D loss: 0.370234] [G loss: 13.043589] [gradient_penalty: 0.008441]\n",
            "[Epoch 1/200] [Batch 1948/3166] [D loss: -0.108223] [G loss: 11.674976] [gradient_penalty: 0.007903]\n",
            "[Epoch 1/200] [Batch 1949/3166] [D loss: 0.250568] [G loss: 12.186383] [gradient_penalty: 0.007640]\n",
            "[Epoch 1/200] [Batch 1950/3166] [D loss: 0.520494] [G loss: 12.079756] [gradient_penalty: 0.008747]\n",
            "[Epoch 1/200] [Batch 1951/3166] [D loss: 0.269708] [G loss: 12.835407] [gradient_penalty: 0.008291]\n",
            "[Epoch 1/200] [Batch 1952/3166] [D loss: 0.336316] [G loss: 12.116315] [gradient_penalty: 0.008485]\n",
            "[Epoch 1/200] [Batch 1953/3166] [D loss: 0.057482] [G loss: 11.036986] [gradient_penalty: 0.005957]\n",
            "[Epoch 1/200] [Batch 1954/3166] [D loss: 0.916013] [G loss: 13.076637] [gradient_penalty: 0.009514]\n",
            "[Epoch 1/200] [Batch 1955/3166] [D loss: 0.690395] [G loss: 12.324830] [gradient_penalty: 0.009179]\n",
            "[Epoch 1/200] [Batch 1956/3166] [D loss: 2.031887] [G loss: 11.375517] [gradient_penalty: 0.014783]\n",
            "[Epoch 1/200] [Batch 1957/3166] [D loss: 1.013586] [G loss: 12.308072] [gradient_penalty: 0.009407]\n",
            "[Epoch 1/200] [Batch 1958/3166] [D loss: -0.027386] [G loss: 12.137879] [gradient_penalty: 0.010853]\n",
            "[Epoch 1/200] [Batch 1959/3166] [D loss: 0.684821] [G loss: 12.301426] [gradient_penalty: 0.008776]\n",
            "[Epoch 1/200] [Batch 1960/3166] [D loss: 0.325116] [G loss: 12.225315] [gradient_penalty: 0.009764]\n",
            "[Epoch 1/200] [Batch 1961/3166] [D loss: 1.343616] [G loss: 12.099257] [gradient_penalty: 0.006932]\n",
            "[Epoch 1/200] [Batch 1962/3166] [D loss: 0.240013] [G loss: 12.851860] [gradient_penalty: 0.007715]\n",
            "[Epoch 1/200] [Batch 1963/3166] [D loss: -0.152257] [G loss: 12.393064] [gradient_penalty: 0.007864]\n",
            "[Epoch 1/200] [Batch 1964/3166] [D loss: 1.111615] [G loss: 12.327345] [gradient_penalty: 0.011676]\n",
            "[Epoch 1/200] [Batch 1965/3166] [D loss: 0.911591] [G loss: 12.650188] [gradient_penalty: 0.012769]\n",
            "[Epoch 1/200] [Batch 1966/3166] [D loss: 1.079949] [G loss: 11.648465] [gradient_penalty: 0.005735]\n",
            "[Epoch 1/200] [Batch 1967/3166] [D loss: -0.139894] [G loss: 12.173093] [gradient_penalty: 0.011138]\n",
            "[Epoch 1/200] [Batch 1968/3166] [D loss: 0.964688] [G loss: 13.184663] [gradient_penalty: 0.009609]\n",
            "[Epoch 1/200] [Batch 1969/3166] [D loss: 0.863732] [G loss: 12.758620] [gradient_penalty: 0.008623]\n",
            "[Epoch 1/200] [Batch 1970/3166] [D loss: -0.145036] [G loss: 12.225145] [gradient_penalty: 0.008621]\n",
            "[Epoch 1/200] [Batch 1971/3166] [D loss: 0.757195] [G loss: 13.156372] [gradient_penalty: 0.008237]\n",
            "[Epoch 1/200] [Batch 1972/3166] [D loss: 0.904497] [G loss: 12.289026] [gradient_penalty: 0.009297]\n",
            "[Epoch 1/200] [Batch 1973/3166] [D loss: 1.697028] [G loss: 12.406021] [gradient_penalty: 0.007761]\n",
            "[Epoch 1/200] [Batch 1974/3166] [D loss: 0.219308] [G loss: 12.420137] [gradient_penalty: 0.010666]\n",
            "[Epoch 1/200] [Batch 1975/3166] [D loss: 1.356004] [G loss: 13.367265] [gradient_penalty: 0.013836]\n",
            "[Epoch 1/200] [Batch 1976/3166] [D loss: 1.872777] [G loss: 11.795378] [gradient_penalty: 0.008435]\n",
            "[Epoch 1/200] [Batch 1977/3166] [D loss: 0.655011] [G loss: 11.787725] [gradient_penalty: 0.008084]\n",
            "[Epoch 1/200] [Batch 1978/3166] [D loss: 0.198200] [G loss: 12.052399] [gradient_penalty: 0.012188]\n",
            "[Epoch 1/200] [Batch 1979/3166] [D loss: 0.318506] [G loss: 12.132616] [gradient_penalty: 0.008865]\n",
            "[Epoch 1/200] [Batch 1980/3166] [D loss: -0.010183] [G loss: 12.400225] [gradient_penalty: 0.007699]\n",
            "[Epoch 1/200] [Batch 1981/3166] [D loss: 0.050109] [G loss: 12.145323] [gradient_penalty: 0.007473]\n",
            "[Epoch 1/200] [Batch 1982/3166] [D loss: 1.406647] [G loss: 12.513082] [gradient_penalty: 0.009737]\n",
            "[Epoch 1/200] [Batch 1983/3166] [D loss: 0.514783] [G loss: 12.846470] [gradient_penalty: 0.011009]\n",
            "[Epoch 1/200] [Batch 1984/3166] [D loss: 0.218106] [G loss: 12.261845] [gradient_penalty: 0.010147]\n",
            "[Epoch 1/200] [Batch 1985/3166] [D loss: -0.305254] [G loss: 11.541864] [gradient_penalty: 0.009435]\n",
            "[Epoch 1/200] [Batch 1986/3166] [D loss: 0.227386] [G loss: 11.500656] [gradient_penalty: 0.008362]\n",
            "[Epoch 1/200] [Batch 1987/3166] [D loss: 0.504347] [G loss: 11.433996] [gradient_penalty: 0.009417]\n",
            "[Epoch 1/200] [Batch 1988/3166] [D loss: 0.567109] [G loss: 12.290468] [gradient_penalty: 0.009986]\n",
            "[Epoch 1/200] [Batch 1989/3166] [D loss: 0.512512] [G loss: 11.201180] [gradient_penalty: 0.010355]\n",
            "[Epoch 1/200] [Batch 1990/3166] [D loss: -0.586527] [G loss: 12.640070] [gradient_penalty: 0.008375]\n",
            "[Epoch 1/200] [Batch 1991/3166] [D loss: 0.727002] [G loss: 11.151818] [gradient_penalty: 0.007824]\n",
            "[Epoch 1/200] [Batch 1992/3166] [D loss: 1.143418] [G loss: 11.973075] [gradient_penalty: 0.007131]\n",
            "[Epoch 1/200] [Batch 1993/3166] [D loss: 1.905685] [G loss: 11.294651] [gradient_penalty: 0.009632]\n",
            "[Epoch 1/200] [Batch 1994/3166] [D loss: 1.613044] [G loss: 11.140387] [gradient_penalty: 0.008333]\n",
            "[Epoch 1/200] [Batch 1995/3166] [D loss: 0.857334] [G loss: 11.272169] [gradient_penalty: 0.009590]\n",
            "[Epoch 1/200] [Batch 1996/3166] [D loss: 0.512180] [G loss: 11.297404] [gradient_penalty: 0.010573]\n",
            "[Epoch 1/200] [Batch 1997/3166] [D loss: 0.483878] [G loss: 12.168488] [gradient_penalty: 0.010147]\n",
            "[Epoch 1/200] [Batch 1998/3166] [D loss: 0.078100] [G loss: 11.944489] [gradient_penalty: 0.007442]\n",
            "[Epoch 1/200] [Batch 1999/3166] [D loss: 0.801831] [G loss: 12.240412] [gradient_penalty: 0.006353]\n",
            "[Epoch 1/200] [Batch 2000/3166] [D loss: 0.627052] [G loss: 11.521683] [gradient_penalty: 0.007802]\n",
            "[Epoch 1/200] [Batch 2001/3166] [D loss: 0.533675] [G loss: 11.399883] [gradient_penalty: 0.005620]\n",
            "[Epoch 1/200] [Batch 2002/3166] [D loss: -0.166673] [G loss: 12.301377] [gradient_penalty: 0.007959]\n",
            "[Epoch 1/200] [Batch 2003/3166] [D loss: 0.590733] [G loss: 12.414481] [gradient_penalty: 0.007947]\n",
            "[Epoch 1/200] [Batch 2004/3166] [D loss: -0.242198] [G loss: 12.138437] [gradient_penalty: 0.007210]\n",
            "[Epoch 1/200] [Batch 2005/3166] [D loss: 0.200968] [G loss: 11.672382] [gradient_penalty: 0.009901]\n",
            "[Epoch 1/200] [Batch 2006/3166] [D loss: 0.367749] [G loss: 11.628970] [gradient_penalty: 0.009067]\n",
            "[Epoch 1/200] [Batch 2007/3166] [D loss: -0.379869] [G loss: 11.725292] [gradient_penalty: 0.007634]\n",
            "[Epoch 1/200] [Batch 2008/3166] [D loss: -1.045630] [G loss: 11.517567] [gradient_penalty: 0.006672]\n",
            "[Epoch 1/200] [Batch 2009/3166] [D loss: 0.315947] [G loss: 11.654152] [gradient_penalty: 0.011313]\n",
            "[Epoch 1/200] [Batch 2010/3166] [D loss: -0.517967] [G loss: 12.236732] [gradient_penalty: 0.008825]\n",
            "[Epoch 1/200] [Batch 2011/3166] [D loss: 1.221225] [G loss: 11.650957] [gradient_penalty: 0.010187]\n",
            "[Epoch 1/200] [Batch 2012/3166] [D loss: 0.442090] [G loss: 12.283049] [gradient_penalty: 0.007122]\n",
            "[Epoch 1/200] [Batch 2013/3166] [D loss: 0.658293] [G loss: 12.253922] [gradient_penalty: 0.009373]\n",
            "[Epoch 1/200] [Batch 2014/3166] [D loss: 0.117632] [G loss: 11.787453] [gradient_penalty: 0.010837]\n",
            "[Epoch 1/200] [Batch 2015/3166] [D loss: 1.303040] [G loss: 11.647261] [gradient_penalty: 0.010830]\n",
            "[Epoch 1/200] [Batch 2016/3166] [D loss: 1.092095] [G loss: 12.087839] [gradient_penalty: 0.010429]\n",
            "[Epoch 1/200] [Batch 2017/3166] [D loss: 0.052963] [G loss: 11.776561] [gradient_penalty: 0.009370]\n",
            "[Epoch 1/200] [Batch 2018/3166] [D loss: 0.140643] [G loss: 11.149805] [gradient_penalty: 0.010818]\n",
            "[Epoch 1/200] [Batch 2019/3166] [D loss: 0.774569] [G loss: 11.847606] [gradient_penalty: 0.009340]\n",
            "[Epoch 1/200] [Batch 2020/3166] [D loss: -0.026357] [G loss: 12.361294] [gradient_penalty: 0.009948]\n",
            "[Epoch 1/200] [Batch 2021/3166] [D loss: 0.670099] [G loss: 11.582783] [gradient_penalty: 0.010471]\n",
            "[Epoch 1/200] [Batch 2022/3166] [D loss: 1.034048] [G loss: 12.312367] [gradient_penalty: 0.007547]\n",
            "[Epoch 1/200] [Batch 2023/3166] [D loss: 1.585729] [G loss: 11.352320] [gradient_penalty: 0.009460]\n",
            "[Epoch 1/200] [Batch 2024/3166] [D loss: 1.681929] [G loss: 11.099880] [gradient_penalty: 0.008422]\n",
            "[Epoch 1/200] [Batch 2025/3166] [D loss: 0.869261] [G loss: 10.607311] [gradient_penalty: 0.008617]\n",
            "[Epoch 1/200] [Batch 2026/3166] [D loss: -0.014351] [G loss: 11.472379] [gradient_penalty: 0.007898]\n",
            "[Epoch 1/200] [Batch 2027/3166] [D loss: 0.001570] [G loss: 11.017731] [gradient_penalty: 0.008165]\n",
            "[Epoch 1/200] [Batch 2028/3166] [D loss: 1.245649] [G loss: 11.460157] [gradient_penalty: 0.009117]\n",
            "[Epoch 1/200] [Batch 2029/3166] [D loss: 0.893896] [G loss: 10.989120] [gradient_penalty: 0.010900]\n",
            "[Epoch 1/200] [Batch 2030/3166] [D loss: 1.307405] [G loss: 11.309037] [gradient_penalty: 0.009360]\n",
            "[Epoch 1/200] [Batch 2031/3166] [D loss: 1.196116] [G loss: 10.868050] [gradient_penalty: 0.007391]\n",
            "[Epoch 1/200] [Batch 2032/3166] [D loss: -0.305685] [G loss: 11.251252] [gradient_penalty: 0.008561]\n",
            "[Epoch 1/200] [Batch 2033/3166] [D loss: 1.173021] [G loss: 11.444378] [gradient_penalty: 0.009388]\n",
            "[Epoch 1/200] [Batch 2034/3166] [D loss: 1.208106] [G loss: 11.143210] [gradient_penalty: 0.008784]\n",
            "log\n",
            "[Epoch 1/200] [Batch 2035/3166] [D loss: 0.599433] [G loss: 10.769279] [gradient_penalty: 0.009905]\n",
            "[Epoch 1/200] [Batch 2036/3166] [D loss: 0.223742] [G loss: 11.418776] [gradient_penalty: 0.006002]\n",
            "[Epoch 1/200] [Batch 2037/3166] [D loss: 1.136503] [G loss: 11.679093] [gradient_penalty: 0.009512]\n",
            "[Epoch 1/200] [Batch 2038/3166] [D loss: 0.763999] [G loss: 11.701408] [gradient_penalty: 0.009885]\n",
            "[Epoch 1/200] [Batch 2039/3166] [D loss: 0.360511] [G loss: 12.194691] [gradient_penalty: 0.012079]\n",
            "[Epoch 1/200] [Batch 2040/3166] [D loss: 1.381940] [G loss: 10.711075] [gradient_penalty: 0.011078]\n",
            "[Epoch 1/200] [Batch 2041/3166] [D loss: 1.762363] [G loss: 11.342747] [gradient_penalty: 0.009176]\n",
            "[Epoch 1/200] [Batch 2042/3166] [D loss: -0.333739] [G loss: 10.520843] [gradient_penalty: 0.008277]\n",
            "[Epoch 1/200] [Batch 2043/3166] [D loss: 0.050656] [G loss: 11.742758] [gradient_penalty: 0.010359]\n",
            "[Epoch 1/200] [Batch 2044/3166] [D loss: 1.017095] [G loss: 10.562955] [gradient_penalty: 0.009318]\n",
            "[Epoch 1/200] [Batch 2045/3166] [D loss: -0.346283] [G loss: 10.281948] [gradient_penalty: 0.007122]\n",
            "[Epoch 1/200] [Batch 2046/3166] [D loss: 0.188085] [G loss: 11.144135] [gradient_penalty: 0.006290]\n",
            "[Epoch 1/200] [Batch 2047/3166] [D loss: 0.593571] [G loss: 10.623159] [gradient_penalty: 0.010682]\n",
            "[Epoch 1/200] [Batch 2048/3166] [D loss: -0.696869] [G loss: 10.585661] [gradient_penalty: 0.008268]\n",
            "[Epoch 1/200] [Batch 2049/3166] [D loss: -0.526437] [G loss: 10.769047] [gradient_penalty: 0.006612]\n",
            "[Epoch 1/200] [Batch 2050/3166] [D loss: 0.713398] [G loss: 9.915348] [gradient_penalty: 0.011711]\n",
            "[Epoch 1/200] [Batch 2051/3166] [D loss: 1.204365] [G loss: 10.775657] [gradient_penalty: 0.009754]\n",
            "[Epoch 1/200] [Batch 2052/3166] [D loss: 1.403391] [G loss: 10.264393] [gradient_penalty: 0.009917]\n",
            "[Epoch 1/200] [Batch 2053/3166] [D loss: 0.441057] [G loss: 10.810425] [gradient_penalty: 0.009714]\n",
            "[Epoch 1/200] [Batch 2054/3166] [D loss: -1.179597] [G loss: 11.415347] [gradient_penalty: 0.010865]\n",
            "[Epoch 1/200] [Batch 2055/3166] [D loss: 0.331232] [G loss: 11.243718] [gradient_penalty: 0.009743]\n",
            "[Epoch 1/200] [Batch 2056/3166] [D loss: 0.905002] [G loss: 11.180307] [gradient_penalty: 0.007643]\n",
            "[Epoch 1/200] [Batch 2057/3166] [D loss: 0.607966] [G loss: 11.468891] [gradient_penalty: 0.011282]\n",
            "[Epoch 1/200] [Batch 2058/3166] [D loss: 1.311149] [G loss: 10.225085] [gradient_penalty: 0.007084]\n",
            "[Epoch 1/200] [Batch 2059/3166] [D loss: 0.591359] [G loss: 10.564074] [gradient_penalty: 0.008901]\n",
            "[Epoch 1/200] [Batch 2060/3166] [D loss: 0.170162] [G loss: 10.433485] [gradient_penalty: 0.008548]\n",
            "[Epoch 1/200] [Batch 2061/3166] [D loss: 0.642921] [G loss: 10.140587] [gradient_penalty: 0.012223]\n",
            "[Epoch 1/200] [Batch 2062/3166] [D loss: 0.335029] [G loss: 10.898706] [gradient_penalty: 0.006474]\n",
            "[Epoch 1/200] [Batch 2063/3166] [D loss: 1.009361] [G loss: 10.764712] [gradient_penalty: 0.007863]\n",
            "[Epoch 1/200] [Batch 2064/3166] [D loss: 0.428600] [G loss: 10.502761] [gradient_penalty: 0.010789]\n",
            "[Epoch 1/200] [Batch 2065/3166] [D loss: 0.413340] [G loss: 10.274305] [gradient_penalty: 0.007504]\n",
            "[Epoch 1/200] [Batch 2066/3166] [D loss: -0.369535] [G loss: 10.781439] [gradient_penalty: 0.008466]\n",
            "[Epoch 1/200] [Batch 2067/3166] [D loss: 0.311560] [G loss: 10.707832] [gradient_penalty: 0.007037]\n",
            "[Epoch 1/200] [Batch 2068/3166] [D loss: 0.795717] [G loss: 9.879593] [gradient_penalty: 0.008585]\n",
            "[Epoch 1/200] [Batch 2069/3166] [D loss: -0.652928] [G loss: 11.777649] [gradient_penalty: 0.005735]\n",
            "[Epoch 1/200] [Batch 2070/3166] [D loss: 1.860012] [G loss: 10.537848] [gradient_penalty: 0.006546]\n",
            "[Epoch 1/200] [Batch 2071/3166] [D loss: 0.495684] [G loss: 10.648963] [gradient_penalty: 0.008550]\n",
            "[Epoch 1/200] [Batch 2072/3166] [D loss: 0.238903] [G loss: 10.077560] [gradient_penalty: 0.007442]\n",
            "[Epoch 1/200] [Batch 2073/3166] [D loss: 1.039045] [G loss: 11.390852] [gradient_penalty: 0.007879]\n",
            "[Epoch 1/200] [Batch 2074/3166] [D loss: 0.353612] [G loss: 11.320048] [gradient_penalty: 0.005857]\n",
            "[Epoch 1/200] [Batch 2075/3166] [D loss: 1.771606] [G loss: 10.711333] [gradient_penalty: 0.010927]\n",
            "[Epoch 1/200] [Batch 2076/3166] [D loss: 0.322128] [G loss: 11.302986] [gradient_penalty: 0.009424]\n",
            "[Epoch 1/200] [Batch 2077/3166] [D loss: 0.464450] [G loss: 9.692924] [gradient_penalty: 0.007566]\n",
            "[Epoch 1/200] [Batch 2078/3166] [D loss: 0.712042] [G loss: 10.245035] [gradient_penalty: 0.007733]\n",
            "[Epoch 1/200] [Batch 2079/3166] [D loss: 1.079098] [G loss: 9.781144] [gradient_penalty: 0.008497]\n",
            "[Epoch 1/200] [Batch 2080/3166] [D loss: 1.108588] [G loss: 10.515644] [gradient_penalty: 0.007785]\n",
            "[Epoch 1/200] [Batch 2081/3166] [D loss: -0.268197] [G loss: 10.052794] [gradient_penalty: 0.008649]\n",
            "[Epoch 1/200] [Batch 2082/3166] [D loss: 0.452582] [G loss: 11.217705] [gradient_penalty: 0.007730]\n",
            "[Epoch 1/200] [Batch 2083/3166] [D loss: 0.095189] [G loss: 10.624193] [gradient_penalty: 0.009271]\n",
            "[Epoch 1/200] [Batch 2084/3166] [D loss: 0.482917] [G loss: 11.330990] [gradient_penalty: 0.006743]\n",
            "[Epoch 1/200] [Batch 2085/3166] [D loss: -0.507432] [G loss: 10.228168] [gradient_penalty: 0.008476]\n",
            "[Epoch 1/200] [Batch 2086/3166] [D loss: 0.486436] [G loss: 10.889307] [gradient_penalty: 0.006173]\n",
            "[Epoch 1/200] [Batch 2087/3166] [D loss: 1.190443] [G loss: 10.843300] [gradient_penalty: 0.009634]\n",
            "[Epoch 1/200] [Batch 2088/3166] [D loss: 0.391509] [G loss: 9.364376] [gradient_penalty: 0.008389]\n",
            "[Epoch 1/200] [Batch 2089/3166] [D loss: 0.723748] [G loss: 10.431628] [gradient_penalty: 0.007599]\n",
            "[Epoch 1/200] [Batch 2090/3166] [D loss: 1.966390] [G loss: 10.669334] [gradient_penalty: 0.011665]\n",
            "[Epoch 1/200] [Batch 2091/3166] [D loss: -0.086178] [G loss: 10.898733] [gradient_penalty: 0.005875]\n",
            "[Epoch 1/200] [Batch 2092/3166] [D loss: -0.175617] [G loss: 10.230749] [gradient_penalty: 0.008708]\n",
            "[Epoch 1/200] [Batch 2093/3166] [D loss: 0.482502] [G loss: 10.851889] [gradient_penalty: 0.009305]\n",
            "[Epoch 1/200] [Batch 2094/3166] [D loss: 1.226794] [G loss: 10.341638] [gradient_penalty: 0.007483]\n",
            "[Epoch 1/200] [Batch 2095/3166] [D loss: 0.076331] [G loss: 10.630955] [gradient_penalty: 0.008021]\n",
            "[Epoch 1/200] [Batch 2096/3166] [D loss: 0.122806] [G loss: 10.493610] [gradient_penalty: 0.007909]\n",
            "[Epoch 1/200] [Batch 2097/3166] [D loss: 0.538675] [G loss: 9.867594] [gradient_penalty: 0.008084]\n",
            "[Epoch 1/200] [Batch 2098/3166] [D loss: 0.479331] [G loss: 10.020945] [gradient_penalty: 0.007406]\n",
            "[Epoch 1/200] [Batch 2099/3166] [D loss: 0.353299] [G loss: 9.924565] [gradient_penalty: 0.008934]\n",
            "[Epoch 1/200] [Batch 2100/3166] [D loss: 0.937826] [G loss: 10.578648] [gradient_penalty: 0.007421]\n",
            "[Epoch 1/200] [Batch 2101/3166] [D loss: 0.773344] [G loss: 11.113761] [gradient_penalty: 0.008331]\n",
            "[Epoch 1/200] [Batch 2102/3166] [D loss: 0.701762] [G loss: 10.747886] [gradient_penalty: 0.007558]\n",
            "[Epoch 1/200] [Batch 2103/3166] [D loss: 1.072483] [G loss: 10.032373] [gradient_penalty: 0.009645]\n",
            "[Epoch 1/200] [Batch 2104/3166] [D loss: 0.958641] [G loss: 10.844213] [gradient_penalty: 0.007420]\n",
            "[Epoch 1/200] [Batch 2105/3166] [D loss: 0.138258] [G loss: 10.268810] [gradient_penalty: 0.011387]\n",
            "[Epoch 1/200] [Batch 2106/3166] [D loss: 0.553446] [G loss: 10.633042] [gradient_penalty: 0.006838]\n",
            "[Epoch 1/200] [Batch 2107/3166] [D loss: -0.132454] [G loss: 10.887653] [gradient_penalty: 0.007596]\n",
            "[Epoch 1/200] [Batch 2108/3166] [D loss: 0.044002] [G loss: 10.646704] [gradient_penalty: 0.009664]\n",
            "[Epoch 1/200] [Batch 2109/3166] [D loss: 1.134339] [G loss: 11.539008] [gradient_penalty: 0.009960]\n",
            "[Epoch 1/200] [Batch 2110/3166] [D loss: -0.359194] [G loss: 11.186898] [gradient_penalty: 0.010175]\n",
            "[Epoch 1/200] [Batch 2111/3166] [D loss: 0.153291] [G loss: 11.722982] [gradient_penalty: 0.010703]\n",
            "[Epoch 1/200] [Batch 2112/3166] [D loss: 0.807627] [G loss: 11.198337] [gradient_penalty: 0.011182]\n",
            "[Epoch 1/200] [Batch 2113/3166] [D loss: -0.311537] [G loss: 11.192464] [gradient_penalty: 0.011030]\n",
            "[Epoch 1/200] [Batch 2114/3166] [D loss: 0.161725] [G loss: 11.808702] [gradient_penalty: 0.007983]\n",
            "[Epoch 1/200] [Batch 2115/3166] [D loss: 1.208626] [G loss: 11.176310] [gradient_penalty: 0.009937]\n",
            "[Epoch 1/200] [Batch 2116/3166] [D loss: 1.277228] [G loss: 11.045898] [gradient_penalty: 0.010865]\n",
            "[Epoch 1/200] [Batch 2117/3166] [D loss: 0.849948] [G loss: 10.116775] [gradient_penalty: 0.007860]\n",
            "[Epoch 1/200] [Batch 2118/3166] [D loss: 0.568662] [G loss: 10.847172] [gradient_penalty: 0.005996]\n",
            "[Epoch 1/200] [Batch 2119/3166] [D loss: 0.400027] [G loss: 10.019242] [gradient_penalty: 0.009854]\n",
            "[Epoch 1/200] [Batch 2120/3166] [D loss: -0.180719] [G loss: 11.434916] [gradient_penalty: 0.010741]\n",
            "[Epoch 1/200] [Batch 2121/3166] [D loss: 0.379390] [G loss: 9.962725] [gradient_penalty: 0.007174]\n",
            "[Epoch 1/200] [Batch 2122/3166] [D loss: 0.585707] [G loss: 11.737684] [gradient_penalty: 0.005620]\n",
            "[Epoch 1/200] [Batch 2123/3166] [D loss: 0.669910] [G loss: 9.775824] [gradient_penalty: 0.009998]\n",
            "[Epoch 1/200] [Batch 2124/3166] [D loss: -0.511301] [G loss: 10.765050] [gradient_penalty: 0.009664]\n",
            "[Epoch 1/200] [Batch 2125/3166] [D loss: 1.232824] [G loss: 9.910521] [gradient_penalty: 0.006218]\n",
            "[Epoch 1/200] [Batch 2126/3166] [D loss: 0.677411] [G loss: 11.237492] [gradient_penalty: 0.006839]\n",
            "[Epoch 1/200] [Batch 2127/3166] [D loss: 0.890300] [G loss: 10.367721] [gradient_penalty: 0.006520]\n",
            "[Epoch 1/200] [Batch 2128/3166] [D loss: 0.823860] [G loss: 10.353174] [gradient_penalty: 0.007525]\n",
            "[Epoch 1/200] [Batch 2129/3166] [D loss: 1.022057] [G loss: 10.028009] [gradient_penalty: 0.006586]\n",
            "[Epoch 1/200] [Batch 2130/3166] [D loss: 0.373712] [G loss: 9.397922] [gradient_penalty: 0.010480]\n",
            "[Epoch 1/200] [Batch 2131/3166] [D loss: 0.987342] [G loss: 10.345043] [gradient_penalty: 0.008912]\n",
            "[Epoch 1/200] [Batch 2132/3166] [D loss: 1.240413] [G loss: 9.630541] [gradient_penalty: 0.006540]\n",
            "[Epoch 1/200] [Batch 2133/3166] [D loss: 0.728297] [G loss: 9.621130] [gradient_penalty: 0.008296]\n",
            "[Epoch 1/200] [Batch 2134/3166] [D loss: -0.068791] [G loss: 10.265135] [gradient_penalty: 0.008743]\n",
            "log\n",
            "[Epoch 1/200] [Batch 2135/3166] [D loss: -1.178672] [G loss: 10.302956] [gradient_penalty: 0.010316]\n",
            "[Epoch 1/200] [Batch 2136/3166] [D loss: 0.505843] [G loss: 11.082802] [gradient_penalty: 0.007911]\n",
            "[Epoch 1/200] [Batch 2137/3166] [D loss: 0.685067] [G loss: 10.617455] [gradient_penalty: 0.007891]\n",
            "[Epoch 1/200] [Batch 2138/3166] [D loss: 0.608348] [G loss: 10.954404] [gradient_penalty: 0.007671]\n",
            "[Epoch 1/200] [Batch 2139/3166] [D loss: -0.113955] [G loss: 10.610151] [gradient_penalty: 0.008426]\n",
            "[Epoch 1/200] [Batch 2140/3166] [D loss: 0.552037] [G loss: 11.559855] [gradient_penalty: 0.010429]\n",
            "[Epoch 1/200] [Batch 2141/3166] [D loss: 0.030915] [G loss: 11.551365] [gradient_penalty: 0.010501]\n",
            "[Epoch 1/200] [Batch 2142/3166] [D loss: 0.798494] [G loss: 11.692019] [gradient_penalty: 0.009154]\n",
            "[Epoch 1/200] [Batch 2143/3166] [D loss: 1.293085] [G loss: 10.962769] [gradient_penalty: 0.010525]\n",
            "[Epoch 1/200] [Batch 2144/3166] [D loss: 1.145851] [G loss: 11.603322] [gradient_penalty: 0.011873]\n",
            "[Epoch 1/200] [Batch 2145/3166] [D loss: 0.243917] [G loss: 11.154240] [gradient_penalty: 0.009183]\n",
            "[Epoch 1/200] [Batch 2146/3166] [D loss: 1.396396] [G loss: 11.220854] [gradient_penalty: 0.008870]\n",
            "[Epoch 1/200] [Batch 2147/3166] [D loss: 0.498810] [G loss: 11.220044] [gradient_penalty: 0.010617]\n",
            "[Epoch 1/200] [Batch 2148/3166] [D loss: 0.111826] [G loss: 11.094315] [gradient_penalty: 0.009520]\n",
            "[Epoch 1/200] [Batch 2149/3166] [D loss: 0.794903] [G loss: 11.272331] [gradient_penalty: 0.006831]\n",
            "[Epoch 1/200] [Batch 2150/3166] [D loss: 0.144777] [G loss: 11.203596] [gradient_penalty: 0.007061]\n",
            "[Epoch 1/200] [Batch 2151/3166] [D loss: 0.968040] [G loss: 10.962737] [gradient_penalty: 0.009582]\n",
            "[Epoch 1/200] [Batch 2152/3166] [D loss: 1.131802] [G loss: 11.000153] [gradient_penalty: 0.005984]\n",
            "[Epoch 1/200] [Batch 2153/3166] [D loss: 0.507578] [G loss: 9.826597] [gradient_penalty: 0.009591]\n",
            "[Epoch 1/200] [Batch 2154/3166] [D loss: 1.734162] [G loss: 10.990131] [gradient_penalty: 0.009193]\n",
            "[Epoch 1/200] [Batch 2155/3166] [D loss: -0.435360] [G loss: 10.691139] [gradient_penalty: 0.009586]\n",
            "[Epoch 1/200] [Batch 2156/3166] [D loss: 0.734117] [G loss: 10.965357] [gradient_penalty: 0.008189]\n",
            "[Epoch 1/200] [Batch 2157/3166] [D loss: 0.099183] [G loss: 10.021502] [gradient_penalty: 0.010209]\n",
            "[Epoch 1/200] [Batch 2158/3166] [D loss: 0.631656] [G loss: 10.992549] [gradient_penalty: 0.008107]\n",
            "[Epoch 1/200] [Batch 2159/3166] [D loss: 0.298261] [G loss: 10.040823] [gradient_penalty: 0.007855]\n",
            "[Epoch 1/200] [Batch 2160/3166] [D loss: -0.735151] [G loss: 10.715691] [gradient_penalty: 0.010847]\n",
            "[Epoch 1/200] [Batch 2161/3166] [D loss: 0.911077] [G loss: 10.310766] [gradient_penalty: 0.008695]\n",
            "[Epoch 1/200] [Batch 2162/3166] [D loss: -0.365572] [G loss: 10.114206] [gradient_penalty: 0.008542]\n",
            "[Epoch 1/200] [Batch 2163/3166] [D loss: 0.123694] [G loss: 10.720658] [gradient_penalty: 0.007227]\n",
            "[Epoch 1/200] [Batch 2164/3166] [D loss: -0.343865] [G loss: 10.971191] [gradient_penalty: 0.009158]\n",
            "[Epoch 1/200] [Batch 2165/3166] [D loss: 0.290717] [G loss: 10.771538] [gradient_penalty: 0.013357]\n",
            "[Epoch 1/200] [Batch 2166/3166] [D loss: -0.583145] [G loss: 10.789567] [gradient_penalty: 0.007488]\n",
            "[Epoch 1/200] [Batch 2167/3166] [D loss: 0.273648] [G loss: 11.185911] [gradient_penalty: 0.010299]\n",
            "[Epoch 1/200] [Batch 2168/3166] [D loss: 0.922022] [G loss: 11.086605] [gradient_penalty: 0.009454]\n",
            "[Epoch 1/200] [Batch 2169/3166] [D loss: 1.110191] [G loss: 11.079044] [gradient_penalty: 0.010459]\n",
            "[Epoch 1/200] [Batch 2170/3166] [D loss: 0.085157] [G loss: 11.345929] [gradient_penalty: 0.007077]\n",
            "[Epoch 1/200] [Batch 2171/3166] [D loss: -0.360722] [G loss: 11.905842] [gradient_penalty: 0.007144]\n",
            "[Epoch 1/200] [Batch 2172/3166] [D loss: -0.330698] [G loss: 11.234141] [gradient_penalty: 0.010326]\n",
            "[Epoch 1/200] [Batch 2173/3166] [D loss: 0.091246] [G loss: 11.114778] [gradient_penalty: 0.007694]\n",
            "[Epoch 1/200] [Batch 2174/3166] [D loss: -0.766496] [G loss: 11.600208] [gradient_penalty: 0.008142]\n",
            "[Epoch 1/200] [Batch 2175/3166] [D loss: -0.095595] [G loss: 10.792465] [gradient_penalty: 0.006798]\n",
            "[Epoch 1/200] [Batch 2176/3166] [D loss: -0.417763] [G loss: 11.950102] [gradient_penalty: 0.011260]\n",
            "[Epoch 1/200] [Batch 2177/3166] [D loss: -0.057152] [G loss: 10.127193] [gradient_penalty: 0.008408]\n",
            "[Epoch 1/200] [Batch 2178/3166] [D loss: 0.227952] [G loss: 10.537831] [gradient_penalty: 0.007624]\n",
            "[Epoch 1/200] [Batch 2179/3166] [D loss: -0.317751] [G loss: 10.806503] [gradient_penalty: 0.006757]\n",
            "[Epoch 1/200] [Batch 2180/3166] [D loss: 1.085099] [G loss: 10.869883] [gradient_penalty: 0.010711]\n",
            "[Epoch 1/200] [Batch 2181/3166] [D loss: 0.503003] [G loss: 11.685596] [gradient_penalty: 0.007624]\n",
            "[Epoch 1/200] [Batch 2182/3166] [D loss: 0.695369] [G loss: 10.365501] [gradient_penalty: 0.009335]\n",
            "[Epoch 1/200] [Batch 2183/3166] [D loss: 1.631624] [G loss: 11.053829] [gradient_penalty: 0.009534]\n",
            "[Epoch 1/200] [Batch 2184/3166] [D loss: 0.313943] [G loss: 10.847574] [gradient_penalty: 0.010535]\n",
            "[Epoch 1/200] [Batch 2185/3166] [D loss: 0.753462] [G loss: 11.332662] [gradient_penalty: 0.010174]\n",
            "[Epoch 1/200] [Batch 2186/3166] [D loss: 0.719982] [G loss: 11.092457] [gradient_penalty: 0.009206]\n",
            "[Epoch 1/200] [Batch 2187/3166] [D loss: 0.586652] [G loss: 11.211788] [gradient_penalty: 0.007118]\n",
            "[Epoch 1/200] [Batch 2188/3166] [D loss: 1.071244] [G loss: 11.146798] [gradient_penalty: 0.010103]\n",
            "[Epoch 1/200] [Batch 2189/3166] [D loss: 0.876082] [G loss: 11.334333] [gradient_penalty: 0.006844]\n",
            "[Epoch 1/200] [Batch 2190/3166] [D loss: -0.133765] [G loss: 10.724531] [gradient_penalty: 0.007977]\n",
            "[Epoch 1/200] [Batch 2191/3166] [D loss: 0.518134] [G loss: 11.591923] [gradient_penalty: 0.007259]\n",
            "[Epoch 1/200] [Batch 2192/3166] [D loss: 0.032684] [G loss: 11.050517] [gradient_penalty: 0.007339]\n",
            "[Epoch 1/200] [Batch 2193/3166] [D loss: 1.138372] [G loss: 11.521890] [gradient_penalty: 0.007850]\n",
            "[Epoch 1/200] [Batch 2194/3166] [D loss: -0.658867] [G loss: 11.676600] [gradient_penalty: 0.006909]\n",
            "[Epoch 1/200] [Batch 2195/3166] [D loss: 0.712996] [G loss: 10.624604] [gradient_penalty: 0.009594]\n",
            "[Epoch 1/200] [Batch 2196/3166] [D loss: 0.824747] [G loss: 10.666262] [gradient_penalty: 0.011010]\n",
            "[Epoch 1/200] [Batch 2197/3166] [D loss: 1.498427] [G loss: 11.344798] [gradient_penalty: 0.006541]\n",
            "[Epoch 1/200] [Batch 2198/3166] [D loss: 1.605242] [G loss: 10.705268] [gradient_penalty: 0.010070]\n",
            "[Epoch 1/200] [Batch 2199/3166] [D loss: 0.690365] [G loss: 11.988373] [gradient_penalty: 0.008119]\n",
            "[Epoch 1/200] [Batch 2200/3166] [D loss: 1.085791] [G loss: 11.818676] [gradient_penalty: 0.011500]\n",
            "[Epoch 1/200] [Batch 2201/3166] [D loss: 0.060551] [G loss: 12.115617] [gradient_penalty: 0.006910]\n",
            "[Epoch 1/200] [Batch 2202/3166] [D loss: 0.162783] [G loss: 11.752218] [gradient_penalty: 0.009790]\n",
            "[Epoch 1/200] [Batch 2203/3166] [D loss: 0.351582] [G loss: 10.831926] [gradient_penalty: 0.008692]\n",
            "[Epoch 1/200] [Batch 2204/3166] [D loss: 0.837702] [G loss: 11.226500] [gradient_penalty: 0.011502]\n",
            "[Epoch 1/200] [Batch 2205/3166] [D loss: 0.348340] [G loss: 11.529627] [gradient_penalty: 0.007860]\n",
            "[Epoch 1/200] [Batch 2206/3166] [D loss: 0.379477] [G loss: 11.468778] [gradient_penalty: 0.008663]\n",
            "[Epoch 1/200] [Batch 2207/3166] [D loss: 0.091345] [G loss: 11.799089] [gradient_penalty: 0.007389]\n",
            "[Epoch 1/200] [Batch 2208/3166] [D loss: 0.598729] [G loss: 11.404102] [gradient_penalty: 0.007539]\n",
            "[Epoch 1/200] [Batch 2209/3166] [D loss: 0.122663] [G loss: 10.971077] [gradient_penalty: 0.007326]\n",
            "[Epoch 1/200] [Batch 2210/3166] [D loss: 1.245441] [G loss: 12.068797] [gradient_penalty: 0.006819]\n",
            "[Epoch 1/200] [Batch 2211/3166] [D loss: -0.090807] [G loss: 11.144155] [gradient_penalty: 0.008401]\n",
            "[Epoch 1/200] [Batch 2212/3166] [D loss: 0.518238] [G loss: 11.098803] [gradient_penalty: 0.008947]\n",
            "[Epoch 1/200] [Batch 2213/3166] [D loss: 0.410520] [G loss: 11.459703] [gradient_penalty: 0.008425]\n",
            "[Epoch 1/200] [Batch 2214/3166] [D loss: 1.481531] [G loss: 11.230446] [gradient_penalty: 0.007864]\n",
            "[Epoch 1/200] [Batch 2215/3166] [D loss: 0.786817] [G loss: 11.109125] [gradient_penalty: 0.006849]\n",
            "[Epoch 1/200] [Batch 2216/3166] [D loss: 0.624617] [G loss: 10.914599] [gradient_penalty: 0.006570]\n",
            "[Epoch 1/200] [Batch 2217/3166] [D loss: 0.711991] [G loss: 10.950689] [gradient_penalty: 0.006890]\n",
            "[Epoch 1/200] [Batch 2218/3166] [D loss: 0.598227] [G loss: 10.931049] [gradient_penalty: 0.007218]\n",
            "[Epoch 1/200] [Batch 2219/3166] [D loss: 0.428619] [G loss: 10.885382] [gradient_penalty: 0.009293]\n",
            "[Epoch 1/200] [Batch 2220/3166] [D loss: 0.214445] [G loss: 10.762366] [gradient_penalty: 0.007972]\n",
            "[Epoch 1/200] [Batch 2221/3166] [D loss: -0.319264] [G loss: 10.198256] [gradient_penalty: 0.008634]\n",
            "[Epoch 1/200] [Batch 2222/3166] [D loss: 1.343903] [G loss: 11.185007] [gradient_penalty: 0.009884]\n",
            "[Epoch 1/200] [Batch 2223/3166] [D loss: -0.049592] [G loss: 11.081175] [gradient_penalty: 0.008332]\n",
            "[Epoch 1/200] [Batch 2224/3166] [D loss: 0.523838] [G loss: 11.017160] [gradient_penalty: 0.007574]\n",
            "[Epoch 1/200] [Batch 2225/3166] [D loss: 0.891299] [G loss: 10.467752] [gradient_penalty: 0.008330]\n",
            "[Epoch 1/200] [Batch 2226/3166] [D loss: 0.594949] [G loss: 10.389521] [gradient_penalty: 0.008086]\n",
            "[Epoch 1/200] [Batch 2227/3166] [D loss: 0.395936] [G loss: 9.851583] [gradient_penalty: 0.006475]\n",
            "[Epoch 1/200] [Batch 2228/3166] [D loss: 0.073550] [G loss: 10.354568] [gradient_penalty: 0.006246]\n",
            "[Epoch 1/200] [Batch 2229/3166] [D loss: -0.273349] [G loss: 10.148384] [gradient_penalty: 0.008079]\n",
            "[Epoch 1/200] [Batch 2230/3166] [D loss: 0.445302] [G loss: 10.375999] [gradient_penalty: 0.008152]\n",
            "[Epoch 1/200] [Batch 2231/3166] [D loss: 1.169546] [G loss: 10.516341] [gradient_penalty: 0.010042]\n",
            "[Epoch 1/200] [Batch 2232/3166] [D loss: 0.985196] [G loss: 10.312367] [gradient_penalty: 0.006392]\n",
            "[Epoch 1/200] [Batch 2233/3166] [D loss: 0.698855] [G loss: 10.891909] [gradient_penalty: 0.009337]\n",
            "[Epoch 1/200] [Batch 2234/3166] [D loss: 0.137725] [G loss: 9.340105] [gradient_penalty: 0.006016]\n",
            "log\n",
            "[Epoch 1/200] [Batch 2235/3166] [D loss: 0.500202] [G loss: 10.699272] [gradient_penalty: 0.007529]\n",
            "[Epoch 1/200] [Batch 2236/3166] [D loss: 1.019645] [G loss: 10.728260] [gradient_penalty: 0.006548]\n",
            "[Epoch 1/200] [Batch 2237/3166] [D loss: -0.667769] [G loss: 9.804026] [gradient_penalty: 0.007005]\n",
            "[Epoch 1/200] [Batch 2238/3166] [D loss: 0.800869] [G loss: 10.203281] [gradient_penalty: 0.010297]\n",
            "[Epoch 1/200] [Batch 2239/3166] [D loss: 1.152116] [G loss: 10.427523] [gradient_penalty: 0.009584]\n",
            "[Epoch 1/200] [Batch 2240/3166] [D loss: 0.302474] [G loss: 10.163454] [gradient_penalty: 0.009573]\n",
            "[Epoch 1/200] [Batch 2241/3166] [D loss: 0.821677] [G loss: 11.140454] [gradient_penalty: 0.009924]\n",
            "[Epoch 1/200] [Batch 2242/3166] [D loss: -0.109048] [G loss: 11.035284] [gradient_penalty: 0.006937]\n",
            "[Epoch 1/200] [Batch 2243/3166] [D loss: -0.231529] [G loss: 11.696512] [gradient_penalty: 0.012158]\n",
            "[Epoch 1/200] [Batch 2244/3166] [D loss: 1.027077] [G loss: 10.885297] [gradient_penalty: 0.008989]\n",
            "[Epoch 1/200] [Batch 2245/3166] [D loss: 0.504251] [G loss: 10.642851] [gradient_penalty: 0.009617]\n",
            "[Epoch 1/200] [Batch 2246/3166] [D loss: 1.329274] [G loss: 11.023316] [gradient_penalty: 0.008174]\n",
            "[Epoch 1/200] [Batch 2247/3166] [D loss: 0.801693] [G loss: 11.041716] [gradient_penalty: 0.007750]\n",
            "[Epoch 1/200] [Batch 2248/3166] [D loss: 0.962291] [G loss: 11.559381] [gradient_penalty: 0.008422]\n",
            "[Epoch 1/200] [Batch 2249/3166] [D loss: -0.266016] [G loss: 11.433285] [gradient_penalty: 0.006811]\n",
            "[Epoch 1/200] [Batch 2250/3166] [D loss: 1.523356] [G loss: 11.001390] [gradient_penalty: 0.009055]\n",
            "[Epoch 1/200] [Batch 2251/3166] [D loss: -0.680702] [G loss: 11.553063] [gradient_penalty: 0.010682]\n",
            "[Epoch 1/200] [Batch 2252/3166] [D loss: 0.177259] [G loss: 11.800357] [gradient_penalty: 0.011970]\n",
            "[Epoch 1/200] [Batch 2253/3166] [D loss: -0.134474] [G loss: 12.141332] [gradient_penalty: 0.008421]\n",
            "[Epoch 1/200] [Batch 2254/3166] [D loss: 1.001274] [G loss: 10.993605] [gradient_penalty: 0.006340]\n",
            "[Epoch 1/200] [Batch 2255/3166] [D loss: 0.016422] [G loss: 11.718398] [gradient_penalty: 0.008915]\n",
            "[Epoch 1/200] [Batch 2256/3166] [D loss: 0.414161] [G loss: 11.253525] [gradient_penalty: 0.006645]\n",
            "[Epoch 1/200] [Batch 2257/3166] [D loss: -0.487484] [G loss: 10.780501] [gradient_penalty: 0.009067]\n",
            "[Epoch 1/200] [Batch 2258/3166] [D loss: -0.007519] [G loss: 11.025164] [gradient_penalty: 0.007349]\n",
            "[Epoch 1/200] [Batch 2259/3166] [D loss: -1.132273] [G loss: 10.885766] [gradient_penalty: 0.008108]\n",
            "[Epoch 1/200] [Batch 2260/3166] [D loss: 1.130408] [G loss: 10.741987] [gradient_penalty: 0.010893]\n",
            "[Epoch 1/200] [Batch 2261/3166] [D loss: 1.192134] [G loss: 11.116221] [gradient_penalty: 0.007893]\n",
            "[Epoch 1/200] [Batch 2262/3166] [D loss: 0.765476] [G loss: 11.202484] [gradient_penalty: 0.006150]\n",
            "[Epoch 1/200] [Batch 2263/3166] [D loss: -0.017137] [G loss: 10.486003] [gradient_penalty: 0.012279]\n",
            "[Epoch 1/200] [Batch 2264/3166] [D loss: 0.714109] [G loss: 11.122515] [gradient_penalty: 0.007184]\n",
            "[Epoch 1/200] [Batch 2265/3166] [D loss: 0.306147] [G loss: 10.568720] [gradient_penalty: 0.007768]\n",
            "[Epoch 1/200] [Batch 2266/3166] [D loss: 0.448423] [G loss: 10.376206] [gradient_penalty: 0.007158]\n",
            "[Epoch 1/200] [Batch 2267/3166] [D loss: -0.799498] [G loss: 11.484489] [gradient_penalty: 0.006221]\n",
            "[Epoch 1/200] [Batch 2268/3166] [D loss: 1.313321] [G loss: 10.176018] [gradient_penalty: 0.010130]\n",
            "[Epoch 1/200] [Batch 2269/3166] [D loss: -0.858578] [G loss: 10.051455] [gradient_penalty: 0.007810]\n",
            "[Epoch 1/200] [Batch 2270/3166] [D loss: 1.785114] [G loss: 10.449856] [gradient_penalty: 0.007026]\n",
            "[Epoch 1/200] [Batch 2271/3166] [D loss: -0.053013] [G loss: 10.417943] [gradient_penalty: 0.007824]\n",
            "[Epoch 1/200] [Batch 2272/3166] [D loss: -0.036406] [G loss: 11.013224] [gradient_penalty: 0.008744]\n",
            "[Epoch 1/200] [Batch 2273/3166] [D loss: -0.047569] [G loss: 9.960129] [gradient_penalty: 0.007932]\n",
            "[Epoch 1/200] [Batch 2274/3166] [D loss: 0.908463] [G loss: 10.648214] [gradient_penalty: 0.006784]\n",
            "[Epoch 1/200] [Batch 2275/3166] [D loss: 0.169494] [G loss: 10.448015] [gradient_penalty: 0.012151]\n",
            "[Epoch 1/200] [Batch 2276/3166] [D loss: 0.769875] [G loss: 9.817361] [gradient_penalty: 0.008299]\n",
            "[Epoch 1/200] [Batch 2277/3166] [D loss: -0.190002] [G loss: 10.580167] [gradient_penalty: 0.009413]\n",
            "[Epoch 1/200] [Batch 2278/3166] [D loss: 0.373267] [G loss: 10.711854] [gradient_penalty: 0.007179]\n",
            "[Epoch 1/200] [Batch 2279/3166] [D loss: 0.495913] [G loss: 10.346230] [gradient_penalty: 0.008144]\n",
            "[Epoch 1/200] [Batch 2280/3166] [D loss: 1.632576] [G loss: 10.177633] [gradient_penalty: 0.007951]\n",
            "[Epoch 1/200] [Batch 2281/3166] [D loss: -0.000210] [G loss: 11.552292] [gradient_penalty: 0.005459]\n",
            "[Epoch 1/200] [Batch 2282/3166] [D loss: 0.299538] [G loss: 11.191240] [gradient_penalty: 0.008928]\n",
            "[Epoch 1/200] [Batch 2283/3166] [D loss: 0.543898] [G loss: 12.032679] [gradient_penalty: 0.009208]\n",
            "[Epoch 1/200] [Batch 2284/3166] [D loss: -0.425741] [G loss: 12.152548] [gradient_penalty: 0.008907]\n",
            "[Epoch 1/200] [Batch 2285/3166] [D loss: 0.525165] [G loss: 11.741198] [gradient_penalty: 0.009113]\n",
            "[Epoch 1/200] [Batch 2286/3166] [D loss: 0.741683] [G loss: 12.412377] [gradient_penalty: 0.009698]\n",
            "[Epoch 1/200] [Batch 2287/3166] [D loss: -0.487066] [G loss: 12.435475] [gradient_penalty: 0.007853]\n",
            "[Epoch 1/200] [Batch 2288/3166] [D loss: 0.470388] [G loss: 11.302174] [gradient_penalty: 0.008969]\n",
            "[Epoch 1/200] [Batch 2289/3166] [D loss: 1.105546] [G loss: 10.794972] [gradient_penalty: 0.009318]\n",
            "[Epoch 1/200] [Batch 2290/3166] [D loss: -0.163317] [G loss: 10.461723] [gradient_penalty: 0.007778]\n",
            "[Epoch 1/200] [Batch 2291/3166] [D loss: 1.206036] [G loss: 10.135347] [gradient_penalty: 0.008992]\n",
            "[Epoch 1/200] [Batch 2292/3166] [D loss: 1.531027] [G loss: 9.499947] [gradient_penalty: 0.009446]\n",
            "[Epoch 1/200] [Batch 2293/3166] [D loss: 0.431897] [G loss: 9.711975] [gradient_penalty: 0.008534]\n",
            "[Epoch 1/200] [Batch 2294/3166] [D loss: 0.571339] [G loss: 9.877131] [gradient_penalty: 0.007408]\n",
            "[Epoch 1/200] [Batch 2295/3166] [D loss: -1.031423] [G loss: 10.094900] [gradient_penalty: 0.005410]\n",
            "[Epoch 1/200] [Batch 2296/3166] [D loss: 0.948620] [G loss: 9.550207] [gradient_penalty: 0.008704]\n",
            "[Epoch 1/200] [Batch 2297/3166] [D loss: 0.546304] [G loss: 8.822097] [gradient_penalty: 0.006365]\n",
            "[Epoch 1/200] [Batch 2298/3166] [D loss: 0.312349] [G loss: 8.642275] [gradient_penalty: 0.006639]\n",
            "[Epoch 1/200] [Batch 2299/3166] [D loss: 0.325581] [G loss: 9.427345] [gradient_penalty: 0.011035]\n",
            "[Epoch 1/200] [Batch 2300/3166] [D loss: -0.332349] [G loss: 9.497026] [gradient_penalty: 0.007468]\n",
            "[Epoch 1/200] [Batch 2301/3166] [D loss: 0.005733] [G loss: 8.398876] [gradient_penalty: 0.008685]\n",
            "[Epoch 1/200] [Batch 2302/3166] [D loss: -1.072696] [G loss: 8.652422] [gradient_penalty: 0.007909]\n",
            "[Epoch 1/200] [Batch 2303/3166] [D loss: 1.398724] [G loss: 8.538916] [gradient_penalty: 0.009967]\n",
            "[Epoch 1/200] [Batch 2304/3166] [D loss: 0.621724] [G loss: 8.048853] [gradient_penalty: 0.008301]\n",
            "[Epoch 1/200] [Batch 2305/3166] [D loss: 0.711416] [G loss: 8.247041] [gradient_penalty: 0.011073]\n",
            "[Epoch 1/200] [Batch 2306/3166] [D loss: -0.443584] [G loss: 9.150064] [gradient_penalty: 0.006873]\n",
            "[Epoch 1/200] [Batch 2307/3166] [D loss: 0.717325] [G loss: 9.105074] [gradient_penalty: 0.009979]\n",
            "[Epoch 1/200] [Batch 2308/3166] [D loss: 0.468242] [G loss: 9.820448] [gradient_penalty: 0.009037]\n",
            "[Epoch 1/200] [Batch 2309/3166] [D loss: 1.626659] [G loss: 9.804808] [gradient_penalty: 0.008071]\n",
            "[Epoch 1/200] [Batch 2310/3166] [D loss: 0.784920] [G loss: 8.802589] [gradient_penalty: 0.011011]\n",
            "[Epoch 1/200] [Batch 2311/3166] [D loss: 0.110431] [G loss: 7.984177] [gradient_penalty: 0.009849]\n",
            "[Epoch 1/200] [Batch 2312/3166] [D loss: 1.237822] [G loss: 7.776863] [gradient_penalty: 0.009601]\n",
            "[Epoch 1/200] [Batch 2313/3166] [D loss: -0.392380] [G loss: 8.825939] [gradient_penalty: 0.006199]\n",
            "[Epoch 1/200] [Batch 2314/3166] [D loss: 1.525621] [G loss: 8.254490] [gradient_penalty: 0.010997]\n",
            "[Epoch 1/200] [Batch 2315/3166] [D loss: 0.634870] [G loss: 9.542188] [gradient_penalty: 0.009993]\n",
            "[Epoch 1/200] [Batch 2316/3166] [D loss: 1.441541] [G loss: 9.162396] [gradient_penalty: 0.009686]\n",
            "[Epoch 1/200] [Batch 2317/3166] [D loss: 1.416515] [G loss: 8.757303] [gradient_penalty: 0.012788]\n",
            "[Epoch 1/200] [Batch 2318/3166] [D loss: 0.777566] [G loss: 9.992538] [gradient_penalty: 0.010333]\n",
            "[Epoch 1/200] [Batch 2319/3166] [D loss: 0.814278] [G loss: 9.115491] [gradient_penalty: 0.009613]\n",
            "[Epoch 1/200] [Batch 2320/3166] [D loss: 2.591096] [G loss: 9.439541] [gradient_penalty: 0.013842]\n",
            "[Epoch 1/200] [Batch 2321/3166] [D loss: 1.580802] [G loss: 9.835919] [gradient_penalty: 0.011354]\n",
            "[Epoch 1/200] [Batch 2322/3166] [D loss: 1.098354] [G loss: 8.888406] [gradient_penalty: 0.007664]\n",
            "[Epoch 1/200] [Batch 2323/3166] [D loss: 0.901200] [G loss: 9.520220] [gradient_penalty: 0.012686]\n",
            "[Epoch 1/200] [Batch 2324/3166] [D loss: 0.795294] [G loss: 9.125210] [gradient_penalty: 0.011244]\n",
            "[Epoch 1/200] [Batch 2325/3166] [D loss: 0.597477] [G loss: 9.406200] [gradient_penalty: 0.008804]\n",
            "[Epoch 1/200] [Batch 2326/3166] [D loss: 0.008129] [G loss: 8.848642] [gradient_penalty: 0.011244]\n",
            "[Epoch 1/200] [Batch 2327/3166] [D loss: 0.075160] [G loss: 9.567636] [gradient_penalty: 0.009614]\n",
            "[Epoch 1/200] [Batch 2328/3166] [D loss: 1.909021] [G loss: 9.716919] [gradient_penalty: 0.010454]\n",
            "[Epoch 1/200] [Batch 2329/3166] [D loss: 1.372991] [G loss: 9.116951] [gradient_penalty: 0.010824]\n",
            "[Epoch 1/200] [Batch 2330/3166] [D loss: 0.589037] [G loss: 9.266417] [gradient_penalty: 0.009656]\n",
            "[Epoch 1/200] [Batch 2331/3166] [D loss: -0.456134] [G loss: 9.250440] [gradient_penalty: 0.010246]\n",
            "[Epoch 1/200] [Batch 2332/3166] [D loss: 0.413173] [G loss: 10.167159] [gradient_penalty: 0.011476]\n",
            "[Epoch 1/200] [Batch 2333/3166] [D loss: 0.682206] [G loss: 10.131951] [gradient_penalty: 0.009958]\n",
            "[Epoch 1/200] [Batch 2334/3166] [D loss: 1.903741] [G loss: 9.028942] [gradient_penalty: 0.012656]\n",
            "log\n",
            "[Epoch 1/200] [Batch 2335/3166] [D loss: 0.971373] [G loss: 10.232643] [gradient_penalty: 0.009668]\n",
            "[Epoch 1/200] [Batch 2336/3166] [D loss: -0.177611] [G loss: 11.264371] [gradient_penalty: 0.010991]\n",
            "[Epoch 1/200] [Batch 2337/3166] [D loss: 0.216332] [G loss: 10.945765] [gradient_penalty: 0.014266]\n",
            "[Epoch 1/200] [Batch 2338/3166] [D loss: 0.096538] [G loss: 10.642205] [gradient_penalty: 0.009462]\n",
            "[Epoch 1/200] [Batch 2339/3166] [D loss: -0.940395] [G loss: 10.204182] [gradient_penalty: 0.006850]\n",
            "[Epoch 1/200] [Batch 2340/3166] [D loss: 0.858343] [G loss: 10.325935] [gradient_penalty: 0.007466]\n",
            "[Epoch 1/200] [Batch 2341/3166] [D loss: 0.911326] [G loss: 11.222433] [gradient_penalty: 0.010379]\n",
            "[Epoch 1/200] [Batch 2342/3166] [D loss: -1.097587] [G loss: 10.177773] [gradient_penalty: 0.008653]\n",
            "[Epoch 1/200] [Batch 2343/3166] [D loss: -0.266428] [G loss: 11.651651] [gradient_penalty: 0.007174]\n",
            "[Epoch 1/200] [Batch 2344/3166] [D loss: 0.566485] [G loss: 10.463440] [gradient_penalty: 0.011325]\n",
            "[Epoch 1/200] [Batch 2345/3166] [D loss: 0.428000] [G loss: 10.967384] [gradient_penalty: 0.010600]\n",
            "[Epoch 1/200] [Batch 2346/3166] [D loss: 0.104975] [G loss: 10.313953] [gradient_penalty: 0.007935]\n",
            "[Epoch 1/200] [Batch 2347/3166] [D loss: -0.133057] [G loss: 8.994696] [gradient_penalty: 0.009205]\n",
            "[Epoch 1/200] [Batch 2348/3166] [D loss: 1.867750] [G loss: 8.479162] [gradient_penalty: 0.009026]\n",
            "[Epoch 1/200] [Batch 2349/3166] [D loss: 1.303585] [G loss: 9.837846] [gradient_penalty: 0.010942]\n",
            "[Epoch 1/200] [Batch 2350/3166] [D loss: 2.057041] [G loss: 9.614485] [gradient_penalty: 0.010642]\n",
            "[Epoch 1/200] [Batch 2351/3166] [D loss: 0.503008] [G loss: 10.592103] [gradient_penalty: 0.009313]\n",
            "[Epoch 1/200] [Batch 2352/3166] [D loss: 0.022830] [G loss: 11.528248] [gradient_penalty: 0.010921]\n",
            "[Epoch 1/200] [Batch 2353/3166] [D loss: -1.292857] [G loss: 11.501101] [gradient_penalty: 0.007734]\n",
            "[Epoch 1/200] [Batch 2354/3166] [D loss: -0.664817] [G loss: 12.963198] [gradient_penalty: 0.010759]\n",
            "[Epoch 1/200] [Batch 2355/3166] [D loss: -0.350806] [G loss: 13.355629] [gradient_penalty: 0.013709]\n",
            "[Epoch 1/200] [Batch 2356/3166] [D loss: -0.412504] [G loss: 12.831284] [gradient_penalty: 0.012171]\n",
            "[Epoch 1/200] [Batch 2357/3166] [D loss: 0.681229] [G loss: 12.924187] [gradient_penalty: 0.009952]\n",
            "[Epoch 1/200] [Batch 2358/3166] [D loss: -0.227395] [G loss: 12.364335] [gradient_penalty: 0.013292]\n",
            "[Epoch 1/200] [Batch 2359/3166] [D loss: 1.024722] [G loss: 11.643947] [gradient_penalty: 0.010007]\n",
            "[Epoch 1/200] [Batch 2360/3166] [D loss: 0.091394] [G loss: 11.591921] [gradient_penalty: 0.010677]\n",
            "[Epoch 1/200] [Batch 2361/3166] [D loss: 0.762567] [G loss: 10.927137] [gradient_penalty: 0.010230]\n",
            "[Epoch 1/200] [Batch 2362/3166] [D loss: 0.158816] [G loss: 10.954334] [gradient_penalty: 0.008873]\n",
            "[Epoch 1/200] [Batch 2363/3166] [D loss: 0.617511] [G loss: 10.454496] [gradient_penalty: 0.010433]\n",
            "[Epoch 1/200] [Batch 2364/3166] [D loss: 0.552271] [G loss: 10.122367] [gradient_penalty: 0.007090]\n",
            "[Epoch 1/200] [Batch 2365/3166] [D loss: 1.560505] [G loss: 9.880444] [gradient_penalty: 0.008946]\n",
            "[Epoch 1/200] [Batch 2366/3166] [D loss: -0.633730] [G loss: 10.509373] [gradient_penalty: 0.007677]\n",
            "[Epoch 1/200] [Batch 2367/3166] [D loss: 1.547646] [G loss: 10.536195] [gradient_penalty: 0.010451]\n",
            "[Epoch 1/200] [Batch 2368/3166] [D loss: 0.560961] [G loss: 9.885223] [gradient_penalty: 0.006478]\n",
            "[Epoch 1/200] [Batch 2369/3166] [D loss: 0.307228] [G loss: 10.196476] [gradient_penalty: 0.009328]\n",
            "[Epoch 1/200] [Batch 2370/3166] [D loss: -0.755414] [G loss: 10.532848] [gradient_penalty: 0.009616]\n",
            "[Epoch 1/200] [Batch 2371/3166] [D loss: 0.185301] [G loss: 10.306957] [gradient_penalty: 0.007784]\n",
            "[Epoch 1/200] [Batch 2372/3166] [D loss: -1.123932] [G loss: 10.302189] [gradient_penalty: 0.006530]\n",
            "[Epoch 1/200] [Batch 2373/3166] [D loss: -0.047572] [G loss: 9.058743] [gradient_penalty: 0.005782]\n",
            "[Epoch 1/200] [Batch 2374/3166] [D loss: 0.246477] [G loss: 9.086966] [gradient_penalty: 0.004812]\n",
            "[Epoch 1/200] [Batch 2375/3166] [D loss: -0.601955] [G loss: 9.812881] [gradient_penalty: 0.006919]\n",
            "[Epoch 1/200] [Batch 2376/3166] [D loss: 1.274898] [G loss: 9.163010] [gradient_penalty: 0.008685]\n",
            "[Epoch 1/200] [Batch 2377/3166] [D loss: 1.260350] [G loss: 9.655934] [gradient_penalty: 0.007471]\n",
            "[Epoch 1/200] [Batch 2378/3166] [D loss: 0.694603] [G loss: 10.310522] [gradient_penalty: 0.010139]\n",
            "[Epoch 1/200] [Batch 2379/3166] [D loss: 1.209041] [G loss: 9.638887] [gradient_penalty: 0.006285]\n",
            "[Epoch 1/200] [Batch 2380/3166] [D loss: 0.706625] [G loss: 9.774089] [gradient_penalty: 0.007379]\n",
            "[Epoch 1/200] [Batch 2381/3166] [D loss: 0.734205] [G loss: 8.980759] [gradient_penalty: 0.009691]\n",
            "[Epoch 1/200] [Batch 2382/3166] [D loss: 0.556343] [G loss: 9.376059] [gradient_penalty: 0.007101]\n",
            "[Epoch 1/200] [Batch 2383/3166] [D loss: 0.190098] [G loss: 9.132162] [gradient_penalty: 0.007504]\n",
            "[Epoch 1/200] [Batch 2384/3166] [D loss: 0.917778] [G loss: 9.505150] [gradient_penalty: 0.008180]\n",
            "[Epoch 1/200] [Batch 2385/3166] [D loss: -0.210328] [G loss: 9.846735] [gradient_penalty: 0.010874]\n",
            "[Epoch 1/200] [Batch 2386/3166] [D loss: 0.501338] [G loss: 9.220806] [gradient_penalty: 0.011544]\n",
            "[Epoch 1/200] [Batch 2387/3166] [D loss: 0.394443] [G loss: 9.088367] [gradient_penalty: 0.009237]\n",
            "[Epoch 1/200] [Batch 2388/3166] [D loss: -0.954365] [G loss: 8.702271] [gradient_penalty: 0.008912]\n",
            "[Epoch 1/200] [Batch 2389/3166] [D loss: 0.340895] [G loss: 9.108628] [gradient_penalty: 0.008654]\n",
            "[Epoch 1/200] [Batch 2390/3166] [D loss: 1.357973] [G loss: 9.344521] [gradient_penalty: 0.011854]\n",
            "[Epoch 1/200] [Batch 2391/3166] [D loss: 1.125544] [G loss: 8.989749] [gradient_penalty: 0.011009]\n",
            "[Epoch 1/200] [Batch 2392/3166] [D loss: -0.555125] [G loss: 8.276911] [gradient_penalty: 0.008877]\n",
            "[Epoch 1/200] [Batch 2393/3166] [D loss: 0.600757] [G loss: 8.653744] [gradient_penalty: 0.010091]\n",
            "[Epoch 1/200] [Batch 2394/3166] [D loss: 1.387671] [G loss: 8.034474] [gradient_penalty: 0.009326]\n",
            "[Epoch 1/200] [Batch 2395/3166] [D loss: -0.175269] [G loss: 8.697836] [gradient_penalty: 0.007025]\n",
            "[Epoch 1/200] [Batch 2396/3166] [D loss: 1.009812] [G loss: 8.642965] [gradient_penalty: 0.007431]\n",
            "[Epoch 1/200] [Batch 2397/3166] [D loss: -0.246055] [G loss: 9.074142] [gradient_penalty: 0.008336]\n",
            "[Epoch 1/200] [Batch 2398/3166] [D loss: 1.086935] [G loss: 9.078331] [gradient_penalty: 0.008402]\n",
            "[Epoch 1/200] [Batch 2399/3166] [D loss: 0.612266] [G loss: 8.820808] [gradient_penalty: 0.009348]\n",
            "[Epoch 1/200] [Batch 2400/3166] [D loss: 0.196082] [G loss: 10.144770] [gradient_penalty: 0.011597]\n",
            "[Epoch 1/200] [Batch 2401/3166] [D loss: 0.898820] [G loss: 9.347658] [gradient_penalty: 0.008228]\n",
            "[Epoch 1/200] [Batch 2402/3166] [D loss: 1.179402] [G loss: 9.791901] [gradient_penalty: 0.007411]\n",
            "[Epoch 1/200] [Batch 2403/3166] [D loss: 0.430684] [G loss: 9.675617] [gradient_penalty: 0.008149]\n",
            "[Epoch 1/200] [Batch 2404/3166] [D loss: 0.342021] [G loss: 10.014777] [gradient_penalty: 0.009773]\n",
            "[Epoch 1/200] [Batch 2405/3166] [D loss: 0.510321] [G loss: 10.330406] [gradient_penalty: 0.008829]\n",
            "[Epoch 1/200] [Batch 2406/3166] [D loss: -1.804446] [G loss: 9.409679] [gradient_penalty: 0.010244]\n",
            "[Epoch 1/200] [Batch 2407/3166] [D loss: -0.571541] [G loss: 9.412310] [gradient_penalty: 0.008857]\n",
            "[Epoch 1/200] [Batch 2408/3166] [D loss: 0.890335] [G loss: 10.292122] [gradient_penalty: 0.009196]\n",
            "[Epoch 1/200] [Batch 2409/3166] [D loss: 1.416351] [G loss: 8.908415] [gradient_penalty: 0.007095]\n",
            "[Epoch 1/200] [Batch 2410/3166] [D loss: -0.245150] [G loss: 10.272189] [gradient_penalty: 0.005737]\n",
            "[Epoch 1/200] [Batch 2411/3166] [D loss: 0.291254] [G loss: 10.386079] [gradient_penalty: 0.008777]\n",
            "[Epoch 1/200] [Batch 2412/3166] [D loss: 0.536484] [G loss: 10.581018] [gradient_penalty: 0.012933]\n",
            "[Epoch 1/200] [Batch 2413/3166] [D loss: 0.095605] [G loss: 9.061898] [gradient_penalty: 0.008483]\n",
            "[Epoch 1/200] [Batch 2414/3166] [D loss: 0.723557] [G loss: 10.875473] [gradient_penalty: 0.009364]\n",
            "[Epoch 1/200] [Batch 2415/3166] [D loss: 1.017220] [G loss: 11.338933] [gradient_penalty: 0.008507]\n",
            "[Epoch 1/200] [Batch 2416/3166] [D loss: 0.755469] [G loss: 10.659773] [gradient_penalty: 0.009137]\n",
            "[Epoch 1/200] [Batch 2417/3166] [D loss: 2.094511] [G loss: 10.709768] [gradient_penalty: 0.008764]\n",
            "[Epoch 1/200] [Batch 2418/3166] [D loss: 0.614275] [G loss: 9.819939] [gradient_penalty: 0.006676]\n",
            "[Epoch 1/200] [Batch 2419/3166] [D loss: 0.216504] [G loss: 9.622082] [gradient_penalty: 0.007041]\n",
            "[Epoch 1/200] [Batch 2420/3166] [D loss: -0.312999] [G loss: 10.775560] [gradient_penalty: 0.007476]\n",
            "[Epoch 1/200] [Batch 2421/3166] [D loss: 1.327180] [G loss: 10.726656] [gradient_penalty: 0.006634]\n",
            "[Epoch 1/200] [Batch 2422/3166] [D loss: 1.414889] [G loss: 11.377064] [gradient_penalty: 0.005315]\n",
            "[Epoch 1/200] [Batch 2423/3166] [D loss: 0.040357] [G loss: 10.720831] [gradient_penalty: 0.006812]\n",
            "[Epoch 1/200] [Batch 2424/3166] [D loss: -0.320548] [G loss: 10.706119] [gradient_penalty: 0.006854]\n",
            "[Epoch 1/200] [Batch 2425/3166] [D loss: 0.579730] [G loss: 10.908772] [gradient_penalty: 0.006709]\n",
            "[Epoch 1/200] [Batch 2426/3166] [D loss: 0.717238] [G loss: 11.287065] [gradient_penalty: 0.010112]\n",
            "[Epoch 1/200] [Batch 2427/3166] [D loss: 0.820875] [G loss: 10.904398] [gradient_penalty: 0.008430]\n",
            "[Epoch 1/200] [Batch 2428/3166] [D loss: 0.371999] [G loss: 10.995001] [gradient_penalty: 0.007272]\n",
            "[Epoch 1/200] [Batch 2429/3166] [D loss: 0.745726] [G loss: 11.665213] [gradient_penalty: 0.007582]\n",
            "[Epoch 1/200] [Batch 2430/3166] [D loss: -0.684928] [G loss: 10.911089] [gradient_penalty: 0.007480]\n",
            "[Epoch 1/200] [Batch 2431/3166] [D loss: 0.045865] [G loss: 11.592926] [gradient_penalty: 0.006551]\n",
            "[Epoch 1/200] [Batch 2432/3166] [D loss: -0.291570] [G loss: 11.704624] [gradient_penalty: 0.008566]\n",
            "[Epoch 1/200] [Batch 2433/3166] [D loss: 1.188591] [G loss: 10.692064] [gradient_penalty: 0.009555]\n",
            "[Epoch 1/200] [Batch 2434/3166] [D loss: 0.248386] [G loss: 11.181567] [gradient_penalty: 0.008498]\n",
            "log\n",
            "[Epoch 1/200] [Batch 2435/3166] [D loss: 0.990476] [G loss: 10.606487] [gradient_penalty: 0.007995]\n",
            "[Epoch 1/200] [Batch 2436/3166] [D loss: 0.552701] [G loss: 10.996107] [gradient_penalty: 0.007766]\n",
            "[Epoch 1/200] [Batch 2437/3166] [D loss: -0.541591] [G loss: 11.008339] [gradient_penalty: 0.005355]\n",
            "[Epoch 1/200] [Batch 2438/3166] [D loss: 1.243955] [G loss: 10.981723] [gradient_penalty: 0.006735]\n",
            "[Epoch 1/200] [Batch 2439/3166] [D loss: 0.679172] [G loss: 11.143408] [gradient_penalty: 0.009448]\n",
            "[Epoch 1/200] [Batch 2440/3166] [D loss: 0.647898] [G loss: 10.476625] [gradient_penalty: 0.007092]\n",
            "[Epoch 1/200] [Batch 2441/3166] [D loss: 0.274521] [G loss: 10.830570] [gradient_penalty: 0.007610]\n",
            "[Epoch 1/200] [Batch 2442/3166] [D loss: 1.028552] [G loss: 10.879449] [gradient_penalty: 0.007929]\n",
            "[Epoch 1/200] [Batch 2443/3166] [D loss: 0.230685] [G loss: 11.130617] [gradient_penalty: 0.009478]\n",
            "[Epoch 1/200] [Batch 2444/3166] [D loss: 0.758182] [G loss: 10.534010] [gradient_penalty: 0.008218]\n",
            "[Epoch 1/200] [Batch 2445/3166] [D loss: -0.550335] [G loss: 10.929150] [gradient_penalty: 0.005327]\n",
            "[Epoch 1/200] [Batch 2446/3166] [D loss: 0.742810] [G loss: 11.083498] [gradient_penalty: 0.009138]\n",
            "[Epoch 1/200] [Batch 2447/3166] [D loss: 1.118199] [G loss: 10.916456] [gradient_penalty: 0.009464]\n",
            "[Epoch 1/200] [Batch 2448/3166] [D loss: 0.768366] [G loss: 10.807590] [gradient_penalty: 0.007672]\n",
            "[Epoch 1/200] [Batch 2449/3166] [D loss: 0.709888] [G loss: 10.248963] [gradient_penalty: 0.008063]\n",
            "[Epoch 1/200] [Batch 2450/3166] [D loss: 0.705295] [G loss: 10.729313] [gradient_penalty: 0.006621]\n",
            "[Epoch 1/200] [Batch 2451/3166] [D loss: 0.019845] [G loss: 10.695902] [gradient_penalty: 0.010008]\n",
            "[Epoch 1/200] [Batch 2452/3166] [D loss: 1.771145] [G loss: 10.526188] [gradient_penalty: 0.009029]\n",
            "[Epoch 1/200] [Batch 2453/3166] [D loss: 0.024530] [G loss: 10.493486] [gradient_penalty: 0.007970]\n",
            "[Epoch 1/200] [Batch 2454/3166] [D loss: 1.439287] [G loss: 9.914330] [gradient_penalty: 0.008538]\n",
            "[Epoch 1/200] [Batch 2455/3166] [D loss: 2.208859] [G loss: 9.535282] [gradient_penalty: 0.008539]\n",
            "[Epoch 1/200] [Batch 2456/3166] [D loss: 1.539484] [G loss: 10.657801] [gradient_penalty: 0.007077]\n",
            "[Epoch 1/200] [Batch 2457/3166] [D loss: 0.421325] [G loss: 11.154211] [gradient_penalty: 0.007260]\n",
            "[Epoch 1/200] [Batch 2458/3166] [D loss: 1.132763] [G loss: 10.341661] [gradient_penalty: 0.006324]\n",
            "[Epoch 1/200] [Batch 2459/3166] [D loss: 0.808246] [G loss: 10.710155] [gradient_penalty: 0.010428]\n",
            "[Epoch 1/200] [Batch 2460/3166] [D loss: 1.198931] [G loss: 9.182203] [gradient_penalty: 0.008921]\n",
            "[Epoch 1/200] [Batch 2461/3166] [D loss: -0.225505] [G loss: 10.113913] [gradient_penalty: 0.006288]\n",
            "[Epoch 1/200] [Batch 2462/3166] [D loss: 0.607202] [G loss: 10.151178] [gradient_penalty: 0.006372]\n",
            "[Epoch 1/200] [Batch 2463/3166] [D loss: 0.388124] [G loss: 10.611082] [gradient_penalty: 0.009224]\n",
            "[Epoch 1/200] [Batch 2464/3166] [D loss: 0.450729] [G loss: 9.338840] [gradient_penalty: 0.006902]\n",
            "[Epoch 1/200] [Batch 2465/3166] [D loss: 0.539520] [G loss: 9.937277] [gradient_penalty: 0.006549]\n",
            "[Epoch 1/200] [Batch 2466/3166] [D loss: 0.539557] [G loss: 10.322752] [gradient_penalty: 0.008925]\n",
            "[Epoch 1/200] [Batch 2467/3166] [D loss: 0.814021] [G loss: 8.919179] [gradient_penalty: 0.007486]\n",
            "[Epoch 1/200] [Batch 2468/3166] [D loss: 0.230910] [G loss: 9.957764] [gradient_penalty: 0.004846]\n",
            "[Epoch 1/200] [Batch 2469/3166] [D loss: 0.145202] [G loss: 9.682604] [gradient_penalty: 0.009197]\n",
            "[Epoch 1/200] [Batch 2470/3166] [D loss: -0.334925] [G loss: 10.064220] [gradient_penalty: 0.006794]\n",
            "[Epoch 1/200] [Batch 2471/3166] [D loss: 1.438828] [G loss: 9.625687] [gradient_penalty: 0.009782]\n",
            "[Epoch 1/200] [Batch 2472/3166] [D loss: 0.489290] [G loss: 9.171766] [gradient_penalty: 0.007656]\n",
            "[Epoch 1/200] [Batch 2473/3166] [D loss: -0.647404] [G loss: 9.777761] [gradient_penalty: 0.006457]\n",
            "[Epoch 1/200] [Batch 2474/3166] [D loss: 1.649661] [G loss: 9.215174] [gradient_penalty: 0.010455]\n",
            "[Epoch 1/200] [Batch 2475/3166] [D loss: 0.797208] [G loss: 9.306175] [gradient_penalty: 0.008762]\n",
            "[Epoch 1/200] [Batch 2476/3166] [D loss: 0.555232] [G loss: 9.958113] [gradient_penalty: 0.009532]\n",
            "[Epoch 1/200] [Batch 2477/3166] [D loss: 1.106068] [G loss: 9.781092] [gradient_penalty: 0.007252]\n",
            "[Epoch 1/200] [Batch 2478/3166] [D loss: 0.551409] [G loss: 9.562735] [gradient_penalty: 0.009615]\n",
            "[Epoch 1/200] [Batch 2479/3166] [D loss: 0.310297] [G loss: 9.864393] [gradient_penalty: 0.008038]\n",
            "[Epoch 1/200] [Batch 2480/3166] [D loss: -0.792148] [G loss: 10.337687] [gradient_penalty: 0.006169]\n",
            "[Epoch 1/200] [Batch 2481/3166] [D loss: 1.220870] [G loss: 9.440106] [gradient_penalty: 0.006501]\n",
            "[Epoch 1/200] [Batch 2482/3166] [D loss: 0.523033] [G loss: 9.334242] [gradient_penalty: 0.005736]\n",
            "[Epoch 1/200] [Batch 2483/3166] [D loss: 0.874545] [G loss: 10.132607] [gradient_penalty: 0.008289]\n",
            "[Epoch 1/200] [Batch 2484/3166] [D loss: 0.489943] [G loss: 10.295910] [gradient_penalty: 0.007547]\n",
            "[Epoch 1/200] [Batch 2485/3166] [D loss: 0.680335] [G loss: 9.701223] [gradient_penalty: 0.006799]\n",
            "[Epoch 1/200] [Batch 2486/3166] [D loss: -0.165301] [G loss: 9.506378] [gradient_penalty: 0.006006]\n",
            "[Epoch 1/200] [Batch 2487/3166] [D loss: 1.099859] [G loss: 10.146101] [gradient_penalty: 0.008113]\n",
            "[Epoch 1/200] [Batch 2488/3166] [D loss: 0.283492] [G loss: 9.577393] [gradient_penalty: 0.008010]\n",
            "[Epoch 1/200] [Batch 2489/3166] [D loss: 0.568098] [G loss: 10.336599] [gradient_penalty: 0.008285]\n",
            "[Epoch 1/200] [Batch 2490/3166] [D loss: -0.252847] [G loss: 10.411242] [gradient_penalty: 0.006623]\n",
            "[Epoch 1/200] [Batch 2491/3166] [D loss: 0.461812] [G loss: 9.927538] [gradient_penalty: 0.007010]\n",
            "[Epoch 1/200] [Batch 2492/3166] [D loss: 0.881081] [G loss: 10.020237] [gradient_penalty: 0.008701]\n",
            "[Epoch 1/200] [Batch 2493/3166] [D loss: 1.512152] [G loss: 9.720413] [gradient_penalty: 0.007093]\n",
            "[Epoch 1/200] [Batch 2494/3166] [D loss: 0.415550] [G loss: 11.105898] [gradient_penalty: 0.006907]\n",
            "[Epoch 1/200] [Batch 2495/3166] [D loss: -0.489691] [G loss: 10.020913] [gradient_penalty: 0.009011]\n",
            "[Epoch 1/200] [Batch 2496/3166] [D loss: 0.697993] [G loss: 8.895431] [gradient_penalty: 0.007375]\n",
            "[Epoch 1/200] [Batch 2497/3166] [D loss: 0.919500] [G loss: 9.902734] [gradient_penalty: 0.008180]\n",
            "[Epoch 1/200] [Batch 2498/3166] [D loss: 0.106401] [G loss: 9.695161] [gradient_penalty: 0.008599]\n",
            "[Epoch 1/200] [Batch 2499/3166] [D loss: 0.000723] [G loss: 9.680905] [gradient_penalty: 0.006734]\n",
            "[Epoch 1/200] [Batch 2500/3166] [D loss: -0.293295] [G loss: 9.375799] [gradient_penalty: 0.007374]\n",
            "[Epoch 1/200] [Batch 2501/3166] [D loss: 0.486850] [G loss: 9.655930] [gradient_penalty: 0.004777]\n",
            "[Epoch 1/200] [Batch 2502/3166] [D loss: -0.025975] [G loss: 10.276218] [gradient_penalty: 0.006243]\n",
            "[Epoch 1/200] [Batch 2503/3166] [D loss: 1.116938] [G loss: 10.018908] [gradient_penalty: 0.007019]\n",
            "[Epoch 1/200] [Batch 2504/3166] [D loss: 0.582648] [G loss: 10.374247] [gradient_penalty: 0.007329]\n",
            "[Epoch 1/200] [Batch 2505/3166] [D loss: 0.043617] [G loss: 9.322847] [gradient_penalty: 0.006204]\n",
            "[Epoch 1/200] [Batch 2506/3166] [D loss: 0.289128] [G loss: 9.742689] [gradient_penalty: 0.007046]\n",
            "[Epoch 1/200] [Batch 2507/3166] [D loss: 0.838326] [G loss: 10.608816] [gradient_penalty: 0.006400]\n",
            "[Epoch 1/200] [Batch 2508/3166] [D loss: 0.839841] [G loss: 10.527594] [gradient_penalty: 0.008070]\n",
            "[Epoch 1/200] [Batch 2509/3166] [D loss: 0.691085] [G loss: 10.714676] [gradient_penalty: 0.009169]\n",
            "[Epoch 1/200] [Batch 2510/3166] [D loss: 0.626324] [G loss: 10.288908] [gradient_penalty: 0.008004]\n",
            "[Epoch 1/200] [Batch 2511/3166] [D loss: 0.422650] [G loss: 10.414207] [gradient_penalty: 0.006988]\n",
            "[Epoch 1/200] [Batch 2512/3166] [D loss: 0.403483] [G loss: 9.629980] [gradient_penalty: 0.012835]\n",
            "[Epoch 1/200] [Batch 2513/3166] [D loss: 1.858220] [G loss: 9.649538] [gradient_penalty: 0.008707]\n",
            "[Epoch 1/200] [Batch 2514/3166] [D loss: -0.642669] [G loss: 10.691496] [gradient_penalty: 0.007219]\n",
            "[Epoch 1/200] [Batch 2515/3166] [D loss: 0.623944] [G loss: 10.505804] [gradient_penalty: 0.008234]\n",
            "[Epoch 1/200] [Batch 2516/3166] [D loss: 0.354625] [G loss: 10.195281] [gradient_penalty: 0.011305]\n",
            "[Epoch 1/200] [Batch 2517/3166] [D loss: 1.307733] [G loss: 9.842472] [gradient_penalty: 0.007060]\n",
            "[Epoch 1/200] [Batch 2518/3166] [D loss: 0.845589] [G loss: 10.704357] [gradient_penalty: 0.006742]\n",
            "[Epoch 1/200] [Batch 2519/3166] [D loss: 0.743983] [G loss: 9.594187] [gradient_penalty: 0.007641]\n",
            "[Epoch 1/200] [Batch 2520/3166] [D loss: 0.127185] [G loss: 10.769969] [gradient_penalty: 0.006392]\n",
            "[Epoch 1/200] [Batch 2521/3166] [D loss: 0.406361] [G loss: 10.349068] [gradient_penalty: 0.011234]\n",
            "[Epoch 1/200] [Batch 2522/3166] [D loss: 0.646670] [G loss: 9.940322] [gradient_penalty: 0.008478]\n",
            "[Epoch 1/200] [Batch 2523/3166] [D loss: 0.054838] [G loss: 9.644453] [gradient_penalty: 0.006043]\n",
            "[Epoch 1/200] [Batch 2524/3166] [D loss: 0.331742] [G loss: 10.055668] [gradient_penalty: 0.007765]\n",
            "[Epoch 1/200] [Batch 2525/3166] [D loss: 0.569874] [G loss: 10.058258] [gradient_penalty: 0.009573]\n",
            "[Epoch 1/200] [Batch 2526/3166] [D loss: 0.098136] [G loss: 9.991019] [gradient_penalty: 0.010681]\n",
            "[Epoch 1/200] [Batch 2527/3166] [D loss: 0.462489] [G loss: 10.010548] [gradient_penalty: 0.010515]\n",
            "[Epoch 1/200] [Batch 2528/3166] [D loss: 0.808755] [G loss: 10.019286] [gradient_penalty: 0.009032]\n",
            "[Epoch 1/200] [Batch 2529/3166] [D loss: -0.519143] [G loss: 9.524546] [gradient_penalty: 0.008174]\n",
            "[Epoch 1/200] [Batch 2530/3166] [D loss: 0.228499] [G loss: 9.480537] [gradient_penalty: 0.006477]\n",
            "[Epoch 1/200] [Batch 2531/3166] [D loss: 0.333598] [G loss: 9.678652] [gradient_penalty: 0.008190]\n",
            "[Epoch 1/200] [Batch 2532/3166] [D loss: 0.438883] [G loss: 9.246059] [gradient_penalty: 0.007594]\n",
            "[Epoch 1/200] [Batch 2533/3166] [D loss: 0.434219] [G loss: 9.335413] [gradient_penalty: 0.011231]\n",
            "[Epoch 1/200] [Batch 2534/3166] [D loss: 0.046141] [G loss: 10.249564] [gradient_penalty: 0.007601]\n",
            "log\n",
            "[Epoch 1/200] [Batch 2535/3166] [D loss: 0.165513] [G loss: 9.844374] [gradient_penalty: 0.006089]\n",
            "[Epoch 1/200] [Batch 2536/3166] [D loss: 0.773721] [G loss: 10.060212] [gradient_penalty: 0.005707]\n",
            "[Epoch 1/200] [Batch 2537/3166] [D loss: -0.444360] [G loss: 10.269041] [gradient_penalty: 0.005446]\n",
            "[Epoch 1/200] [Batch 2538/3166] [D loss: 1.548424] [G loss: 10.085247] [gradient_penalty: 0.005626]\n",
            "[Epoch 1/200] [Batch 2539/3166] [D loss: -0.292397] [G loss: 10.226191] [gradient_penalty: 0.009101]\n",
            "[Epoch 1/200] [Batch 2540/3166] [D loss: 0.395259] [G loss: 10.542946] [gradient_penalty: 0.009515]\n",
            "[Epoch 1/200] [Batch 2541/3166] [D loss: 0.162805] [G loss: 10.335272] [gradient_penalty: 0.008859]\n",
            "[Epoch 1/200] [Batch 2542/3166] [D loss: 0.643752] [G loss: 9.999420] [gradient_penalty: 0.008401]\n",
            "[Epoch 1/200] [Batch 2543/3166] [D loss: 1.374149] [G loss: 10.585719] [gradient_penalty: 0.008880]\n",
            "[Epoch 1/200] [Batch 2544/3166] [D loss: 0.331636] [G loss: 11.238750] [gradient_penalty: 0.009998]\n",
            "[Epoch 1/200] [Batch 2545/3166] [D loss: 0.786196] [G loss: 9.441587] [gradient_penalty: 0.009740]\n",
            "[Epoch 1/200] [Batch 2546/3166] [D loss: 1.434569] [G loss: 10.214136] [gradient_penalty: 0.006753]\n",
            "[Epoch 1/200] [Batch 2547/3166] [D loss: 1.351488] [G loss: 9.749975] [gradient_penalty: 0.006591]\n",
            "[Epoch 1/200] [Batch 2548/3166] [D loss: 0.661984] [G loss: 9.539688] [gradient_penalty: 0.007500]\n",
            "[Epoch 1/200] [Batch 2549/3166] [D loss: 0.587797] [G loss: 10.185315] [gradient_penalty: 0.007823]\n",
            "[Epoch 1/200] [Batch 2550/3166] [D loss: -0.345478] [G loss: 9.557061] [gradient_penalty: 0.006134]\n",
            "[Epoch 1/200] [Batch 2551/3166] [D loss: 0.385038] [G loss: 9.966619] [gradient_penalty: 0.008066]\n",
            "[Epoch 1/200] [Batch 2552/3166] [D loss: 0.342495] [G loss: 10.136904] [gradient_penalty: 0.007019]\n",
            "[Epoch 1/200] [Batch 2553/3166] [D loss: -0.157181] [G loss: 9.994991] [gradient_penalty: 0.008495]\n",
            "[Epoch 1/200] [Batch 2554/3166] [D loss: 0.829294] [G loss: 9.590961] [gradient_penalty: 0.007419]\n",
            "[Epoch 1/200] [Batch 2555/3166] [D loss: 0.457523] [G loss: 8.722659] [gradient_penalty: 0.006982]\n",
            "[Epoch 1/200] [Batch 2556/3166] [D loss: 0.619973] [G loss: 9.385576] [gradient_penalty: 0.006817]\n",
            "[Epoch 1/200] [Batch 2557/3166] [D loss: 0.515328] [G loss: 9.045380] [gradient_penalty: 0.006276]\n",
            "[Epoch 1/200] [Batch 2558/3166] [D loss: 0.080327] [G loss: 9.072150] [gradient_penalty: 0.008813]\n",
            "[Epoch 1/200] [Batch 2559/3166] [D loss: 0.374299] [G loss: 9.170257] [gradient_penalty: 0.005007]\n",
            "[Epoch 1/200] [Batch 2560/3166] [D loss: 0.377275] [G loss: 9.487482] [gradient_penalty: 0.006458]\n",
            "[Epoch 1/200] [Batch 2561/3166] [D loss: -0.123361] [G loss: 9.834897] [gradient_penalty: 0.007897]\n",
            "[Epoch 1/200] [Batch 2562/3166] [D loss: -0.634621] [G loss: 10.316307] [gradient_penalty: 0.007824]\n",
            "[Epoch 1/200] [Batch 2563/3166] [D loss: -0.426799] [G loss: 10.677746] [gradient_penalty: 0.006863]\n",
            "[Epoch 1/200] [Batch 2564/3166] [D loss: 0.573321] [G loss: 10.936481] [gradient_penalty: 0.009225]\n",
            "[Epoch 1/200] [Batch 2565/3166] [D loss: 1.185047] [G loss: 10.976660] [gradient_penalty: 0.008929]\n",
            "[Epoch 1/200] [Batch 2566/3166] [D loss: 0.729167] [G loss: 11.390017] [gradient_penalty: 0.008202]\n",
            "[Epoch 1/200] [Batch 2567/3166] [D loss: 0.596932] [G loss: 10.711517] [gradient_penalty: 0.007214]\n",
            "[Epoch 1/200] [Batch 2568/3166] [D loss: 1.118407] [G loss: 10.824808] [gradient_penalty: 0.006526]\n",
            "[Epoch 1/200] [Batch 2569/3166] [D loss: 0.649005] [G loss: 10.069576] [gradient_penalty: 0.006995]\n",
            "[Epoch 1/200] [Batch 2570/3166] [D loss: 0.654889] [G loss: 10.334492] [gradient_penalty: 0.007384]\n",
            "[Epoch 1/200] [Batch 2571/3166] [D loss: -0.002909] [G loss: 10.074297] [gradient_penalty: 0.009221]\n",
            "[Epoch 1/200] [Batch 2572/3166] [D loss: 0.418524] [G loss: 11.824156] [gradient_penalty: 0.006601]\n",
            "[Epoch 1/200] [Batch 2573/3166] [D loss: 0.782444] [G loss: 11.323109] [gradient_penalty: 0.008821]\n",
            "[Epoch 1/200] [Batch 2574/3166] [D loss: 0.552360] [G loss: 11.188005] [gradient_penalty: 0.007523]\n",
            "[Epoch 1/200] [Batch 2575/3166] [D loss: -0.344376] [G loss: 10.946180] [gradient_penalty: 0.006601]\n",
            "[Epoch 1/200] [Batch 2576/3166] [D loss: -0.449606] [G loss: 10.795893] [gradient_penalty: 0.007728]\n",
            "[Epoch 1/200] [Batch 2577/3166] [D loss: 0.330340] [G loss: 10.513913] [gradient_penalty: 0.009686]\n",
            "[Epoch 1/200] [Batch 2578/3166] [D loss: 0.605327] [G loss: 11.269114] [gradient_penalty: 0.007588]\n",
            "[Epoch 1/200] [Batch 2579/3166] [D loss: 0.115416] [G loss: 9.996983] [gradient_penalty: 0.006252]\n",
            "[Epoch 1/200] [Batch 2580/3166] [D loss: 1.282867] [G loss: 9.661289] [gradient_penalty: 0.007684]\n",
            "[Epoch 1/200] [Batch 2581/3166] [D loss: 0.268801] [G loss: 10.496054] [gradient_penalty: 0.009489]\n",
            "[Epoch 1/200] [Batch 2582/3166] [D loss: 0.464237] [G loss: 10.173783] [gradient_penalty: 0.007719]\n",
            "[Epoch 1/200] [Batch 2583/3166] [D loss: 0.271468] [G loss: 10.365885] [gradient_penalty: 0.010466]\n",
            "[Epoch 1/200] [Batch 2584/3166] [D loss: 1.135126] [G loss: 10.268991] [gradient_penalty: 0.009097]\n",
            "[Epoch 1/200] [Batch 2585/3166] [D loss: 0.169004] [G loss: 10.229478] [gradient_penalty: 0.006399]\n",
            "[Epoch 1/200] [Batch 2586/3166] [D loss: 0.150689] [G loss: 10.299358] [gradient_penalty: 0.010469]\n",
            "[Epoch 1/200] [Batch 2587/3166] [D loss: 0.839670] [G loss: 10.492813] [gradient_penalty: 0.005773]\n",
            "[Epoch 1/200] [Batch 2588/3166] [D loss: 1.209409] [G loss: 10.491860] [gradient_penalty: 0.008890]\n",
            "[Epoch 1/200] [Batch 2589/3166] [D loss: -0.024653] [G loss: 10.941724] [gradient_penalty: 0.007763]\n",
            "[Epoch 1/200] [Batch 2590/3166] [D loss: 0.219948] [G loss: 10.171135] [gradient_penalty: 0.009741]\n",
            "[Epoch 1/200] [Batch 2591/3166] [D loss: 0.860382] [G loss: 10.244570] [gradient_penalty: 0.010150]\n",
            "[Epoch 1/200] [Batch 2592/3166] [D loss: 0.424654] [G loss: 10.369592] [gradient_penalty: 0.008059]\n",
            "[Epoch 1/200] [Batch 2593/3166] [D loss: 0.644920] [G loss: 9.593624] [gradient_penalty: 0.008670]\n",
            "[Epoch 1/200] [Batch 2594/3166] [D loss: 0.723233] [G loss: 10.008255] [gradient_penalty: 0.010884]\n",
            "[Epoch 1/200] [Batch 2595/3166] [D loss: 0.586283] [G loss: 11.165627] [gradient_penalty: 0.008042]\n",
            "[Epoch 1/200] [Batch 2596/3166] [D loss: 0.054509] [G loss: 10.767225] [gradient_penalty: 0.010739]\n",
            "[Epoch 1/200] [Batch 2597/3166] [D loss: -0.459781] [G loss: 10.038937] [gradient_penalty: 0.005341]\n",
            "[Epoch 1/200] [Batch 2598/3166] [D loss: 1.809677] [G loss: 10.733226] [gradient_penalty: 0.009709]\n",
            "[Epoch 1/200] [Batch 2599/3166] [D loss: 0.974154] [G loss: 10.453208] [gradient_penalty: 0.008069]\n",
            "[Epoch 1/200] [Batch 2600/3166] [D loss: 0.334479] [G loss: 10.806454] [gradient_penalty: 0.008259]\n",
            "[Epoch 1/200] [Batch 2601/3166] [D loss: 1.382798] [G loss: 11.057667] [gradient_penalty: 0.008064]\n",
            "[Epoch 1/200] [Batch 2602/3166] [D loss: 0.146012] [G loss: 10.605215] [gradient_penalty: 0.010721]\n",
            "[Epoch 1/200] [Batch 2603/3166] [D loss: 0.577951] [G loss: 10.856895] [gradient_penalty: 0.007129]\n",
            "[Epoch 1/200] [Batch 2604/3166] [D loss: -0.003107] [G loss: 9.971302] [gradient_penalty: 0.009066]\n",
            "[Epoch 1/200] [Batch 2605/3166] [D loss: -0.277626] [G loss: 10.141754] [gradient_penalty: 0.007833]\n",
            "[Epoch 1/200] [Batch 2606/3166] [D loss: 1.165300] [G loss: 9.978914] [gradient_penalty: 0.007545]\n",
            "[Epoch 1/200] [Batch 2607/3166] [D loss: 0.321644] [G loss: 10.137476] [gradient_penalty: 0.007418]\n",
            "[Epoch 1/200] [Batch 2608/3166] [D loss: 1.186452] [G loss: 10.492192] [gradient_penalty: 0.007197]\n",
            "[Epoch 1/200] [Batch 2609/3166] [D loss: 1.533969] [G loss: 9.201867] [gradient_penalty: 0.005854]\n",
            "[Epoch 1/200] [Batch 2610/3166] [D loss: 0.372181] [G loss: 9.401976] [gradient_penalty: 0.006909]\n",
            "[Epoch 1/200] [Batch 2611/3166] [D loss: 0.516902] [G loss: 9.758541] [gradient_penalty: 0.006456]\n",
            "[Epoch 1/200] [Batch 2612/3166] [D loss: 1.282602] [G loss: 10.516100] [gradient_penalty: 0.004951]\n",
            "[Epoch 1/200] [Batch 2613/3166] [D loss: 0.252559] [G loss: 10.237862] [gradient_penalty: 0.008118]\n",
            "[Epoch 1/200] [Batch 2614/3166] [D loss: 0.247662] [G loss: 10.508714] [gradient_penalty: 0.006961]\n",
            "[Epoch 1/200] [Batch 2615/3166] [D loss: -0.556304] [G loss: 11.020296] [gradient_penalty: 0.007231]\n",
            "[Epoch 1/200] [Batch 2616/3166] [D loss: 0.588655] [G loss: 11.687506] [gradient_penalty: 0.007105]\n",
            "[Epoch 1/200] [Batch 2617/3166] [D loss: -0.556577] [G loss: 12.313332] [gradient_penalty: 0.007605]\n",
            "[Epoch 1/200] [Batch 2618/3166] [D loss: 1.157364] [G loss: 12.244103] [gradient_penalty: 0.007991]\n",
            "[Epoch 1/200] [Batch 2619/3166] [D loss: 1.958231] [G loss: 10.881567] [gradient_penalty: 0.008479]\n",
            "[Epoch 1/200] [Batch 2620/3166] [D loss: 0.367777] [G loss: 11.152750] [gradient_penalty: 0.007396]\n",
            "[Epoch 1/200] [Batch 2621/3166] [D loss: -1.182408] [G loss: 10.700608] [gradient_penalty: 0.006361]\n",
            "[Epoch 1/200] [Batch 2622/3166] [D loss: 0.337889] [G loss: 11.070618] [gradient_penalty: 0.007113]\n",
            "[Epoch 1/200] [Batch 2623/3166] [D loss: 0.361795] [G loss: 10.921696] [gradient_penalty: 0.006428]\n",
            "[Epoch 1/200] [Batch 2624/3166] [D loss: 0.685698] [G loss: 10.797256] [gradient_penalty: 0.009672]\n",
            "[Epoch 1/200] [Batch 2625/3166] [D loss: 0.001301] [G loss: 11.960403] [gradient_penalty: 0.008210]\n",
            "[Epoch 1/200] [Batch 2626/3166] [D loss: -0.294067] [G loss: 11.585161] [gradient_penalty: 0.006946]\n",
            "[Epoch 1/200] [Batch 2627/3166] [D loss: -0.269556] [G loss: 12.096532] [gradient_penalty: 0.008989]\n",
            "[Epoch 1/200] [Batch 2628/3166] [D loss: -1.202277] [G loss: 12.089055] [gradient_penalty: 0.007051]\n",
            "[Epoch 1/200] [Batch 2629/3166] [D loss: -0.414350] [G loss: 12.080840] [gradient_penalty: 0.015846]\n",
            "[Epoch 1/200] [Batch 2630/3166] [D loss: -1.574094] [G loss: 11.540227] [gradient_penalty: 0.006346]\n",
            "[Epoch 1/200] [Batch 2631/3166] [D loss: 0.856712] [G loss: 10.418768] [gradient_penalty: 0.006809]\n",
            "[Epoch 1/200] [Batch 2632/3166] [D loss: 0.259003] [G loss: 9.860120] [gradient_penalty: 0.007184]\n",
            "[Epoch 1/200] [Batch 2633/3166] [D loss: 0.497395] [G loss: 9.728020] [gradient_penalty: 0.008565]\n",
            "[Epoch 1/200] [Batch 2634/3166] [D loss: 1.428273] [G loss: 8.617374] [gradient_penalty: 0.008250]\n",
            "log\n",
            "[Epoch 1/200] [Batch 2635/3166] [D loss: 1.669426] [G loss: 8.559797] [gradient_penalty: 0.006386]\n",
            "[Epoch 1/200] [Batch 2636/3166] [D loss: 0.675075] [G loss: 9.382680] [gradient_penalty: 0.007846]\n",
            "[Epoch 1/200] [Batch 2637/3166] [D loss: 0.730377] [G loss: 9.612057] [gradient_penalty: 0.006735]\n",
            "[Epoch 1/200] [Batch 2638/3166] [D loss: 0.422895] [G loss: 11.803961] [gradient_penalty: 0.006566]\n",
            "[Epoch 1/200] [Batch 2639/3166] [D loss: -0.359113] [G loss: 12.240614] [gradient_penalty: 0.008356]\n",
            "[Epoch 1/200] [Batch 2640/3166] [D loss: -1.285928] [G loss: 11.791368] [gradient_penalty: 0.010210]\n",
            "[Epoch 1/200] [Batch 2641/3166] [D loss: -1.224138] [G loss: 13.860489] [gradient_penalty: 0.007953]\n",
            "[Epoch 1/200] [Batch 2642/3166] [D loss: -1.438822] [G loss: 13.135409] [gradient_penalty: 0.006873]\n",
            "[Epoch 1/200] [Batch 2643/3166] [D loss: -1.591588] [G loss: 12.894026] [gradient_penalty: 0.008756]\n",
            "[Epoch 1/200] [Batch 2644/3166] [D loss: -1.092092] [G loss: 12.812355] [gradient_penalty: 0.009753]\n",
            "[Epoch 1/200] [Batch 2645/3166] [D loss: 0.333823] [G loss: 11.951675] [gradient_penalty: 0.009587]\n",
            "[Epoch 1/200] [Batch 2646/3166] [D loss: 1.742364] [G loss: 12.546633] [gradient_penalty: 0.012482]\n",
            "[Epoch 1/200] [Batch 2647/3166] [D loss: 0.457972] [G loss: 10.590467] [gradient_penalty: 0.009246]\n",
            "[Epoch 1/200] [Batch 2648/3166] [D loss: 1.655255] [G loss: 10.771446] [gradient_penalty: 0.010539]\n",
            "[Epoch 1/200] [Batch 2649/3166] [D loss: 2.587126] [G loss: 10.287705] [gradient_penalty: 0.010006]\n",
            "[Epoch 1/200] [Batch 2650/3166] [D loss: 1.112281] [G loss: 10.136965] [gradient_penalty: 0.009876]\n",
            "[Epoch 1/200] [Batch 2651/3166] [D loss: 2.371091] [G loss: 9.170362] [gradient_penalty: 0.008927]\n",
            "[Epoch 1/200] [Batch 2652/3166] [D loss: 0.494971] [G loss: 9.716904] [gradient_penalty: 0.008816]\n",
            "[Epoch 1/200] [Batch 2653/3166] [D loss: 1.434968] [G loss: 8.367548] [gradient_penalty: 0.006020]\n",
            "[Epoch 1/200] [Batch 2654/3166] [D loss: 0.729119] [G loss: 9.562662] [gradient_penalty: 0.006858]\n",
            "[Epoch 1/200] [Batch 2655/3166] [D loss: -0.603486] [G loss: 9.158717] [gradient_penalty: 0.005753]\n",
            "[Epoch 1/200] [Batch 2656/3166] [D loss: -0.024734] [G loss: 9.593660] [gradient_penalty: 0.005663]\n",
            "[Epoch 1/200] [Batch 2657/3166] [D loss: 0.247857] [G loss: 8.871804] [gradient_penalty: 0.007665]\n",
            "[Epoch 1/200] [Batch 2658/3166] [D loss: -0.834149] [G loss: 9.261870] [gradient_penalty: 0.006805]\n",
            "[Epoch 1/200] [Batch 2659/3166] [D loss: -1.486472] [G loss: 8.783365] [gradient_penalty: 0.008320]\n",
            "[Epoch 1/200] [Batch 2660/3166] [D loss: -0.046874] [G loss: 9.323452] [gradient_penalty: 0.007838]\n",
            "[Epoch 1/200] [Batch 2661/3166] [D loss: -0.494928] [G loss: 9.009071] [gradient_penalty: 0.009644]\n",
            "[Epoch 1/200] [Batch 2662/3166] [D loss: -0.818895] [G loss: 9.340542] [gradient_penalty: 0.010160]\n",
            "[Epoch 1/200] [Batch 2663/3166] [D loss: -1.433385] [G loss: 8.212597] [gradient_penalty: 0.005398]\n",
            "[Epoch 1/200] [Batch 2664/3166] [D loss: 0.152811] [G loss: 9.539403] [gradient_penalty: 0.009318]\n",
            "[Epoch 1/200] [Batch 2665/3166] [D loss: 0.216436] [G loss: 7.601461] [gradient_penalty: 0.007941]\n",
            "[Epoch 1/200] [Batch 2666/3166] [D loss: 0.394727] [G loss: 7.598071] [gradient_penalty: 0.008157]\n",
            "[Epoch 1/200] [Batch 2667/3166] [D loss: 2.030296] [G loss: 6.375762] [gradient_penalty: 0.004900]\n",
            "[Epoch 1/200] [Batch 2668/3166] [D loss: 2.712685] [G loss: 5.176158] [gradient_penalty: 0.007921]\n",
            "[Epoch 1/200] [Batch 2669/3166] [D loss: 3.129027] [G loss: 5.242012] [gradient_penalty: 0.008617]\n",
            "[Epoch 1/200] [Batch 2670/3166] [D loss: 1.940468] [G loss: 5.433095] [gradient_penalty: 0.008323]\n",
            "[Epoch 1/200] [Batch 2671/3166] [D loss: 2.028638] [G loss: 5.441153] [gradient_penalty: 0.006690]\n",
            "[Epoch 1/200] [Batch 2672/3166] [D loss: 2.688025] [G loss: 6.419941] [gradient_penalty: 0.007105]\n",
            "[Epoch 1/200] [Batch 2673/3166] [D loss: 2.343061] [G loss: 6.222337] [gradient_penalty: 0.007264]\n",
            "[Epoch 1/200] [Batch 2674/3166] [D loss: 1.709754] [G loss: 8.065970] [gradient_penalty: 0.009924]\n",
            "[Epoch 1/200] [Batch 2675/3166] [D loss: 1.194312] [G loss: 9.590876] [gradient_penalty: 0.009727]\n",
            "[Epoch 1/200] [Batch 2676/3166] [D loss: -0.527432] [G loss: 9.320141] [gradient_penalty: 0.007813]\n",
            "[Epoch 1/200] [Batch 2677/3166] [D loss: -1.354959] [G loss: 11.139996] [gradient_penalty: 0.007401]\n",
            "[Epoch 1/200] [Batch 2678/3166] [D loss: -1.314542] [G loss: 11.197132] [gradient_penalty: 0.009285]\n",
            "[Epoch 1/200] [Batch 2679/3166] [D loss: -1.125958] [G loss: 10.920943] [gradient_penalty: 0.009628]\n",
            "[Epoch 1/200] [Batch 2680/3166] [D loss: -0.874910] [G loss: 11.412159] [gradient_penalty: 0.007907]\n",
            "[Epoch 1/200] [Batch 2681/3166] [D loss: -0.109458] [G loss: 11.510950] [gradient_penalty: 0.012989]\n",
            "[Epoch 1/200] [Batch 2682/3166] [D loss: 0.704399] [G loss: 11.221748] [gradient_penalty: 0.009210]\n",
            "[Epoch 1/200] [Batch 2683/3166] [D loss: 0.367525] [G loss: 10.015537] [gradient_penalty: 0.005353]\n",
            "[Epoch 1/200] [Batch 2684/3166] [D loss: 0.367271] [G loss: 10.677152] [gradient_penalty: 0.007495]\n",
            "[Epoch 1/200] [Batch 2685/3166] [D loss: 2.204122] [G loss: 11.015279] [gradient_penalty: 0.010952]\n",
            "[Epoch 1/200] [Batch 2686/3166] [D loss: 0.068586] [G loss: 9.881728] [gradient_penalty: 0.011030]\n",
            "[Epoch 1/200] [Batch 2687/3166] [D loss: 1.190533] [G loss: 9.456956] [gradient_penalty: 0.008167]\n",
            "[Epoch 1/200] [Batch 2688/3166] [D loss: 0.818874] [G loss: 9.651264] [gradient_penalty: 0.008023]\n",
            "[Epoch 1/200] [Batch 2689/3166] [D loss: 2.042047] [G loss: 9.250735] [gradient_penalty: 0.007170]\n",
            "[Epoch 1/200] [Batch 2690/3166] [D loss: 0.652167] [G loss: 9.331225] [gradient_penalty: 0.008758]\n",
            "[Epoch 1/200] [Batch 2691/3166] [D loss: -0.295138] [G loss: 10.726670] [gradient_penalty: 0.007425]\n",
            "[Epoch 1/200] [Batch 2692/3166] [D loss: 0.001526] [G loss: 11.002985] [gradient_penalty: 0.014349]\n",
            "[Epoch 1/200] [Batch 2693/3166] [D loss: -1.381811] [G loss: 11.462773] [gradient_penalty: 0.007345]\n",
            "[Epoch 1/200] [Batch 2694/3166] [D loss: -0.906329] [G loss: 11.487736] [gradient_penalty: 0.013009]\n",
            "[Epoch 1/200] [Batch 2695/3166] [D loss: -3.183901] [G loss: 13.581173] [gradient_penalty: 0.008050]\n",
            "[Epoch 1/200] [Batch 2696/3166] [D loss: -2.836719] [G loss: 12.378432] [gradient_penalty: 0.008444]\n",
            "[Epoch 1/200] [Batch 2697/3166] [D loss: -2.117272] [G loss: 12.590065] [gradient_penalty: 0.011448]\n",
            "[Epoch 1/200] [Batch 2698/3166] [D loss: -2.189254] [G loss: 12.059361] [gradient_penalty: 0.011822]\n",
            "[Epoch 1/200] [Batch 2699/3166] [D loss: -0.689494] [G loss: 10.122339] [gradient_penalty: 0.015063]\n",
            "[Epoch 1/200] [Batch 2700/3166] [D loss: -0.276181] [G loss: 9.892493] [gradient_penalty: 0.008337]\n",
            "[Epoch 1/200] [Batch 2701/3166] [D loss: 1.666266] [G loss: 8.216814] [gradient_penalty: 0.011807]\n",
            "[Epoch 1/200] [Batch 2702/3166] [D loss: 3.033555] [G loss: 7.659725] [gradient_penalty: 0.009041]\n",
            "[Epoch 1/200] [Batch 2703/3166] [D loss: 2.252136] [G loss: 7.188975] [gradient_penalty: 0.007886]\n",
            "[Epoch 1/200] [Batch 2704/3166] [D loss: 2.489366] [G loss: 7.068069] [gradient_penalty: 0.006272]\n",
            "[Epoch 1/200] [Batch 2705/3166] [D loss: 2.988610] [G loss: 6.345556] [gradient_penalty: 0.008082]\n",
            "[Epoch 1/200] [Batch 2706/3166] [D loss: 1.694420] [G loss: 7.250989] [gradient_penalty: 0.007213]\n",
            "[Epoch 1/200] [Batch 2707/3166] [D loss: 1.440243] [G loss: 8.183864] [gradient_penalty: 0.007695]\n",
            "[Epoch 1/200] [Batch 2708/3166] [D loss: 1.871832] [G loss: 8.102356] [gradient_penalty: 0.008234]\n",
            "[Epoch 1/200] [Batch 2709/3166] [D loss: 0.388100] [G loss: 8.103706] [gradient_penalty: 0.006852]\n",
            "[Epoch 1/200] [Batch 2710/3166] [D loss: 0.558586] [G loss: 9.633869] [gradient_penalty: 0.008415]\n",
            "[Epoch 1/200] [Batch 2711/3166] [D loss: 1.705744] [G loss: 9.335711] [gradient_penalty: 0.008775]\n",
            "[Epoch 1/200] [Batch 2712/3166] [D loss: -0.614559] [G loss: 10.785753] [gradient_penalty: 0.007385]\n",
            "[Epoch 1/200] [Batch 2713/3166] [D loss: -0.942436] [G loss: 10.898277] [gradient_penalty: 0.006402]\n",
            "[Epoch 1/200] [Batch 2714/3166] [D loss: -0.943308] [G loss: 10.921345] [gradient_penalty: 0.007082]\n",
            "[Epoch 1/200] [Batch 2715/3166] [D loss: -0.825399] [G loss: 12.291090] [gradient_penalty: 0.006697]\n",
            "[Epoch 1/200] [Batch 2716/3166] [D loss: -2.920493] [G loss: 13.226284] [gradient_penalty: 0.008863]\n",
            "[Epoch 1/200] [Batch 2717/3166] [D loss: -2.914135] [G loss: 13.715862] [gradient_penalty: 0.010397]\n",
            "[Epoch 1/200] [Batch 2718/3166] [D loss: -1.456912] [G loss: 13.597401] [gradient_penalty: 0.018312]\n",
            "[Epoch 1/200] [Batch 2719/3166] [D loss: -1.129473] [G loss: 14.600544] [gradient_penalty: 0.009682]\n",
            "[Epoch 1/200] [Batch 2720/3166] [D loss: 0.241439] [G loss: 13.082412] [gradient_penalty: 0.008736]\n",
            "[Epoch 1/200] [Batch 2721/3166] [D loss: -0.528667] [G loss: 13.084793] [gradient_penalty: 0.007912]\n",
            "[Epoch 1/200] [Batch 2722/3166] [D loss: -0.971978] [G loss: 13.133635] [gradient_penalty: 0.007438]\n",
            "[Epoch 1/200] [Batch 2723/3166] [D loss: -0.508841] [G loss: 11.781456] [gradient_penalty: 0.008148]\n",
            "[Epoch 1/200] [Batch 2724/3166] [D loss: -0.611827] [G loss: 12.368204] [gradient_penalty: 0.007964]\n",
            "[Epoch 1/200] [Batch 2725/3166] [D loss: 0.644743] [G loss: 10.076803] [gradient_penalty: 0.009714]\n",
            "[Epoch 1/200] [Batch 2726/3166] [D loss: 0.612306] [G loss: 10.061658] [gradient_penalty: 0.009421]\n",
            "[Epoch 1/200] [Batch 2727/3166] [D loss: 3.190385] [G loss: 9.358496] [gradient_penalty: 0.009881]\n",
            "[Epoch 1/200] [Batch 2728/3166] [D loss: 1.832183] [G loss: 9.023678] [gradient_penalty: 0.008990]\n",
            "[Epoch 1/200] [Batch 2729/3166] [D loss: 1.738266] [G loss: 8.488170] [gradient_penalty: 0.005304]\n",
            "[Epoch 1/200] [Batch 2730/3166] [D loss: 2.768330] [G loss: 7.526109] [gradient_penalty: 0.007570]\n",
            "[Epoch 1/200] [Batch 2731/3166] [D loss: 1.617573] [G loss: 7.773740] [gradient_penalty: 0.007271]\n",
            "[Epoch 1/200] [Batch 2732/3166] [D loss: 0.885521] [G loss: 8.271039] [gradient_penalty: 0.005139]\n",
            "[Epoch 1/200] [Batch 2733/3166] [D loss: 1.495986] [G loss: 7.619150] [gradient_penalty: 0.010814]\n",
            "[Epoch 1/200] [Batch 2734/3166] [D loss: -0.329682] [G loss: 7.913446] [gradient_penalty: 0.006538]\n",
            "log\n",
            "[Epoch 1/200] [Batch 2735/3166] [D loss: -0.463745] [G loss: 8.475119] [gradient_penalty: 0.009133]\n",
            "[Epoch 1/200] [Batch 2736/3166] [D loss: -0.739087] [G loss: 8.352288] [gradient_penalty: 0.007004]\n",
            "[Epoch 1/200] [Batch 2737/3166] [D loss: -1.920084] [G loss: 7.662358] [gradient_penalty: 0.008136]\n",
            "[Epoch 1/200] [Batch 2738/3166] [D loss: -0.432029] [G loss: 8.583826] [gradient_penalty: 0.006073]\n",
            "[Epoch 1/200] [Batch 2739/3166] [D loss: -1.912777] [G loss: 7.710275] [gradient_penalty: 0.009123]\n",
            "[Epoch 1/200] [Batch 2740/3166] [D loss: -1.409312] [G loss: 8.449351] [gradient_penalty: 0.007505]\n",
            "[Epoch 1/200] [Batch 2741/3166] [D loss: -2.810495] [G loss: 8.827705] [gradient_penalty: 0.009071]\n",
            "[Epoch 1/200] [Batch 2742/3166] [D loss: -1.612743] [G loss: 7.288104] [gradient_penalty: 0.009160]\n",
            "[Epoch 1/200] [Batch 2743/3166] [D loss: -2.120769] [G loss: 7.034965] [gradient_penalty: 0.011642]\n",
            "[Epoch 1/200] [Batch 2744/3166] [D loss: -2.117280] [G loss: 6.659458] [gradient_penalty: 0.008157]\n",
            "[Epoch 1/200] [Batch 2745/3166] [D loss: -0.549966] [G loss: 5.166134] [gradient_penalty: 0.010209]\n",
            "[Epoch 1/200] [Batch 2746/3166] [D loss: -0.490544] [G loss: 4.377326] [gradient_penalty: 0.010415]\n",
            "[Epoch 1/200] [Batch 2747/3166] [D loss: -0.965556] [G loss: 2.796829] [gradient_penalty: 0.009781]\n",
            "[Epoch 1/200] [Batch 2748/3166] [D loss: 1.427119] [G loss: 2.636153] [gradient_penalty: 0.009615]\n",
            "[Epoch 1/200] [Batch 2749/3166] [D loss: 1.548991] [G loss: 2.006984] [gradient_penalty: 0.009167]\n",
            "[Epoch 1/200] [Batch 2750/3166] [D loss: 3.365285] [G loss: 0.627225] [gradient_penalty: 0.008270]\n",
            "[Epoch 1/200] [Batch 2751/3166] [D loss: 5.285886] [G loss: 0.510289] [gradient_penalty: 0.008426]\n",
            "[Epoch 1/200] [Batch 2752/3166] [D loss: 2.376398] [G loss: 0.753551] [gradient_penalty: 0.008950]\n",
            "[Epoch 1/200] [Batch 2753/3166] [D loss: 2.476030] [G loss: 1.757510] [gradient_penalty: 0.007881]\n",
            "[Epoch 1/200] [Batch 2754/3166] [D loss: 1.831200] [G loss: 2.365535] [gradient_penalty: 0.008978]\n",
            "[Epoch 1/200] [Batch 2755/3166] [D loss: 2.430370] [G loss: 2.526295] [gradient_penalty: 0.008175]\n",
            "[Epoch 1/200] [Batch 2756/3166] [D loss: 1.759176] [G loss: 3.360881] [gradient_penalty: 0.008864]\n",
            "[Epoch 1/200] [Batch 2757/3166] [D loss: 0.994454] [G loss: 5.296576] [gradient_penalty: 0.009512]\n",
            "[Epoch 1/200] [Batch 2758/3166] [D loss: 0.319140] [G loss: 6.078883] [gradient_penalty: 0.009693]\n",
            "[Epoch 1/200] [Batch 2759/3166] [D loss: -1.254863] [G loss: 7.215611] [gradient_penalty: 0.009986]\n",
            "[Epoch 1/200] [Batch 2760/3166] [D loss: -2.088463] [G loss: 9.077963] [gradient_penalty: 0.009453]\n",
            "[Epoch 1/200] [Batch 2761/3166] [D loss: -2.123764] [G loss: 10.228813] [gradient_penalty: 0.008237]\n",
            "[Epoch 1/200] [Batch 2762/3166] [D loss: -4.833149] [G loss: 10.926979] [gradient_penalty: 0.008341]\n",
            "[Epoch 1/200] [Batch 2763/3166] [D loss: -1.617176] [G loss: 11.334911] [gradient_penalty: 0.014823]\n",
            "[Epoch 1/200] [Batch 2764/3166] [D loss: -3.195447] [G loss: 12.107230] [gradient_penalty: 0.010224]\n",
            "[Epoch 1/200] [Batch 2765/3166] [D loss: -2.707450] [G loss: 11.260445] [gradient_penalty: 0.008654]\n",
            "[Epoch 1/200] [Batch 2766/3166] [D loss: -2.528569] [G loss: 11.358571] [gradient_penalty: 0.012702]\n",
            "[Epoch 1/200] [Batch 2767/3166] [D loss: -3.505857] [G loss: 11.197709] [gradient_penalty: 0.011437]\n",
            "[Epoch 1/200] [Batch 2768/3166] [D loss: -0.385542] [G loss: 9.926334] [gradient_penalty: 0.010474]\n",
            "[Epoch 1/200] [Batch 2769/3166] [D loss: 2.320972] [G loss: 10.594585] [gradient_penalty: 0.011383]\n",
            "[Epoch 1/200] [Batch 2770/3166] [D loss: 0.660866] [G loss: 9.562763] [gradient_penalty: 0.007918]\n",
            "[Epoch 1/200] [Batch 2771/3166] [D loss: 1.531880] [G loss: 7.738836] [gradient_penalty: 0.009836]\n",
            "[Epoch 1/200] [Batch 2772/3166] [D loss: 0.948957] [G loss: 6.878908] [gradient_penalty: 0.006809]\n",
            "[Epoch 1/200] [Batch 2773/3166] [D loss: 3.339001] [G loss: 7.260683] [gradient_penalty: 0.008521]\n",
            "[Epoch 1/200] [Batch 2774/3166] [D loss: 3.218672] [G loss: 5.610783] [gradient_penalty: 0.009618]\n",
            "[Epoch 1/200] [Batch 2775/3166] [D loss: 3.079764] [G loss: 6.261433] [gradient_penalty: 0.008873]\n",
            "[Epoch 1/200] [Batch 2776/3166] [D loss: 2.181465] [G loss: 6.473364] [gradient_penalty: 0.009678]\n",
            "[Epoch 1/200] [Batch 2777/3166] [D loss: 3.163023] [G loss: 6.162323] [gradient_penalty: 0.008652]\n",
            "[Epoch 1/200] [Batch 2778/3166] [D loss: 2.619971] [G loss: 6.144923] [gradient_penalty: 0.011925]\n",
            "[Epoch 1/200] [Batch 2779/3166] [D loss: 3.443624] [G loss: 6.133219] [gradient_penalty: 0.007042]\n",
            "[Epoch 1/200] [Batch 2780/3166] [D loss: 2.806326] [G loss: 7.118567] [gradient_penalty: 0.009866]\n",
            "[Epoch 1/200] [Batch 2781/3166] [D loss: 1.510470] [G loss: 7.423152] [gradient_penalty: 0.006115]\n",
            "[Epoch 1/200] [Batch 2782/3166] [D loss: -0.961057] [G loss: 8.280186] [gradient_penalty: 0.005995]\n",
            "[Epoch 1/200] [Batch 2783/3166] [D loss: -1.163334] [G loss: 9.544189] [gradient_penalty: 0.007204]\n",
            "[Epoch 1/200] [Batch 2784/3166] [D loss: -1.913502] [G loss: 10.335526] [gradient_penalty: 0.007724]\n",
            "[Epoch 1/200] [Batch 2785/3166] [D loss: -3.581639] [G loss: 11.485325] [gradient_penalty: 0.006731]\n",
            "[Epoch 1/200] [Batch 2786/3166] [D loss: -3.536454] [G loss: 12.733975] [gradient_penalty: 0.011230]\n",
            "[Epoch 1/200] [Batch 2787/3166] [D loss: -5.811869] [G loss: 14.901466] [gradient_penalty: 0.010343]\n",
            "[Epoch 1/200] [Batch 2788/3166] [D loss: -4.938416] [G loss: 14.554513] [gradient_penalty: 0.011553]\n",
            "[Epoch 1/200] [Batch 2789/3166] [D loss: -6.254939] [G loss: 14.464681] [gradient_penalty: 0.014681]\n",
            "[Epoch 1/200] [Batch 2790/3166] [D loss: -4.267824] [G loss: 14.273088] [gradient_penalty: 0.025735]\n",
            "[Epoch 1/200] [Batch 2791/3166] [D loss: -2.147659] [G loss: 14.166422] [gradient_penalty: 0.015113]\n",
            "[Epoch 1/200] [Batch 2792/3166] [D loss: -0.622911] [G loss: 12.493298] [gradient_penalty: 0.019238]\n",
            "[Epoch 1/200] [Batch 2793/3166] [D loss: -0.222142] [G loss: 11.211380] [gradient_penalty: 0.011645]\n",
            "[Epoch 1/200] [Batch 2794/3166] [D loss: 0.458344] [G loss: 11.115122] [gradient_penalty: 0.021214]\n",
            "[Epoch 1/200] [Batch 2795/3166] [D loss: 0.531739] [G loss: 9.965140] [gradient_penalty: 0.014951]\n",
            "[Epoch 1/200] [Batch 2796/3166] [D loss: 4.181968] [G loss: 7.038116] [gradient_penalty: 0.013923]\n",
            "[Epoch 1/200] [Batch 2797/3166] [D loss: 2.122217] [G loss: 8.536800] [gradient_penalty: 0.014520]\n",
            "[Epoch 1/200] [Batch 2798/3166] [D loss: 4.244841] [G loss: 8.455125] [gradient_penalty: 0.011407]\n",
            "[Epoch 1/200] [Batch 2799/3166] [D loss: 4.678938] [G loss: 7.429318] [gradient_penalty: 0.008813]\n",
            "[Epoch 1/200] [Batch 2800/3166] [D loss: 3.046355] [G loss: 6.842834] [gradient_penalty: 0.011727]\n",
            "[Epoch 1/200] [Batch 2801/3166] [D loss: 3.138395] [G loss: 6.377579] [gradient_penalty: 0.013921]\n",
            "[Epoch 1/200] [Batch 2802/3166] [D loss: 2.369707] [G loss: 6.993642] [gradient_penalty: 0.015537]\n",
            "[Epoch 1/200] [Batch 2803/3166] [D loss: 3.984592] [G loss: 5.505679] [gradient_penalty: 0.009451]\n",
            "[Epoch 1/200] [Batch 2804/3166] [D loss: 2.741184] [G loss: 5.392127] [gradient_penalty: 0.009851]\n",
            "[Epoch 1/200] [Batch 2805/3166] [D loss: 3.753721] [G loss: 6.356818] [gradient_penalty: 0.011453]\n",
            "[Epoch 1/200] [Batch 2806/3166] [D loss: 2.834159] [G loss: 6.607004] [gradient_penalty: 0.011126]\n",
            "[Epoch 1/200] [Batch 2807/3166] [D loss: 2.608266] [G loss: 5.810156] [gradient_penalty: 0.009294]\n",
            "[Epoch 1/200] [Batch 2808/3166] [D loss: 2.156272] [G loss: 7.150277] [gradient_penalty: 0.005905]\n",
            "[Epoch 1/200] [Batch 2809/3166] [D loss: 2.058622] [G loss: 6.463124] [gradient_penalty: 0.006740]\n",
            "[Epoch 1/200] [Batch 2810/3166] [D loss: 0.387851] [G loss: 8.069576] [gradient_penalty: 0.007611]\n",
            "[Epoch 1/200] [Batch 2811/3166] [D loss: 1.305710] [G loss: 8.858598] [gradient_penalty: 0.009178]\n",
            "[Epoch 1/200] [Batch 2812/3166] [D loss: 1.816705] [G loss: 8.848157] [gradient_penalty: 0.010242]\n",
            "[Epoch 1/200] [Batch 2813/3166] [D loss: 0.266492] [G loss: 10.051786] [gradient_penalty: 0.009229]\n",
            "[Epoch 1/200] [Batch 2814/3166] [D loss: -0.671425] [G loss: 10.667699] [gradient_penalty: 0.006347]\n",
            "[Epoch 1/200] [Batch 2815/3166] [D loss: -1.025110] [G loss: 10.879105] [gradient_penalty: 0.007452]\n",
            "[Epoch 1/200] [Batch 2816/3166] [D loss: -1.105002] [G loss: 12.064640] [gradient_penalty: 0.007116]\n",
            "[Epoch 1/200] [Batch 2817/3166] [D loss: -0.885676] [G loss: 12.886936] [gradient_penalty: 0.009005]\n",
            "[Epoch 1/200] [Batch 2818/3166] [D loss: -2.307031] [G loss: 14.039932] [gradient_penalty: 0.006987]\n",
            "[Epoch 1/200] [Batch 2819/3166] [D loss: -1.957836] [G loss: 12.859169] [gradient_penalty: 0.009737]\n",
            "[Epoch 1/200] [Batch 2820/3166] [D loss: -0.535725] [G loss: 13.359082] [gradient_penalty: 0.015854]\n",
            "[Epoch 1/200] [Batch 2821/3166] [D loss: -0.420184] [G loss: 12.708794] [gradient_penalty: 0.008817]\n",
            "[Epoch 1/200] [Batch 2822/3166] [D loss: -2.059756] [G loss: 14.055583] [gradient_penalty: 0.005698]\n",
            "[Epoch 1/200] [Batch 2823/3166] [D loss: -1.625885] [G loss: 13.335340] [gradient_penalty: 0.013253]\n",
            "[Epoch 1/200] [Batch 2824/3166] [D loss: -0.864195] [G loss: 12.445495] [gradient_penalty: 0.008828]\n",
            "[Epoch 1/200] [Batch 2825/3166] [D loss: 2.568020] [G loss: 12.494217] [gradient_penalty: 0.008118]\n",
            "[Epoch 1/200] [Batch 2826/3166] [D loss: 0.779896] [G loss: 10.651411] [gradient_penalty: 0.008261]\n",
            "[Epoch 1/200] [Batch 2827/3166] [D loss: 2.004649] [G loss: 10.331261] [gradient_penalty: 0.010201]\n",
            "[Epoch 1/200] [Batch 2828/3166] [D loss: 0.862669] [G loss: 10.676947] [gradient_penalty: 0.007592]\n",
            "[Epoch 1/200] [Batch 2829/3166] [D loss: 3.154855] [G loss: 10.720905] [gradient_penalty: 0.007403]\n",
            "[Epoch 1/200] [Batch 2830/3166] [D loss: 3.439634] [G loss: 9.920759] [gradient_penalty: 0.008608]\n",
            "[Epoch 1/200] [Batch 2831/3166] [D loss: 3.264181] [G loss: 9.260778] [gradient_penalty: 0.009051]\n",
            "[Epoch 1/200] [Batch 2832/3166] [D loss: 2.332963] [G loss: 8.609280] [gradient_penalty: 0.007613]\n",
            "[Epoch 1/200] [Batch 2833/3166] [D loss: 3.135872] [G loss: 8.796078] [gradient_penalty: 0.010843]\n",
            "[Epoch 1/200] [Batch 2834/3166] [D loss: 1.746260] [G loss: 9.240564] [gradient_penalty: 0.006638]\n",
            "log\n",
            "[Epoch 1/200] [Batch 2835/3166] [D loss: 1.364687] [G loss: 8.914802] [gradient_penalty: 0.006790]\n",
            "[Epoch 1/200] [Batch 2836/3166] [D loss: 0.458427] [G loss: 9.323141] [gradient_penalty: 0.009109]\n",
            "[Epoch 1/200] [Batch 2837/3166] [D loss: 0.401599] [G loss: 11.100487] [gradient_penalty: 0.009735]\n",
            "[Epoch 1/200] [Batch 2838/3166] [D loss: -0.408317] [G loss: 10.462225] [gradient_penalty: 0.010245]\n",
            "[Epoch 1/200] [Batch 2839/3166] [D loss: -0.153364] [G loss: 10.377062] [gradient_penalty: 0.007949]\n",
            "[Epoch 1/200] [Batch 2840/3166] [D loss: -2.300947] [G loss: 10.345572] [gradient_penalty: 0.004571]\n",
            "[Epoch 1/200] [Batch 2841/3166] [D loss: -1.623312] [G loss: 11.437445] [gradient_penalty: 0.006832]\n",
            "[Epoch 1/200] [Batch 2842/3166] [D loss: -1.240376] [G loss: 11.909247] [gradient_penalty: 0.008249]\n",
            "[Epoch 1/200] [Batch 2843/3166] [D loss: -1.577407] [G loss: 12.171361] [gradient_penalty: 0.008082]\n",
            "[Epoch 1/200] [Batch 2844/3166] [D loss: -1.705588] [G loss: 11.233449] [gradient_penalty: 0.008843]\n",
            "[Epoch 1/200] [Batch 2845/3166] [D loss: -0.657309] [G loss: 11.184983] [gradient_penalty: 0.006642]\n",
            "[Epoch 1/200] [Batch 2846/3166] [D loss: 0.390655] [G loss: 11.104446] [gradient_penalty: 0.006543]\n",
            "[Epoch 1/200] [Batch 2847/3166] [D loss: -0.041803] [G loss: 10.408552] [gradient_penalty: 0.008072]\n",
            "[Epoch 1/200] [Batch 2848/3166] [D loss: -1.873257] [G loss: 9.504086] [gradient_penalty: 0.008883]\n",
            "[Epoch 1/200] [Batch 2849/3166] [D loss: 1.270114] [G loss: 10.146882] [gradient_penalty: 0.006617]\n",
            "[Epoch 1/200] [Batch 2850/3166] [D loss: 1.142644] [G loss: 8.649240] [gradient_penalty: 0.007986]\n",
            "[Epoch 1/200] [Batch 2851/3166] [D loss: 1.342357] [G loss: 9.589582] [gradient_penalty: 0.009024]\n",
            "[Epoch 1/200] [Batch 2852/3166] [D loss: 2.227599] [G loss: 9.069835] [gradient_penalty: 0.007542]\n",
            "[Epoch 1/200] [Batch 2853/3166] [D loss: 2.950583] [G loss: 7.986074] [gradient_penalty: 0.007043]\n",
            "[Epoch 1/200] [Batch 2854/3166] [D loss: 1.610132] [G loss: 8.354846] [gradient_penalty: 0.006124]\n",
            "[Epoch 1/200] [Batch 2855/3166] [D loss: 0.991811] [G loss: 7.074174] [gradient_penalty: 0.006651]\n",
            "[Epoch 1/200] [Batch 2856/3166] [D loss: 1.970486] [G loss: 8.382168] [gradient_penalty: 0.009408]\n",
            "[Epoch 1/200] [Batch 2857/3166] [D loss: 3.129304] [G loss: 7.137073] [gradient_penalty: 0.007111]\n",
            "[Epoch 1/200] [Batch 2858/3166] [D loss: 2.960860] [G loss: 7.182300] [gradient_penalty: 0.005662]\n",
            "[Epoch 1/200] [Batch 2859/3166] [D loss: 1.959310] [G loss: 8.606091] [gradient_penalty: 0.008859]\n",
            "[Epoch 1/200] [Batch 2860/3166] [D loss: 2.050665] [G loss: 8.109014] [gradient_penalty: 0.006242]\n",
            "[Epoch 1/200] [Batch 2861/3166] [D loss: 1.323975] [G loss: 9.051138] [gradient_penalty: 0.007614]\n",
            "[Epoch 1/200] [Batch 2862/3166] [D loss: -0.174476] [G loss: 9.958491] [gradient_penalty: 0.007360]\n",
            "[Epoch 1/200] [Batch 2863/3166] [D loss: -0.397210] [G loss: 9.817785] [gradient_penalty: 0.007827]\n",
            "[Epoch 1/200] [Batch 2864/3166] [D loss: -0.299906] [G loss: 11.530331] [gradient_penalty: 0.006472]\n",
            "[Epoch 1/200] [Batch 2865/3166] [D loss: -0.516118] [G loss: 11.764686] [gradient_penalty: 0.006000]\n",
            "[Epoch 1/200] [Batch 2866/3166] [D loss: -0.822846] [G loss: 12.770689] [gradient_penalty: 0.008476]\n",
            "[Epoch 1/200] [Batch 2867/3166] [D loss: -1.207806] [G loss: 13.167538] [gradient_penalty: 0.006701]\n",
            "[Epoch 1/200] [Batch 2868/3166] [D loss: -1.307247] [G loss: 14.203376] [gradient_penalty: 0.007780]\n",
            "[Epoch 1/200] [Batch 2869/3166] [D loss: -2.934448] [G loss: 14.550327] [gradient_penalty: 0.009397]\n",
            "[Epoch 1/200] [Batch 2870/3166] [D loss: -3.551867] [G loss: 15.729427] [gradient_penalty: 0.008484]\n",
            "[Epoch 1/200] [Batch 2871/3166] [D loss: -2.456839] [G loss: 14.847414] [gradient_penalty: 0.010785]\n",
            "[Epoch 1/200] [Batch 2872/3166] [D loss: -1.073749] [G loss: 14.971462] [gradient_penalty: 0.014994]\n",
            "[Epoch 1/200] [Batch 2873/3166] [D loss: -1.301130] [G loss: 14.971419] [gradient_penalty: 0.013935]\n",
            "[Epoch 1/200] [Batch 2874/3166] [D loss: -0.261823] [G loss: 13.257555] [gradient_penalty: 0.010925]\n",
            "[Epoch 1/200] [Batch 2875/3166] [D loss: 1.187055] [G loss: 12.313223] [gradient_penalty: 0.011813]\n",
            "[Epoch 1/200] [Batch 2876/3166] [D loss: 2.483850] [G loss: 12.011499] [gradient_penalty: 0.009337]\n",
            "[Epoch 1/200] [Batch 2877/3166] [D loss: 0.542607] [G loss: 10.851545] [gradient_penalty: 0.009744]\n",
            "[Epoch 1/200] [Batch 2878/3166] [D loss: 0.035827] [G loss: 11.155254] [gradient_penalty: 0.006410]\n",
            "[Epoch 1/200] [Batch 2879/3166] [D loss: 2.534107] [G loss: 10.540507] [gradient_penalty: 0.009127]\n",
            "[Epoch 1/200] [Batch 2880/3166] [D loss: 1.269912] [G loss: 10.521101] [gradient_penalty: 0.010210]\n",
            "[Epoch 1/200] [Batch 2881/3166] [D loss: 2.751419] [G loss: 8.934381] [gradient_penalty: 0.008642]\n",
            "[Epoch 1/200] [Batch 2882/3166] [D loss: 1.894377] [G loss: 9.719654] [gradient_penalty: 0.007716]\n",
            "[Epoch 1/200] [Batch 2883/3166] [D loss: 2.282116] [G loss: 10.082743] [gradient_penalty: 0.006737]\n",
            "[Epoch 1/200] [Batch 2884/3166] [D loss: 1.485304] [G loss: 9.489506] [gradient_penalty: 0.007862]\n",
            "[Epoch 1/200] [Batch 2885/3166] [D loss: 1.534529] [G loss: 8.196048] [gradient_penalty: 0.007554]\n",
            "[Epoch 1/200] [Batch 2886/3166] [D loss: 1.277055] [G loss: 9.322979] [gradient_penalty: 0.007198]\n",
            "[Epoch 1/200] [Batch 2887/3166] [D loss: 1.525124] [G loss: 8.506432] [gradient_penalty: 0.010300]\n",
            "[Epoch 1/200] [Batch 2888/3166] [D loss: 1.117282] [G loss: 8.490941] [gradient_penalty: 0.004966]\n",
            "[Epoch 1/200] [Batch 2889/3166] [D loss: 0.583953] [G loss: 8.484047] [gradient_penalty: 0.006976]\n",
            "[Epoch 1/200] [Batch 2890/3166] [D loss: -1.765982] [G loss: 9.031271] [gradient_penalty: 0.007032]\n",
            "[Epoch 1/200] [Batch 2891/3166] [D loss: -0.544168] [G loss: 8.930529] [gradient_penalty: 0.008869]\n",
            "[Epoch 1/200] [Batch 2892/3166] [D loss: -0.105555] [G loss: 8.666512] [gradient_penalty: 0.006721]\n",
            "[Epoch 1/200] [Batch 2893/3166] [D loss: -1.452877] [G loss: 8.787275] [gradient_penalty: 0.007673]\n",
            "[Epoch 1/200] [Batch 2894/3166] [D loss: -1.586990] [G loss: 8.996801] [gradient_penalty: 0.004861]\n",
            "[Epoch 1/200] [Batch 2895/3166] [D loss: -0.795530] [G loss: 9.218804] [gradient_penalty: 0.007767]\n",
            "[Epoch 1/200] [Batch 2896/3166] [D loss: -0.877190] [G loss: 9.135810] [gradient_penalty: 0.006380]\n",
            "[Epoch 1/200] [Batch 2897/3166] [D loss: -1.642494] [G loss: 8.621256] [gradient_penalty: 0.006521]\n",
            "[Epoch 1/200] [Batch 2898/3166] [D loss: -2.068659] [G loss: 8.544477] [gradient_penalty: 0.009166]\n",
            "[Epoch 1/200] [Batch 2899/3166] [D loss: -2.302828] [G loss: 7.811724] [gradient_penalty: 0.007631]\n",
            "[Epoch 1/200] [Batch 2900/3166] [D loss: -1.798346] [G loss: 7.608933] [gradient_penalty: 0.007450]\n",
            "[Epoch 1/200] [Batch 2901/3166] [D loss: -1.356642] [G loss: 7.944163] [gradient_penalty: 0.005787]\n",
            "[Epoch 1/200] [Batch 2902/3166] [D loss: -1.154102] [G loss: 6.873937] [gradient_penalty: 0.007734]\n",
            "[Epoch 1/200] [Batch 2903/3166] [D loss: 0.225631] [G loss: 6.940676] [gradient_penalty: 0.007650]\n",
            "[Epoch 1/200] [Batch 2904/3166] [D loss: 0.340270] [G loss: 5.459731] [gradient_penalty: 0.008053]\n",
            "[Epoch 1/200] [Batch 2905/3166] [D loss: 0.824553] [G loss: 4.476479] [gradient_penalty: 0.007691]\n",
            "[Epoch 1/200] [Batch 2906/3166] [D loss: 1.459884] [G loss: 4.842247] [gradient_penalty: 0.007151]\n",
            "[Epoch 1/200] [Batch 2907/3166] [D loss: 1.529685] [G loss: 3.743938] [gradient_penalty: 0.006246]\n",
            "[Epoch 1/200] [Batch 2908/3166] [D loss: 1.885992] [G loss: 2.535220] [gradient_penalty: 0.005901]\n",
            "[Epoch 1/200] [Batch 2909/3166] [D loss: 1.996901] [G loss: 3.799876] [gradient_penalty: 0.007393]\n",
            "[Epoch 1/200] [Batch 2910/3166] [D loss: 3.074665] [G loss: 3.481233] [gradient_penalty: 0.008698]\n",
            "[Epoch 1/200] [Batch 2911/3166] [D loss: 2.694873] [G loss: 2.719161] [gradient_penalty: 0.006807]\n",
            "[Epoch 1/200] [Batch 2912/3166] [D loss: 2.887266] [G loss: 1.739436] [gradient_penalty: 0.007008]\n",
            "[Epoch 1/200] [Batch 2913/3166] [D loss: 1.925103] [G loss: 2.964446] [gradient_penalty: 0.007436]\n",
            "[Epoch 1/200] [Batch 2914/3166] [D loss: 2.686592] [G loss: 2.552627] [gradient_penalty: 0.008949]\n",
            "[Epoch 1/200] [Batch 2915/3166] [D loss: 1.477228] [G loss: 2.896677] [gradient_penalty: 0.007632]\n",
            "[Epoch 1/200] [Batch 2916/3166] [D loss: 2.233907] [G loss: 2.519393] [gradient_penalty: 0.008440]\n",
            "[Epoch 1/200] [Batch 2917/3166] [D loss: 1.712126] [G loss: 2.846530] [gradient_penalty: 0.006306]\n",
            "[Epoch 1/200] [Batch 2918/3166] [D loss: 1.670642] [G loss: 3.317499] [gradient_penalty: 0.005861]\n",
            "[Epoch 1/200] [Batch 2919/3166] [D loss: 0.610428] [G loss: 3.362389] [gradient_penalty: 0.006274]\n",
            "[Epoch 1/200] [Batch 2920/3166] [D loss: 0.589553] [G loss: 4.285633] [gradient_penalty: 0.006528]\n",
            "[Epoch 1/200] [Batch 2921/3166] [D loss: -0.117488] [G loss: 4.690623] [gradient_penalty: 0.007180]\n",
            "[Epoch 1/200] [Batch 2922/3166] [D loss: 0.123519] [G loss: 5.160876] [gradient_penalty: 0.007298]\n",
            "[Epoch 1/200] [Batch 2923/3166] [D loss: -1.631709] [G loss: 5.408526] [gradient_penalty: 0.007517]\n",
            "[Epoch 1/200] [Batch 2924/3166] [D loss: -1.630615] [G loss: 6.184152] [gradient_penalty: 0.005767]\n",
            "[Epoch 1/200] [Batch 2925/3166] [D loss: -0.488281] [G loss: 6.094082] [gradient_penalty: 0.006945]\n",
            "[Epoch 1/200] [Batch 2926/3166] [D loss: -2.924650] [G loss: 6.762604] [gradient_penalty: 0.006437]\n",
            "[Epoch 1/200] [Batch 2927/3166] [D loss: -2.220463] [G loss: 7.307899] [gradient_penalty: 0.006489]\n",
            "[Epoch 1/200] [Batch 2928/3166] [D loss: -0.664662] [G loss: 8.122074] [gradient_penalty: 0.011038]\n",
            "[Epoch 1/200] [Batch 2929/3166] [D loss: -2.927298] [G loss: 8.223766] [gradient_penalty: 0.007995]\n",
            "[Epoch 1/200] [Batch 2930/3166] [D loss: -2.560415] [G loss: 8.485657] [gradient_penalty: 0.011392]\n",
            "[Epoch 1/200] [Batch 2931/3166] [D loss: -2.204113] [G loss: 8.330036] [gradient_penalty: 0.009157]\n",
            "[Epoch 1/200] [Batch 2932/3166] [D loss: -1.103289] [G loss: 7.234222] [gradient_penalty: 0.010104]\n",
            "[Epoch 1/200] [Batch 2933/3166] [D loss: -1.458232] [G loss: 8.110928] [gradient_penalty: 0.009838]\n",
            "[Epoch 1/200] [Batch 2934/3166] [D loss: -0.119285] [G loss: 6.402203] [gradient_penalty: 0.009277]\n",
            "log\n",
            "[Epoch 1/200] [Batch 2935/3166] [D loss: 0.560115] [G loss: 6.184324] [gradient_penalty: 0.008690]\n",
            "[Epoch 1/200] [Batch 2936/3166] [D loss: 1.432858] [G loss: 5.794682] [gradient_penalty: 0.010364]\n",
            "[Epoch 1/200] [Batch 2937/3166] [D loss: 0.152871] [G loss: 5.594030] [gradient_penalty: 0.008947]\n",
            "[Epoch 1/200] [Batch 2938/3166] [D loss: 1.337409] [G loss: 4.548299] [gradient_penalty: 0.007363]\n",
            "[Epoch 1/200] [Batch 2939/3166] [D loss: 1.896883] [G loss: 5.303903] [gradient_penalty: 0.011252]\n",
            "[Epoch 1/200] [Batch 2940/3166] [D loss: 1.782077] [G loss: 5.024038] [gradient_penalty: 0.007292]\n",
            "[Epoch 1/200] [Batch 2941/3166] [D loss: 2.994180] [G loss: 3.887251] [gradient_penalty: 0.009704]\n",
            "[Epoch 1/200] [Batch 2942/3166] [D loss: 2.839539] [G loss: 3.505093] [gradient_penalty: 0.010715]\n",
            "[Epoch 1/200] [Batch 2943/3166] [D loss: 2.711791] [G loss: 3.666539] [gradient_penalty: 0.006025]\n",
            "[Epoch 1/200] [Batch 2944/3166] [D loss: 2.550282] [G loss: 2.650823] [gradient_penalty: 0.006396]\n",
            "[Epoch 1/200] [Batch 2945/3166] [D loss: 1.693896] [G loss: 3.448931] [gradient_penalty: 0.010943]\n",
            "[Epoch 1/200] [Batch 2946/3166] [D loss: 2.929524] [G loss: 3.619397] [gradient_penalty: 0.006344]\n",
            "[Epoch 1/200] [Batch 2947/3166] [D loss: 2.174404] [G loss: 3.419265] [gradient_penalty: 0.006969]\n",
            "[Epoch 1/200] [Batch 2948/3166] [D loss: 2.257425] [G loss: 3.455536] [gradient_penalty: 0.008854]\n",
            "[Epoch 1/200] [Batch 2949/3166] [D loss: 2.437076] [G loss: 3.243504] [gradient_penalty: 0.007554]\n",
            "[Epoch 1/200] [Batch 2950/3166] [D loss: 0.799386] [G loss: 3.746485] [gradient_penalty: 0.006706]\n",
            "[Epoch 1/200] [Batch 2951/3166] [D loss: 1.059515] [G loss: 3.729709] [gradient_penalty: 0.004288]\n",
            "[Epoch 1/200] [Batch 2952/3166] [D loss: 1.054824] [G loss: 3.492673] [gradient_penalty: 0.005825]\n",
            "[Epoch 1/200] [Batch 2953/3166] [D loss: 0.272106] [G loss: 4.150872] [gradient_penalty: 0.006301]\n",
            "[Epoch 1/200] [Batch 2954/3166] [D loss: -1.429501] [G loss: 4.480239] [gradient_penalty: 0.006254]\n",
            "[Epoch 1/200] [Batch 2955/3166] [D loss: -0.776940] [G loss: 4.966012] [gradient_penalty: 0.008228]\n",
            "[Epoch 1/200] [Batch 2956/3166] [D loss: -1.023476] [G loss: 5.063106] [gradient_penalty: 0.007005]\n",
            "[Epoch 1/200] [Batch 2957/3166] [D loss: -2.070428] [G loss: 6.057382] [gradient_penalty: 0.006991]\n",
            "[Epoch 1/200] [Batch 2958/3166] [D loss: -2.324296] [G loss: 5.310870] [gradient_penalty: 0.006754]\n",
            "[Epoch 1/200] [Batch 2959/3166] [D loss: -2.964684] [G loss: 5.392895] [gradient_penalty: 0.006233]\n",
            "[Epoch 1/200] [Batch 2960/3166] [D loss: -2.615030] [G loss: 6.053047] [gradient_penalty: 0.010709]\n",
            "[Epoch 1/200] [Batch 2961/3166] [D loss: -2.957862] [G loss: 5.562503] [gradient_penalty: 0.006976]\n",
            "[Epoch 1/200] [Batch 2962/3166] [D loss: -2.317940] [G loss: 5.372247] [gradient_penalty: 0.008616]\n",
            "[Epoch 1/200] [Batch 2963/3166] [D loss: -2.110257] [G loss: 4.735045] [gradient_penalty: 0.008542]\n",
            "[Epoch 1/200] [Batch 2964/3166] [D loss: -1.563416] [G loss: 4.757507] [gradient_penalty: 0.009236]\n",
            "[Epoch 1/200] [Batch 2965/3166] [D loss: -1.758799] [G loss: 3.688358] [gradient_penalty: 0.008366]\n",
            "[Epoch 1/200] [Batch 2966/3166] [D loss: 1.086364] [G loss: 2.751450] [gradient_penalty: 0.007950]\n",
            "[Epoch 1/200] [Batch 2967/3166] [D loss: 0.281800] [G loss: 3.973149] [gradient_penalty: 0.008918]\n",
            "[Epoch 1/200] [Batch 2968/3166] [D loss: 1.127100] [G loss: 2.442542] [gradient_penalty: 0.005971]\n",
            "[Epoch 1/200] [Batch 2969/3166] [D loss: 1.065461] [G loss: 1.509366] [gradient_penalty: 0.010849]\n",
            "[Epoch 1/200] [Batch 2970/3166] [D loss: 2.129974] [G loss: 1.377366] [gradient_penalty: 0.006951]\n",
            "[Epoch 1/200] [Batch 2971/3166] [D loss: 2.901945] [G loss: 0.319259] [gradient_penalty: 0.007933]\n",
            "[Epoch 1/200] [Batch 2972/3166] [D loss: 4.512566] [G loss: 0.319680] [gradient_penalty: 0.046224]\n",
            "[Epoch 1/200] [Batch 2973/3166] [D loss: 3.031042] [G loss: 0.650177] [gradient_penalty: 0.008067]\n",
            "[Epoch 1/200] [Batch 2974/3166] [D loss: 3.312518] [G loss: 1.297581] [gradient_penalty: 0.007088]\n",
            "[Epoch 1/200] [Batch 2975/3166] [D loss: 3.027683] [G loss: 0.915407] [gradient_penalty: 0.008314]\n",
            "[Epoch 1/200] [Batch 2976/3166] [D loss: 2.881386] [G loss: 0.591862] [gradient_penalty: 0.008152]\n",
            "[Epoch 1/200] [Batch 2977/3166] [D loss: 2.658285] [G loss: -0.059470] [gradient_penalty: 0.007803]\n",
            "[Epoch 1/200] [Batch 2978/3166] [D loss: 1.437503] [G loss: 0.072107] [gradient_penalty: 0.005996]\n",
            "[Epoch 1/200] [Batch 2979/3166] [D loss: 2.101394] [G loss: 1.462598] [gradient_penalty: 0.006377]\n",
            "[Epoch 1/200] [Batch 2980/3166] [D loss: 0.373443] [G loss: 1.557541] [gradient_penalty: 0.005777]\n",
            "[Epoch 1/200] [Batch 2981/3166] [D loss: 0.997763] [G loss: 2.352315] [gradient_penalty: 0.007521]\n",
            "[Epoch 1/200] [Batch 2982/3166] [D loss: 1.061643] [G loss: 2.335836] [gradient_penalty: 0.008713]\n",
            "[Epoch 1/200] [Batch 2983/3166] [D loss: -0.102033] [G loss: 3.425999] [gradient_penalty: 0.005577]\n",
            "[Epoch 1/200] [Batch 2984/3166] [D loss: 0.138347] [G loss: 3.751036] [gradient_penalty: 0.006784]\n",
            "[Epoch 1/200] [Batch 2985/3166] [D loss: -0.575034] [G loss: 4.582572] [gradient_penalty: 0.006430]\n",
            "[Epoch 1/200] [Batch 2986/3166] [D loss: -1.057665] [G loss: 4.015649] [gradient_penalty: 0.005310]\n",
            "[Epoch 1/200] [Batch 2987/3166] [D loss: -1.642874] [G loss: 4.960331] [gradient_penalty: 0.008263]\n",
            "[Epoch 1/200] [Batch 2988/3166] [D loss: -1.115834] [G loss: 5.505373] [gradient_penalty: 0.010742]\n",
            "[Epoch 1/200] [Batch 2989/3166] [D loss: -3.112456] [G loss: 6.002699] [gradient_penalty: 0.007146]\n",
            "[Epoch 1/200] [Batch 2990/3166] [D loss: -0.485821] [G loss: 7.169333] [gradient_penalty: 0.008254]\n",
            "[Epoch 1/200] [Batch 2991/3166] [D loss: -2.310687] [G loss: 6.064278] [gradient_penalty: 0.009486]\n",
            "[Epoch 1/200] [Batch 2992/3166] [D loss: -2.765735] [G loss: 6.344815] [gradient_penalty: 0.008758]\n",
            "[Epoch 1/200] [Batch 2993/3166] [D loss: -2.550855] [G loss: 7.051688] [gradient_penalty: 0.007475]\n",
            "[Epoch 1/200] [Batch 2994/3166] [D loss: -0.836796] [G loss: 6.748204] [gradient_penalty: 0.010610]\n",
            "[Epoch 1/200] [Batch 2995/3166] [D loss: -0.080568] [G loss: 6.663370] [gradient_penalty: 0.009049]\n",
            "[Epoch 1/200] [Batch 2996/3166] [D loss: -0.192616] [G loss: 5.587172] [gradient_penalty: 0.012301]\n",
            "[Epoch 1/200] [Batch 2997/3166] [D loss: -0.094536] [G loss: 4.500359] [gradient_penalty: 0.008454]\n",
            "[Epoch 1/200] [Batch 2998/3166] [D loss: 0.488995] [G loss: 5.349414] [gradient_penalty: 0.006591]\n",
            "[Epoch 1/200] [Batch 2999/3166] [D loss: 1.219577] [G loss: 4.344533] [gradient_penalty: 0.007269]\n",
            "[Epoch 1/200] [Batch 3000/3166] [D loss: 1.331786] [G loss: 3.788909] [gradient_penalty: 0.007810]\n",
            "[Epoch 1/200] [Batch 3001/3166] [D loss: 0.619210] [G loss: 3.788193] [gradient_penalty: 0.009469]\n",
            "[Epoch 1/200] [Batch 3002/3166] [D loss: 0.203007] [G loss: 2.566960] [gradient_penalty: 0.007953]\n",
            "[Epoch 1/200] [Batch 3003/3166] [D loss: 2.029561] [G loss: 2.926003] [gradient_penalty: 0.009478]\n",
            "[Epoch 1/200] [Batch 3004/3166] [D loss: 3.399858] [G loss: 2.350371] [gradient_penalty: 0.007651]\n",
            "[Epoch 1/200] [Batch 3005/3166] [D loss: 1.442867] [G loss: 1.759961] [gradient_penalty: 0.005809]\n",
            "[Epoch 1/200] [Batch 3006/3166] [D loss: 2.020764] [G loss: 1.285256] [gradient_penalty: 0.008176]\n",
            "[Epoch 1/200] [Batch 3007/3166] [D loss: 2.753072] [G loss: 1.421412] [gradient_penalty: 0.007146]\n",
            "[Epoch 1/200] [Batch 3008/3166] [D loss: 3.294337] [G loss: 1.332425] [gradient_penalty: 0.007825]\n",
            "[Epoch 1/200] [Batch 3009/3166] [D loss: 1.993011] [G loss: 0.858051] [gradient_penalty: 0.007962]\n",
            "[Epoch 1/200] [Batch 3010/3166] [D loss: 2.971222] [G loss: 1.448084] [gradient_penalty: 0.008149]\n",
            "[Epoch 1/200] [Batch 3011/3166] [D loss: 2.478152] [G loss: 1.417675] [gradient_penalty: 0.008628]\n",
            "[Epoch 1/200] [Batch 3012/3166] [D loss: 0.905411] [G loss: 1.336366] [gradient_penalty: 0.007555]\n",
            "[Epoch 1/200] [Batch 3013/3166] [D loss: 1.764999] [G loss: 1.571887] [gradient_penalty: 0.007325]\n",
            "[Epoch 1/200] [Batch 3014/3166] [D loss: 0.728323] [G loss: 2.314174] [gradient_penalty: 0.005033]\n",
            "[Epoch 1/200] [Batch 3015/3166] [D loss: 0.493656] [G loss: 1.566708] [gradient_penalty: 0.006031]\n",
            "[Epoch 1/200] [Batch 3016/3166] [D loss: -0.090035] [G loss: 2.239300] [gradient_penalty: 0.008150]\n",
            "[Epoch 1/200] [Batch 3017/3166] [D loss: -0.676669] [G loss: 2.496358] [gradient_penalty: 0.007401]\n",
            "[Epoch 1/200] [Batch 3018/3166] [D loss: -1.336708] [G loss: 3.752986] [gradient_penalty: 0.009482]\n",
            "[Epoch 1/200] [Batch 3019/3166] [D loss: -1.872981] [G loss: 2.388242] [gradient_penalty: 0.004501]\n",
            "[Epoch 1/200] [Batch 3020/3166] [D loss: -1.737117] [G loss: 2.906260] [gradient_penalty: 0.006367]\n",
            "[Epoch 1/200] [Batch 3021/3166] [D loss: -1.330738] [G loss: 2.042786] [gradient_penalty: 0.008228]\n",
            "[Epoch 1/200] [Batch 3022/3166] [D loss: -1.932032] [G loss: 2.273205] [gradient_penalty: 0.008370]\n",
            "[Epoch 1/200] [Batch 3023/3166] [D loss: -2.323969] [G loss: 2.649676] [gradient_penalty: 0.007113]\n",
            "[Epoch 1/200] [Batch 3024/3166] [D loss: 0.153042] [G loss: 1.179583] [gradient_penalty: 0.010131]\n",
            "[Epoch 1/200] [Batch 3025/3166] [D loss: 0.032206] [G loss: 1.806827] [gradient_penalty: 0.005315]\n",
            "[Epoch 1/200] [Batch 3026/3166] [D loss: 1.323227] [G loss: 0.933678] [gradient_penalty: 0.015457]\n",
            "[Epoch 1/200] [Batch 3027/3166] [D loss: 0.166371] [G loss: 0.253599] [gradient_penalty: 0.006903]\n",
            "[Epoch 1/200] [Batch 3028/3166] [D loss: 1.127147] [G loss: 0.172949] [gradient_penalty: 0.007289]\n",
            "[Epoch 1/200] [Batch 3029/3166] [D loss: 1.174300] [G loss: 0.064248] [gradient_penalty: 0.006276]\n",
            "[Epoch 1/200] [Batch 3030/3166] [D loss: 2.523697] [G loss: -0.240169] [gradient_penalty: 0.007673]\n",
            "[Epoch 1/200] [Batch 3031/3166] [D loss: 1.644218] [G loss: -0.568790] [gradient_penalty: 0.005813]\n",
            "[Epoch 1/200] [Batch 3032/3166] [D loss: 1.409923] [G loss: -0.223927] [gradient_penalty: 0.006743]\n",
            "[Epoch 1/200] [Batch 3033/3166] [D loss: 2.403145] [G loss: -0.581236] [gradient_penalty: 0.007873]\n",
            "[Epoch 1/200] [Batch 3034/3166] [D loss: 1.541705] [G loss: -0.121818] [gradient_penalty: 0.006335]\n",
            "log\n",
            "[Epoch 1/200] [Batch 3035/3166] [D loss: 1.502220] [G loss: 0.158165] [gradient_penalty: 0.007567]\n",
            "[Epoch 1/200] [Batch 3036/3166] [D loss: 1.871794] [G loss: 0.850710] [gradient_penalty: 0.006998]\n",
            "[Epoch 1/200] [Batch 3037/3166] [D loss: 1.424991] [G loss: 0.990022] [gradient_penalty: 0.004776]\n",
            "[Epoch 1/200] [Batch 3038/3166] [D loss: 1.645879] [G loss: 1.183674] [gradient_penalty: 0.007757]\n",
            "[Epoch 1/200] [Batch 3039/3166] [D loss: 1.256209] [G loss: 1.558335] [gradient_penalty: 0.008293]\n",
            "[Epoch 1/200] [Batch 3040/3166] [D loss: 0.665517] [G loss: 1.901885] [gradient_penalty: 0.006587]\n",
            "[Epoch 1/200] [Batch 3041/3166] [D loss: 1.372024] [G loss: 1.998081] [gradient_penalty: 0.007397]\n",
            "[Epoch 1/200] [Batch 3042/3166] [D loss: 0.254287] [G loss: 1.825697] [gradient_penalty: 0.006889]\n",
            "[Epoch 1/200] [Batch 3043/3166] [D loss: 0.354368] [G loss: 2.953518] [gradient_penalty: 0.007345]\n",
            "[Epoch 1/200] [Batch 3044/3166] [D loss: 0.096073] [G loss: 3.058841] [gradient_penalty: 0.006433]\n",
            "[Epoch 1/200] [Batch 3045/3166] [D loss: 0.080211] [G loss: 3.204462] [gradient_penalty: 0.006122]\n",
            "[Epoch 1/200] [Batch 3046/3166] [D loss: -1.027318] [G loss: 3.207342] [gradient_penalty: 0.006681]\n",
            "[Epoch 1/200] [Batch 3047/3166] [D loss: -1.332062] [G loss: 4.608425] [gradient_penalty: 0.004135]\n",
            "[Epoch 1/200] [Batch 3048/3166] [D loss: -1.374349] [G loss: 3.748620] [gradient_penalty: 0.008578]\n",
            "[Epoch 1/200] [Batch 3049/3166] [D loss: -0.325246] [G loss: 4.922874] [gradient_penalty: 0.007110]\n",
            "[Epoch 1/200] [Batch 3050/3166] [D loss: -0.605880] [G loss: 4.730971] [gradient_penalty: 0.005993]\n",
            "[Epoch 1/200] [Batch 3051/3166] [D loss: -2.290788] [G loss: 4.777490] [gradient_penalty: 0.005851]\n",
            "[Epoch 1/200] [Batch 3052/3166] [D loss: -2.047078] [G loss: 4.598007] [gradient_penalty: 0.007835]\n",
            "[Epoch 1/200] [Batch 3053/3166] [D loss: -1.463661] [G loss: 4.682413] [gradient_penalty: 0.006829]\n",
            "[Epoch 1/200] [Batch 3054/3166] [D loss: -1.273950] [G loss: 4.471866] [gradient_penalty: 0.005772]\n",
            "[Epoch 1/200] [Batch 3055/3166] [D loss: -1.119067] [G loss: 4.605443] [gradient_penalty: 0.009089]\n",
            "[Epoch 1/200] [Batch 3056/3166] [D loss: -0.712266] [G loss: 4.150537] [gradient_penalty: 0.005862]\n",
            "[Epoch 1/200] [Batch 3057/3166] [D loss: -0.611073] [G loss: 4.306894] [gradient_penalty: 0.005899]\n",
            "[Epoch 1/200] [Batch 3058/3166] [D loss: -0.757334] [G loss: 2.856688] [gradient_penalty: 0.006913]\n",
            "[Epoch 1/200] [Batch 3059/3166] [D loss: -0.186905] [G loss: 3.108291] [gradient_penalty: 0.007312]\n",
            "[Epoch 1/200] [Batch 3060/3166] [D loss: 1.343117] [G loss: 2.770340] [gradient_penalty: 0.005285]\n",
            "[Epoch 1/200] [Batch 3061/3166] [D loss: 1.774540] [G loss: 2.125479] [gradient_penalty: 0.009176]\n",
            "[Epoch 1/200] [Batch 3062/3166] [D loss: 2.803680] [G loss: 1.818702] [gradient_penalty: 0.006238]\n",
            "[Epoch 1/200] [Batch 3063/3166] [D loss: 2.929400] [G loss: 1.324945] [gradient_penalty: 0.007203]\n",
            "[Epoch 1/200] [Batch 3064/3166] [D loss: 2.369645] [G loss: 2.149391] [gradient_penalty: 0.010844]\n",
            "[Epoch 1/200] [Batch 3065/3166] [D loss: 1.493506] [G loss: 2.213676] [gradient_penalty: 0.006640]\n",
            "[Epoch 1/200] [Batch 3066/3166] [D loss: 1.635815] [G loss: 1.819588] [gradient_penalty: 0.006744]\n",
            "[Epoch 1/200] [Batch 3067/3166] [D loss: 2.037936] [G loss: 1.458082] [gradient_penalty: 0.009731]\n",
            "[Epoch 1/200] [Batch 3068/3166] [D loss: 1.299406] [G loss: 2.351294] [gradient_penalty: 0.006890]\n"
          ]
        }
      ],
      "source": [
        "# Tạo noise cố định để theo dõi chất lượng qua các epoch\n",
        "z_fixed = torch.randn(25, opt.latent_dim).type(Tensor)\n",
        "\n",
        "lambda_gp = 50\n",
        "n_critic = 1  # số lần train D mỗi batch của G\n",
        "\n",
        "# Vòng lặp training\n",
        "for epoch in range(opt.n_epochs):\n",
        "    for i, (imgs, _) in enumerate(dataloader):\n",
        "\n",
        "        # Lưu ảnh mẫu sau mỗi batches được xử lí\n",
        "        batches_done = epoch * len(dataloader) + i\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        # Chuẩn bị nhãn và dữ liệu trên device\n",
        "        # valid = torch.full((imgs.size(0), 1), 0.9, device=device)\n",
        "        # fake  = torch.zeros((imgs.size(0), 1), device=device)\n",
        "        real_imgs = imgs.to(device)\n",
        "        batch_size = real_imgs.size(0)\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ------------------- --\n",
        "        for _ in range(n_critic):\n",
        "            optimizer_D.zero_grad()\n",
        "\n",
        "            z = torch.randn(batch_size, opt.latent_dim, device=device)\n",
        "            fake_imgs = generator(z).detach()\n",
        "\n",
        "            real_validity = discriminator(real_imgs)\n",
        "            fake_validity = discriminator(fake_imgs)\n",
        "\n",
        "            gradient_penalty = compute_gradient_penalty(discriminator, real_imgs, fake_imgs)\n",
        "            d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
        "\n",
        "            d_loss.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "        # -----------------\n",
        "        #  Train Generator\n",
        "        # -----------------\n",
        "        for _ in range(2):\n",
        "            optimizer_G.zero_grad()\n",
        "\n",
        "            z = torch.randn(batch_size, opt.latent_dim, device=device)\n",
        "            gen_imgs = generator(z)\n",
        "            g_loss = -torch.mean(discriminator(gen_imgs))\n",
        "\n",
        "            g_loss.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "        # In ra thông tin về quá trình huẩn luyện\n",
        "        print(\n",
        "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f] [gradient_penalty: %f]\"\n",
        "            % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item(), gradient_penalty.item(),)\n",
        "        )\n",
        "\n",
        "        if g_loss.item() > 50:\n",
        "            n_critic = 1  # Train D ít hơn khi G struggle\n",
        "        else:\n",
        "            n_critic = 3\n",
        "\n",
        "        # Wandb log loss\n",
        "        wandb.log(\n",
        "            {\n",
        "                \"loss/D\": d_loss.item(),\n",
        "                \"loss/G\": g_loss.item(),\n",
        "                \"real_loss\": torch.mean(real_validity),\n",
        "                \"fake_loss\": torch.mean(fake_validity),\n",
        "                \"gradient_penalty\": gradient_penalty.item(),\n",
        "                \"epoch\": epoch,\n",
        "            },\n",
        "            step=epoch * len(dataloader) + i,\n",
        "        )\n",
        "\n",
        "        if batches_done % opt.sample_interval == 0:\n",
        "            # Lưu ảnh vào local storage\n",
        "            save_image(gen_imgs.data[:25], \"images/%d.png\" % batches_done, nrow=5, normalize=True)\n",
        "\n",
        "            # Log grid của ảnh được gen hiện tại\n",
        "            gen_imgs_vis = (gen_imgs + 1) / 2  # [-1,1] -> [0,1]\n",
        "            # gen_imgs_vis = gen_imgs\n",
        "            grid_current = make_grid(gen_imgs_vis[:25], nrow=5)\n",
        "            wandb.log({\n",
        "                \"samples/current_batch\": wandb.Image(grid_current, caption=f\"step {batches_done}\")\n",
        "            }, step=batches_done)\n",
        "\n",
        "            # 4.3) Log grid ảnh từ z cố định (giúp so sánh chất lượng qua thời gian)\n",
        "            with torch.no_grad():\n",
        "                gen_fixed = generator(z_fixed)\n",
        "            gen_fixed_vis = (gen_fixed + 1)/2\n",
        "            # gen_fixed_vis = gen_fixed\n",
        "            grid_fixed = make_grid(gen_fixed_vis, nrow=5)\n",
        "            wandb.log({\n",
        "                \"samples/fixed_noise\": wandb.Image(grid_fixed, caption=f\"fixed z @ step {batches_done}\")\n",
        "            }, step=batches_done)\n",
        "            print(\"log\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}