{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T15:07:42.156219Z",
     "iopub.status.busy": "2025-08-17T15:07:42.155653Z",
     "iopub.status.idle": "2025-08-17T15:07:42.161868Z",
     "shell.execute_reply": "2025-08-17T15:07:42.161108Z",
     "shell.execute_reply.started": "2025-08-17T15:07:42.156195Z"
    },
    "id": "3ihqLA1E4xPb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import cÃ¡c library, classs vÃ  hÃ m cáº§n thiáº¿t\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-08-17T15:07:42.168335Z",
     "iopub.status.busy": "2025-08-17T15:07:42.168085Z",
     "iopub.status.idle": "2025-08-17T15:07:45.402449Z",
     "shell.execute_reply": "2025-08-17T15:07:45.401731Z",
     "shell.execute_reply.started": "2025-08-17T15:07:42.168307Z"
    },
    "id": "K-YojFz25HBj",
    "outputId": "85e9faa1-00f1-4317-aab8-ef0f2efb22c1",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.20.1)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from wandb) (25.0)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (7.0.0)\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.4)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.14.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.6.15)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CÃ i vÃ  import wandb\n",
    "!pip install wandb\n",
    "import wandb\n",
    "# GÃ¡n key trá»±c tiáº¿p\n",
    "os.environ[\"WANDB_API_KEY\"] = \"...\"\n",
    "\n",
    "# Login tá»± Ä‘á»™ng\n",
    "wandb.login(key=os.environ[\"WANDB_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T15:07:45.404323Z",
     "iopub.status.busy": "2025-08-17T15:07:45.404061Z",
     "iopub.status.idle": "2025-08-17T15:07:45.409072Z",
     "shell.execute_reply": "2025-08-17T15:07:45.408351Z",
     "shell.execute_reply.started": "2025-08-17T15:07:45.404298Z"
    },
    "id": "w3C0-C015Vr7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Download Input\n",
    "# import kagglehub\n",
    "# path = kagglehub.dataset_download(\"jessicali9530/celeba-dataset\")\n",
    "# print(\"Path to dataset files:\", path)\n",
    "# !cp -r {path} ./celeba-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T15:07:45.409988Z",
     "iopub.status.busy": "2025-08-17T15:07:45.409705Z",
     "iopub.status.idle": "2025-08-17T15:07:45.421191Z",
     "shell.execute_reply": "2025-08-17T15:07:45.420564Z",
     "shell.execute_reply.started": "2025-08-17T15:07:45.409965Z"
    },
    "id": "gwfCjiSI6ySq",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Download Output\n",
    "# import kagglehub\n",
    "# path = kagglehub.dataset_download(\"romaingraux/bitmojis\")\n",
    "# print(\"Path to dataset files:\", path)\n",
    "# !cp -r {path} ./bitmojis-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T15:07:45.423267Z",
     "iopub.status.busy": "2025-08-17T15:07:45.423027Z",
     "iopub.status.idle": "2025-08-17T15:07:45.436193Z",
     "shell.execute_reply": "2025-08-17T15:07:45.435501Z",
     "shell.execute_reply.started": "2025-08-17T15:07:45.423252Z"
    },
    "id": "p7OgslLnGtG2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "celeba_root = '/kaggle/input/celebahq-resized-256x256/celeba_hq_256'\n",
    "bitmoji_root = '/kaggle/input/bitmojis/bitmojis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-08-17T15:07:45.437160Z",
     "iopub.status.busy": "2025-08-17T15:07:45.436981Z",
     "iopub.status.idle": "2025-08-17T15:07:45.451988Z",
     "shell.execute_reply": "2025-08-17T15:07:45.451282Z",
     "shell.execute_reply.started": "2025-08-17T15:07:45.437139Z"
    },
    "id": "0yvEemc97UM6",
    "outputId": "a6450565-0846-4caa-d9ee-7dc6d2229429",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(n_epochs=10, batch_size=16, lr=0.0002, b1=0.5, b2=0.999, n_cpu=8, latent_dim=100, img_size=224, channels=3, sample_interval=50)\n"
     ]
    }
   ],
   "source": [
    "# Thiáº¿t láº­p cÃ¡c tham sá»‘ cho mÃ´ hÃ¬nh GAN\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=10, help=\"number of epochs of training\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=16, help=\"size of the batches\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\n",
    "parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\n",
    "parser.add_argument(\"--latent_dim\", type=int, default=100, help=\"dimensionality of the latent space\")\n",
    "parser.add_argument(\"--img_size\", type=int, default=224, help=\"size of each image dimension\")\n",
    "parser.add_argument(\"--channels\", type=int, default=3, help=\"number of image channels\")\n",
    "parser.add_argument(\"--sample_interval\", type=int, default=50, help=\"interval between image sampling\")\n",
    "opt = parser.parse_args(args=[])\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "execution": {
     "iopub.execute_input": "2025-08-17T15:07:45.452924Z",
     "iopub.status.busy": "2025-08-17T15:07:45.452730Z",
     "iopub.status.idle": "2025-08-17T15:07:52.031871Z",
     "shell.execute_reply": "2025-08-17T15:07:52.031242Z",
     "shell.execute_reply.started": "2025-08-17T15:07:45.452900Z"
    },
    "id": "1WdvqPbm6bo2",
    "outputId": "a96bec2d-2940-4f9c-e13b-b103aba30868",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>loss/CONST</td><td>â–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>loss/D</td><td>â–ˆâ–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–</td></tr><tr><td>loss/G</td><td>â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>loss/GAN</td><td>â–ƒâ–„â–â–ƒâ–‚â–…â–ˆâ–†â–†â–…â–…â–…â–…â–†â–…â–†â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–…â–…â–…â–…â–…â–…â–…</td></tr><tr><td>loss/TID</td><td>â–ˆâ–…â–…â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–‚â–â–â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>loss/TV</td><td>â–ˆâ–…â–…â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>loss/CONST</td><td>0.00047</td></tr><tr><td>loss/D</td><td>3.2959</td></tr><tr><td>loss/G</td><td>2.36442</td></tr><tr><td>loss/GAN</td><td>2.19717</td></tr><tr><td>loss/TID</td><td>0.11188</td></tr><tr><td>loss/TV</td><td>0.164</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Emoji Creation new</strong> at: <a href='https://wandb.ai/khuengv2007-institut-polytechnique-de-paris/DTN/runs/y9pntw4u' target=\"_blank\">https://wandb.ai/khuengv2007-institut-polytechnique-de-paris/DTN/runs/y9pntw4u</a><br> View project at: <a href='https://wandb.ai/khuengv2007-institut-polytechnique-de-paris/DTN' target=\"_blank\">https://wandb.ai/khuengv2007-institut-polytechnique-de-paris/DTN</a><br>Synced 5 W&B file(s), 72 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250817_145010-y9pntw4u/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250817_150745-f9inu44f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/khuengv2007-institut-polytechnique-de-paris/DTN/runs/f9inu44f' target=\"_blank\">Emoji Creation new</a></strong> to <a href='https://wandb.ai/khuengv2007-institut-polytechnique-de-paris/DTN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/khuengv2007-institut-polytechnique-de-paris/DTN' target=\"_blank\">https://wandb.ai/khuengv2007-institut-polytechnique-de-paris/DTN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/khuengv2007-institut-polytechnique-de-paris/DTN/runs/f9inu44f' target=\"_blank\">https://wandb.ai/khuengv2007-institut-polytechnique-de-paris/DTN/runs/f9inu44f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/khuengv2007-institut-polytechnique-de-paris/DTN/runs/f9inu44f?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7984c13598d0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project = \"DTN\",\n",
    "    name = \"Emoji Creation new\",\n",
    "    config = opt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T15:07:52.032978Z",
     "iopub.status.busy": "2025-08-17T15:07:52.032691Z",
     "iopub.status.idle": "2025-08-17T15:07:52.038828Z",
     "shell.execute_reply": "2025-08-17T15:07:52.038252Z",
     "shell.execute_reply.started": "2025-08-17T15:07:52.032955Z"
    },
    "id": "Tl62n6x_71Nf",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Táº¡o thÆ° má»¥c images Ä‘á»ƒ lÆ°u cÃ¡c áº£nh káº¿t quáº£\n",
    "\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "os.makedirs(\"samples\", exist_ok=True)\n",
    "\n",
    "\n",
    "# Thiáº¿t láº­p kÃ­ch thÆ°á»›c áº£nh vÃ  check xem cÃ³ sá»­ dá»¥ng GPU hay khÃ´ng\n",
    "\n",
    "img_shape = (opt.channels, opt.img_size, opt.img_size)\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T15:07:52.040222Z",
     "iopub.status.busy": "2025-08-17T15:07:52.039993Z",
     "iopub.status.idle": "2025-08-17T15:07:55.535598Z",
     "shell.execute_reply": "2025-08-17T15:07:55.534928Z",
     "shell.execute_reply.started": "2025-08-17T15:07:52.040204Z"
    },
    "id": "_Xu9Nr-U8wmt",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: facenet_pytorch in /usr/local/lib/python3.11/dist-packages (2.6.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from facenet_pytorch) (1.26.4)\n",
      "Requirement already satisfied: Pillow<10.3.0,>=10.2.0 in /usr/local/lib/python3.11/dist-packages (from facenet_pytorch) (10.2.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from facenet_pytorch) (2.32.4)\n",
      "Requirement already satisfied: torch<2.3.0,>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from facenet_pytorch) (2.2.2)\n",
      "Requirement already satisfied: torchvision<0.18.0,>=0.17.0 in /usr/local/lib/python3.11/dist-packages (from facenet_pytorch) (0.17.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from facenet_pytorch) (4.67.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.24.0->facenet_pytorch) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.24.0->facenet_pytorch) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.24.0->facenet_pytorch) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.24.0->facenet_pytorch) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.24.0->facenet_pytorch) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.24.0->facenet_pytorch) (2.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->facenet_pytorch) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->facenet_pytorch) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->facenet_pytorch) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->facenet_pytorch) (2025.6.15)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (4.14.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (2025.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<2.3.0,>=2.2.0->facenet_pytorch) (12.5.82)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<2.3.0,>=2.2.0->facenet_pytorch) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0.0,>=1.24.0->facenet_pytorch) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0.0,>=1.24.0->facenet_pytorch) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.0.0,>=1.24.0->facenet_pytorch) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.0.0,>=1.24.0->facenet_pytorch) (2024.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch<2.3.0,>=2.2.0->facenet_pytorch) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.0.0,>=1.24.0->facenet_pytorch) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install facenet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T15:07:55.536796Z",
     "iopub.status.busy": "2025-08-17T15:07:55.536579Z",
     "iopub.status.idle": "2025-08-17T15:07:55.542669Z",
     "shell.execute_reply": "2025-08-17T15:07:55.542018Z",
     "shell.execute_reply.started": "2025-08-17T15:07:55.536773Z"
    },
    "id": "h9pi5ffB8fVh",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "\n",
    "def create_face_extractor():\n",
    "    \"\"\"Simple FaceNet-based feature extractor\"\"\"\n",
    "    facenet = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "\n",
    "    # Freeze parameters\n",
    "    for param in facenet.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    return facenet  # Output: 512D features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T15:07:55.545729Z",
     "iopub.status.busy": "2025-08-17T15:07:55.545548Z",
     "iopub.status.idle": "2025-08-17T15:07:55.560949Z",
     "shell.execute_reply": "2025-08-17T15:07:55.560281Z",
     "shell.execute_reply.started": "2025-08-17T15:07:55.545715Z"
    },
    "id": "jyTMHSodDaFU",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=512, img_size=64):  # DTN specs\n",
    "        super(Generator, self).__init__()\n",
    "        self.init_size = img_size // 32  # Start from 2x2\n",
    "\n",
    "        # Project 512D features to spatial dimensions\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512 * self.init_size ** 2)\n",
    "        )\n",
    "\n",
    "        # DTN-style conv blocks (paper: 5 upscaling blocks + 1x1 convs)\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(512),\n",
    "\n",
    "            # Block 1: 2x2 -> 4x4\n",
    "            nn.ConvTranspose2d(512, 512, 4, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 1),  # 1x1 conv (paper mentions this)\n",
    "\n",
    "            # Block 2: 4x4 -> 8x8\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, 1),  # 1x1 conv\n",
    "\n",
    "            # Block 3: 8x8 -> 16x16\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, 1),  # 1x1 conv\n",
    "\n",
    "            # Block 4: 16x16 -> 32x32\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 1),  # 1x1 conv\n",
    "\n",
    "            # Block 5: 32x32 -> 64x64\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, 1),  # 1x1 conv\n",
    "\n",
    "            # Final layer\n",
    "            nn.Conv2d(32, 3, 3, stride=1, padding=1),  # RGB output\n",
    "            nn.Tanh(),  # [-1, 1] range\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        # z = face features from f network (batch, 512)\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 512, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img  # (batch, 3, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T15:07:55.562140Z",
     "iopub.status.busy": "2025-08-17T15:07:55.561860Z",
     "iopub.status.idle": "2025-08-17T15:07:55.577189Z",
     "shell.execute_reply": "2025-08-17T15:07:55.576626Z",
     "shell.execute_reply.started": "2025-08-17T15:07:55.562116Z"
    },
    "id": "RfZjI1fdEFmL",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_size=152):  # DTN target domain size\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            \"\"\"DTN-style: Conv + BatchNorm + LeakyReLU(0.2)\"\"\"\n",
    "            block = [\n",
    "                nn.Conv2d(in_filters, out_filters, 4, 2, 1),  # stride=2\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            ]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters))\n",
    "            return block\n",
    "\n",
    "        # 6 blocks (paper specification)\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(3, 64, bn=False),    # 152x152 -> 76x76\n",
    "            *discriminator_block(64, 128),            # 76x76 -> 38x38\n",
    "            *discriminator_block(128, 256),           # 38x38 -> 19x19\n",
    "            *discriminator_block(256, 512),           # 19x19 -> 9x9\n",
    "            *discriminator_block(512, 1024),          # 9x9 -> 4x4\n",
    "            *discriminator_block(1024, 2048),         # 4x4 -> 2x2\n",
    "        )\n",
    "\n",
    "        # Ternary classification (not binary!)\n",
    "        ds_size = img_size // (2 ** 6)  # After 6 blocks\n",
    "        self.adv_layer = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),  # Global average pooling\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2048, 3),       # 3 classes instead of 1\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        # img: (batch, 3, 152, 152)\n",
    "        out = self.model(img)\n",
    "        validity = self.adv_layer(out)\n",
    "        return validity  # (batch, 3) - probabilities for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T15:07:55.578069Z",
     "iopub.status.busy": "2025-08-17T15:07:55.577846Z",
     "iopub.status.idle": "2025-08-17T15:07:55.591431Z",
     "shell.execute_reply": "2025-08-17T15:07:55.590786Z",
     "shell.execute_reply.started": "2025-08-17T15:07:55.578053Z"
    },
    "id": "6jnkMAimEjT0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DTNLoss:\n",
    "    def __init__(self, alpha=300, beta=10, gamma=0.1):\n",
    "        self.alpha = alpha  # L_CONST weight (critical!)\n",
    "        self.beta = beta    # L_TID weight\n",
    "        self.gamma = gamma  # L_TV weight\n",
    "\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def discriminator_loss(self, D, g_outputs_s, g_outputs_t, x_t_real):\n",
    "        \"\"\"\n",
    "        L_D = -E[log D1(g(f(x_s)))] - E[log D2(g(f(x_t)))] - E[log D3(x_t_real)]\n",
    "        \"\"\"\n",
    "        batch_size = g_outputs_s.size(0)\n",
    "        device = g_outputs_s.device\n",
    "\n",
    "        # Class labels\n",
    "        label_s = torch.zeros(batch_size, dtype=torch.long, device=device)      # Class 0\n",
    "        label_t = torch.ones(batch_size, dtype=torch.long, device=device)       # Class 1\n",
    "        label_real = torch.full((batch_size,), 2, dtype=torch.long, device=device) # Class 2\n",
    "\n",
    "        # Discriminator predictions\n",
    "        d_s = D(g_outputs_s)\n",
    "        d_t = D(g_outputs_t)\n",
    "        d_real = D(x_t_real)\n",
    "\n",
    "        # Losses\n",
    "        loss_s = self.ce_loss(d_s, label_s)\n",
    "        loss_t = self.ce_loss(d_t, label_t)\n",
    "        loss_real = self.ce_loss(d_real, label_real)\n",
    "\n",
    "        return (loss_s + loss_t + loss_real)\n",
    "\n",
    "    def generator_loss(self, D, f, g, x_s, x_t, x_t_real):\n",
    "        \"\"\"\n",
    "        L_G = L_GAN_G + Î±*L_CONST + Î²*L_TID + Î³*L_TV\n",
    "        \"\"\"\n",
    "        batch_size = x_s.size(0)\n",
    "        device = x_s.device\n",
    "\n",
    "        # Forward pass\n",
    "        f_x_s = f(x_s)  # (batch, 512)\n",
    "        f_x_t = f(x_t)  # (batch, 512)\n",
    "\n",
    "        g_f_x_s = g(f_x_s)  # (batch, 3, 64, 64)\n",
    "        g_f_x_t = g(f_x_t)  # (batch, 3, 64, 64)\n",
    "\n",
    "        # Upscale for discriminator (64x64 -> 152x152)\n",
    "        g_f_x_s_up = F.interpolate(g_f_x_s, size=(152, 152), mode='bilinear')\n",
    "        g_f_x_t_up = F.interpolate(g_f_x_t, size=(152, 152), mode='bilinear')\n",
    "\n",
    "        # L_GAN_G: Fool discriminator (want class 2 = real)\n",
    "        target_real = torch.full((batch_size,), 2, dtype=torch.long, device=device)\n",
    "\n",
    "        d_s = D(g_f_x_s_up)\n",
    "        d_t = D(g_f_x_t_up)\n",
    "\n",
    "        loss_gan_s = self.ce_loss(d_s, target_real)\n",
    "        loss_gan_t = self.ce_loss(d_t, target_real)\n",
    "        loss_gan = loss_gan_s + loss_gan_t\n",
    "\n",
    "        # L_CONST: f-constancy\n",
    "        f_g_f_x_s = f(F.interpolate(g_f_x_s, size=(160, 160), mode='bilinear'))\n",
    "        loss_const = self.mse_loss(f_x_s, f_g_f_x_s)\n",
    "\n",
    "        # L_TID: Identity mapping for target domain\n",
    "        x_t_64 = F.interpolate(x_t, size=(64, 64), mode='bilinear')\n",
    "        loss_tid = self.mse_loss(x_t_64, g_f_x_t)\n",
    "\n",
    "        # L_TV: Total variation\n",
    "        loss_tv = self.total_variation_loss(g_f_x_s)\n",
    "\n",
    "        # Combine losses\n",
    "        total_loss = loss_gan + self.alpha * loss_const + self.beta * loss_tid + self.gamma * loss_tv\n",
    "\n",
    "        return total_loss, {\n",
    "            'gan': loss_gan.item(),\n",
    "            'const': loss_const.item(),\n",
    "            'tid': loss_tid.item(),\n",
    "            'tv': loss_tv.item()\n",
    "        }\n",
    "\n",
    "    def total_variation_loss(self, images):\n",
    "        \"\"\"Safe isotropic TV loss vá»›i epsilon\"\"\"\n",
    "        diff_h = torch.pow(images[:, :, :, 1:] - images[:, :, :, :-1], 2)\n",
    "        diff_v = torch.pow(images[:, :, 1:, :] - images[:, :, :-1, :], 2)\n",
    "        \n",
    "        # CRITICAL: Add epsilon before sqrt\n",
    "        diff_h_matched = diff_h[:, :, :-1, :]\n",
    "        diff_v_matched = diff_v[:, :, :, :-1]\n",
    "        gradient_magnitude = torch.sqrt(diff_h_matched + diff_v_matched + 1e-8)\n",
    "        \n",
    "        return torch.mean(gradient_magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T15:07:55.592375Z",
     "iopub.status.busy": "2025-08-17T15:07:55.592171Z",
     "iopub.status.idle": "2025-08-17T15:07:55.605597Z",
     "shell.execute_reply": "2025-08-17T15:07:55.604993Z",
     "shell.execute_reply.started": "2025-08-17T15:07:55.592359Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "def crop_face_tight(pil_image, target_size=(160, 160), padding_ratio=0.3):\n",
    "    \"\"\"Face cropping for source\"\"\"\n",
    "    try:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        margin = int(target_size[0] * padding_ratio)\n",
    "        detector = MTCNN(image_size=target_size, margin=margin, keep_all=False, device=device)\n",
    "        \n",
    "        face_tensor = detector(pil_image)\n",
    "        if face_tensor is not None:\n",
    "            face_array = face_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "            face_array = ((face_array + 1) * 127.5).clip(0, 255).astype(np.uint8)\n",
    "            return Image.fromarray(face_array)\n",
    "        else:\n",
    "            return generous_center_crop(pil_image, target_size, crop_ratio=0.7)\n",
    "    except Exception as e:\n",
    "        return generous_center_crop(pil_image, target_size, crop_ratio=0.7)\n",
    "\n",
    "def crop_face_emoji(pil_image, target_size=(152, 152), padding_ratio=0.25):\n",
    "    \"\"\"Face cropping for emoji (unchanged from before)\"\"\"\n",
    "    try:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        margin = int(target_size[0] * padding_ratio)\n",
    "        detector = MTCNN(image_size=target_size, margin=margin, keep_all=False, device=device)\n",
    "        \n",
    "        face_tensor = detector(pil_image)\n",
    "        if face_tensor is not None:\n",
    "            face_array = face_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "            face_array = ((face_array + 1) * 127.5).clip(0, 255).astype(np.uint8)\n",
    "            return Image.fromarray(face_array)\n",
    "        else:\n",
    "            return generous_center_crop_emoji(pil_image, target_size, crop_ratio=0.75)\n",
    "    except Exception as e:\n",
    "        return generous_center_crop_emoji(pil_image, target_size, crop_ratio=0.75)\n",
    "\n",
    "def generous_center_crop(pil_image, target_size=(160, 160), crop_ratio=0.7):\n",
    "    \"\"\"Generous center crop for real faces\"\"\"\n",
    "    w, h = pil_image.size\n",
    "    crop_w, crop_h = w * crop_ratio, h * crop_ratio\n",
    "    left = (w - crop_w) / 2\n",
    "    top = (h - crop_h) / 2\n",
    "    cropped = pil_image.crop((left, top, left + crop_w, top + crop_h))\n",
    "    return cropped.resize(target_size, Image.LANCZOS)\n",
    "\n",
    "def generous_center_crop_emoji(pil_image, target_size=(152, 152), crop_ratio=0.75):\n",
    "    \"\"\"Generous center crop for emoji\"\"\"\n",
    "    w, h = pil_image.size\n",
    "    crop_w, crop_h = w * crop_ratio, h * crop_ratio\n",
    "    left = (w - crop_w) / 2\n",
    "    top = (h - crop_h) / 2\n",
    "    cropped = pil_image.crop((left, top, left + crop_w, top + crop_h))\n",
    "    return cropped.resize(target_size, Image.LANCZOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T15:07:55.606620Z",
     "iopub.status.busy": "2025-08-17T15:07:55.606388Z",
     "iopub.status.idle": "2025-08-17T15:07:55.625262Z",
     "shell.execute_reply": "2025-08-17T15:07:55.624601Z",
     "shell.execute_reply.started": "2025-08-17T15:07:55.606598Z"
    },
    "id": "OJfHWCadn3v_",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CelebADataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None, face_crop=True, target_size=(152, 152), \n",
    "                 padding_ratio=0.3, subset_size=None, subset_method='random', seed=42):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.face_crop = face_crop\n",
    "        self.target_size = target_size\n",
    "        self.padding_ratio = padding_ratio\n",
    "        \n",
    "        # Get all image paths (CelebA chá»§ yáº¿u lÃ  JPG)\n",
    "        all_image_paths = glob.glob(os.path.join(data_dir, \"*.jpg\"))\n",
    "        total_images = len(all_image_paths)\n",
    "        \n",
    "        # Apply subsetting if requested\n",
    "        if subset_size and subset_size < total_images:\n",
    "            self.image_paths = self._create_subset(all_image_paths, subset_size, subset_method, seed)\n",
    "            print(f\"ğŸ“Š Using subset: {len(self.image_paths)}/{total_images} CelebA images ({subset_method})\")\n",
    "        else:\n",
    "            self.image_paths = all_image_paths\n",
    "            print(f\"ğŸ“Š Using full dataset: {len(self.image_paths)} CelebA images\")\n",
    "        \n",
    "        if face_crop:\n",
    "            print(f\"âœ… Source face cropping ENABLED with padding_ratio={padding_ratio}\")\n",
    "\n",
    "    def _create_subset(self, all_paths, subset_size, method='random', seed=42):\n",
    "        random.seed(seed)  # For reproducible results\n",
    "        \n",
    "        if method == 'random':\n",
    "            return random.sample(all_paths, subset_size)\n",
    "        elif method == 'first':\n",
    "            return all_paths[:subset_size]\n",
    "        elif method == 'interval':\n",
    "            interval = len(all_paths) // subset_size\n",
    "            return all_paths[::interval][:subset_size]\n",
    "        elif method == 'balanced':\n",
    "            return random.sample(all_paths, subset_size)\n",
    "        else:\n",
    "            return random.sample(all_paths, subset_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_path = self.image_paths[idx]\n",
    "            image = Image.open(img_path).convert('RGB')  # CelebA JPG â†’ RGB trá»±c tiáº¿p\n",
    "            \n",
    "            if self.face_crop:\n",
    "                image = crop_face_tight(image, self.target_size, self.padding_ratio)\n",
    "            else:\n",
    "                image = image.resize(self.target_size)\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "                \n",
    "            return image, 0\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            black_img = Image.new('RGB', self.target_size, (0, 0, 0))\n",
    "            if self.transform:\n",
    "                black_img = self.transform(black_img)\n",
    "            return black_img, 0\n",
    "\n",
    "\n",
    "class BitmojiDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None, face_crop=True, target_size=(152, 152),\n",
    "                 padding_ratio=0.25, subset_size=None, subset_method='random', seed=42):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.face_crop = face_crop\n",
    "        self.target_size = target_size\n",
    "        self.padding_ratio = padding_ratio\n",
    "        \n",
    "        all_image_paths = glob.glob(os.path.join(data_dir, \"*.png\"))\n",
    "        total_images = len(all_image_paths)\n",
    "        \n",
    "        # Apply subsetting if requested\n",
    "        if subset_size and subset_size < total_images:\n",
    "            self.image_paths = self._create_subset(all_image_paths, subset_size, subset_method, seed)\n",
    "            print(f\"ğŸ“Š Using subset: {len(self.image_paths)}/{total_images} Bitmoji images ({subset_method})\")\n",
    "        else:\n",
    "            self.image_paths = all_image_paths\n",
    "            print(f\"ğŸ“Š Using full dataset: {len(self.image_paths)} Bitmoji images\")\n",
    "        \n",
    "        # Check format distribution\n",
    "        png_count = sum(1 for p in self.image_paths if p.lower().endswith('.png'))\n",
    "        jpg_count = len(self.image_paths) - png_count\n",
    "        print(f\"ğŸ“Š Format distribution: {png_count} PNG, {jpg_count} JPG/JPEG\")\n",
    "        \n",
    "        if face_crop:\n",
    "            print(f\"Target face cropping ENABLED with padding_ratio={padding_ratio}\")\n",
    "\n",
    "    def _create_subset(self, all_paths, subset_size, method='random', seed=42):\n",
    "        \"\"\"Create dataset subset using different methods\"\"\"\n",
    "        random.seed(seed)\n",
    "        \n",
    "        if method == 'random':\n",
    "            return random.sample(all_paths, subset_size)\n",
    "        elif method == 'first':\n",
    "            return all_paths[:subset_size]\n",
    "        elif method == 'interval':\n",
    "            interval = len(all_paths) // subset_size\n",
    "            return all_paths[::interval][:subset_size]\n",
    "        else:\n",
    "            return random.sample(all_paths, subset_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_path = self.image_paths[idx]\n",
    "            image = Image.open(img_path)\n",
    "            \n",
    "            # Proper PNG transparency handling\n",
    "            if image.mode == 'RGBA':\n",
    "                # Create white background for transparent PNGs\n",
    "                background = Image.new('RGB', image.size, (255, 255, 255))\n",
    "                background.paste(image, mask=image.split()[-1])  # Use alpha as mask\n",
    "                image = background\n",
    "            elif image.mode == 'P':\n",
    "                # Handle palette mode with possible transparency\n",
    "                image = image.convert('RGBA')\n",
    "                background = Image.new('RGB', image.size, (255, 255, 255))\n",
    "                background.paste(image, mask=image.split()[-1] if len(image.split()) == 4 else None)\n",
    "                image = background\n",
    "            elif image.mode != 'RGB':\n",
    "                # Convert any other mode to RGB\n",
    "                image = image.convert('RGB')\n",
    "            \n",
    "            if self.face_crop:\n",
    "                image = crop_face_emoji(image, self.target_size, self.padding_ratio)\n",
    "            else:\n",
    "                image = image.resize(self.target_size)\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "                \n",
    "            return image, 0\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            black_img = Image.new('RGB', self.target_size, (0, 0, 0))\n",
    "            if self.transform:\n",
    "                black_img = self.transform(black_img)\n",
    "            return black_img, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-08-17T15:07:55.626706Z",
     "iopub.status.busy": "2025-08-17T15:07:55.626012Z",
     "iopub.status.idle": "2025-08-17T15:07:56.849148Z",
     "shell.execute_reply": "2025-08-17T15:07:56.848356Z",
     "shell.execute_reply.started": "2025-08-17T15:07:55.626683Z"
    },
    "id": "VmN318jwFdax",
    "outputId": "00f8eb45-ddf1-438b-c662-1991d18d5995",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Using full dataset: 30000 CelebA images\n",
      "âœ… Source face cropping ENABLED with padding_ratio=0.35\n",
      "ğŸ“Š Using full dataset: 130227 Bitmoji images\n",
      "ğŸ“Š Format distribution: 130227 PNG, 0 JPG/JPEG\n",
      "ğŸš€ Ready for fast training!\n",
      "ğŸ“Š Source: 30000 images\n",
      "ğŸ“Š Target: 130227 images\n",
      "â±ï¸  Expected speedup: ~6x faster\n"
     ]
    }
   ],
   "source": [
    "# Transforms\n",
    "face_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "emoji_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "source_dataset = CelebADataset(\n",
    "    data_dir=celeba_root, \n",
    "    transform=face_transform,\n",
    "    face_crop=True,\n",
    "    target_size=(160, 160),\n",
    "    padding_ratio=0.35,\n",
    "    #subset_size=10000,      \n",
    "    #subset_method='first'  \n",
    ")\n",
    "\n",
    "target_dataset = BitmojiDataset(\n",
    "    data_dir=bitmoji_root, \n",
    "    transform=emoji_transform,\n",
    "    face_crop=False,\n",
    "    target_size=(152, 152),\n",
    "    padding_ratio=0.3,\n",
    "    #subset_size=10000,       \n",
    "    #subset_method='first'  \n",
    ")\n",
    "\n",
    "# Data loaders\n",
    "source_loader = torch.utils.data.DataLoader(\n",
    "    source_dataset, \n",
    "    batch_size=opt.batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "target_loader = torch.utils.data.DataLoader(\n",
    "    target_dataset, \n",
    "    batch_size=opt.batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "print(f\"Ready for fast training!\")\n",
    "print(f\"Source: {len(source_dataset)} images\")\n",
    "print(f\"Target: {len(target_dataset)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T15:07:56.850248Z",
     "iopub.status.busy": "2025-08-17T15:07:56.849996Z",
     "iopub.status.idle": "2025-08-17T15:07:57.857211Z",
     "shell.execute_reply": "2025-08-17T15:07:57.856383Z",
     "shell.execute_reply.started": "2025-08-17T15:07:56.850219Z"
    },
    "id": "8uYDRaM2HZ3-",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Khá»Ÿi táº¡o generator and discriminator vÃ  thÃªm wandb Ä‘á»ƒ track\n",
    "g_net = Generator().cuda()\n",
    "d_net = Discriminator().cuda()\n",
    "f_net = create_face_extractor().cuda()\n",
    "\n",
    "wandb.watch(g_net, log=\"all\", log_freq=200)\n",
    "wandb.watch(d_net, log=\"all\", log_freq=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T15:07:57.858356Z",
     "iopub.status.busy": "2025-08-17T15:07:57.858101Z",
     "iopub.status.idle": "2025-08-17T15:07:57.864803Z",
     "shell.execute_reply": "2025-08-17T15:07:57.864218Z",
     "shell.execute_reply.started": "2025-08-17T15:07:57.858334Z"
    },
    "id": "3KFKK7ZdHlYl",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(g_net.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_D = torch.optim.Adam(d_net.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T15:07:57.865761Z",
     "iopub.status.busy": "2025-08-17T15:07:57.865528Z",
     "iopub.status.idle": "2025-08-17T15:07:57.874793Z",
     "shell.execute_reply": "2025-08-17T15:07:57.874093Z",
     "shell.execute_reply.started": "2025-08-17T15:07:57.865735Z"
    },
    "id": "qQEoiZA3Hl0w",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = DTNLoss(alpha=100, beta=1, gamma=0.05)\n",
    "# Äá»‹nh dáº¡ng Tensor sá»­ dá»¥ng GPU náº¿u cÃ³ vÃ  device type\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T15:07:57.875769Z",
     "iopub.status.busy": "2025-08-17T15:07:57.875520Z",
     "iopub.status.idle": "2025-08-17T15:07:57.887449Z",
     "shell.execute_reply": "2025-08-17T15:07:57.886927Z",
     "shell.execute_reply.started": "2025-08-17T15:07:57.875748Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ThÃªm vÃ o sau pháº§n khá»Ÿi táº¡o models, trÆ°á»›c training loop:\n",
    "import os\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "def save_checkpoint(g_net, d_net, f_net, optimizer_G, optimizer_D, batches_done, loss_dict):\n",
    "    \"\"\"Save model checkpoint\"\"\"\n",
    "    checkpoint = {\n",
    "        'batches_done': batches_done,\n",
    "        'generator_state_dict': g_net.state_dict(),\n",
    "        'discriminator_state_dict': d_net.state_dict(),\n",
    "        'face_extractor_state_dict': f_net.state_dict(),\n",
    "        'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "        'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "        'loss_dict': loss_dict,\n",
    "        'model_config': {\n",
    "            'latent_dim': 512,\n",
    "            'img_size': 64,\n",
    "            'discriminator_img_size': 152\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    checkpoint_path = f\"{checkpoint_dir}/dtn_checkpoint_batch_{batches_done}.pth\"\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "    return checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-08-17T15:07:57.888499Z",
     "iopub.status.busy": "2025-08-17T15:07:57.888271Z",
     "iopub.status.idle": "2025-08-17T15:32:37.547888Z",
     "shell.execute_reply": "2025-08-17T15:32:37.547036Z",
     "shell.execute_reply.started": "2025-08-17T15:07:57.888473Z"
    },
    "id": "FhU3sTQWH0Cp",
    "outputId": "569e5225-9c68-441a-d764-6d86b423aaf6",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed features shape: torch.Size([16, 512])\n",
      "[Epoch 0/10] [Batch 0/1875] [D loss: 3.7330] [G loss: 3.3843] [GAN: 2.2540] [CONST: 0.0040] [TID: 0.7157] [TV: 0.2853]\n",
      "logged samples\n",
      "[Epoch 0/10] [Batch 1/1875] [D loss: 6.9208] [G loss: 5.2542] [GAN: 4.1399] [CONST: 0.0038] [TID: 0.7199] [TV: 0.2872]\n",
      "[Epoch 0/10] [Batch 2/1875] [D loss: 5.1734] [G loss: 4.1643] [GAN: 3.0649] [CONST: 0.0039] [TID: 0.6966] [TV: 0.2914]\n",
      "[Epoch 0/10] [Batch 3/1875] [D loss: 4.3150] [G loss: 3.4459] [GAN: 2.3349] [CONST: 0.0039] [TID: 0.7097] [TV: 0.2986]\n",
      "[Epoch 0/10] [Batch 4/1875] [D loss: 4.2921] [G loss: 3.3617] [GAN: 2.2511] [CONST: 0.0039] [TID: 0.7069] [TV: 0.3016]\n",
      "[Epoch 0/10] [Batch 5/1875] [D loss: 3.5878] [G loss: 3.3515] [GAN: 2.2496] [CONST: 0.0040] [TID: 0.6903] [TV: 0.3030]\n",
      "[Epoch 0/10] [Batch 6/1875] [D loss: 3.4084] [G loss: 3.3240] [GAN: 2.2507] [CONST: 0.0039] [TID: 0.6636] [TV: 0.3005]\n",
      "[Epoch 0/10] [Batch 7/1875] [D loss: 3.4519] [G loss: 3.2805] [GAN: 2.2423] [CONST: 0.0036] [TID: 0.6674] [TV: 0.2980]\n",
      "[Epoch 0/10] [Batch 8/1875] [D loss: 3.3660] [G loss: 3.3234] [GAN: 2.2394] [CONST: 0.0039] [TID: 0.6744] [TV: 0.2954]\n",
      "[Epoch 0/10] [Batch 9/1875] [D loss: 3.3367] [G loss: 3.2819] [GAN: 2.2390] [CONST: 0.0037] [TID: 0.6546] [TV: 0.2908]\n",
      "[Epoch 0/10] [Batch 10/1875] [D loss: 3.3365] [G loss: 3.2291] [GAN: 2.2360] [CONST: 0.0036] [TID: 0.6137] [TV: 0.2882]\n",
      "[Epoch 0/10] [Batch 11/1875] [D loss: 3.3079] [G loss: 3.2228] [GAN: 2.2337] [CONST: 0.0036] [TID: 0.6130] [TV: 0.2831]\n",
      "[Epoch 0/10] [Batch 12/1875] [D loss: 3.3110] [G loss: 3.2102] [GAN: 2.2329] [CONST: 0.0036] [TID: 0.6044] [TV: 0.2782]\n",
      "[Epoch 0/10] [Batch 13/1875] [D loss: 3.3088] [G loss: 3.2136] [GAN: 2.2308] [CONST: 0.0038] [TID: 0.5926] [TV: 0.2746]\n",
      "[Epoch 0/10] [Batch 14/1875] [D loss: 3.3082] [G loss: 3.1791] [GAN: 2.2296] [CONST: 0.0037] [TID: 0.5704] [TV: 0.2701]\n",
      "[Epoch 0/10] [Batch 15/1875] [D loss: 3.3061] [G loss: 3.1622] [GAN: 2.2284] [CONST: 0.0035] [TID: 0.5656] [TV: 0.2673]\n",
      "[Epoch 0/10] [Batch 16/1875] [D loss: 3.3009] [G loss: 3.1401] [GAN: 2.2270] [CONST: 0.0035] [TID: 0.5502] [TV: 0.2632]\n",
      "[Epoch 0/10] [Batch 17/1875] [D loss: 3.3064] [G loss: 3.1210] [GAN: 2.2265] [CONST: 0.0035] [TID: 0.5358] [TV: 0.2621]\n",
      "[Epoch 0/10] [Batch 18/1875] [D loss: 3.3055] [G loss: 3.0769] [GAN: 2.2251] [CONST: 0.0031] [TID: 0.5333] [TV: 0.2601]\n",
      "[Epoch 0/10] [Batch 19/1875] [D loss: 3.3053] [G loss: 3.0681] [GAN: 2.2266] [CONST: 0.0034] [TID: 0.4914] [TV: 0.2555]\n",
      "[Epoch 0/10] [Batch 20/1875] [D loss: 3.3111] [G loss: 3.1070] [GAN: 2.2356] [CONST: 0.0035] [TID: 0.5111] [TV: 0.2527]\n",
      "[Epoch 0/10] [Batch 21/1875] [D loss: 3.3075] [G loss: 3.0720] [GAN: 2.2387] [CONST: 0.0032] [TID: 0.4986] [TV: 0.2511]\n",
      "[Epoch 0/10] [Batch 22/1875] [D loss: 3.3031] [G loss: 3.0180] [GAN: 2.2234] [CONST: 0.0032] [TID: 0.4596] [TV: 0.2494]\n",
      "[Epoch 0/10] [Batch 23/1875] [D loss: 3.3027] [G loss: 3.0034] [GAN: 2.2204] [CONST: 0.0031] [TID: 0.4611] [TV: 0.2480]\n",
      "[Epoch 0/10] [Batch 24/1875] [D loss: 3.2988] [G loss: 2.9668] [GAN: 2.2174] [CONST: 0.0031] [TID: 0.4228] [TV: 0.2457]\n",
      "[Epoch 0/10] [Batch 25/1875] [D loss: 3.3023] [G loss: 2.9739] [GAN: 2.2154] [CONST: 0.0032] [TID: 0.4240] [TV: 0.2449]\n",
      "[Epoch 0/10] [Batch 26/1875] [D loss: 3.2990] [G loss: 2.9556] [GAN: 2.2142] [CONST: 0.0031] [TID: 0.4242] [TV: 0.2431]\n",
      "[Epoch 0/10] [Batch 27/1875] [D loss: 3.3014] [G loss: 2.9165] [GAN: 2.2130] [CONST: 0.0028] [TID: 0.4086] [TV: 0.2403]\n",
      "[Epoch 0/10] [Batch 28/1875] [D loss: 3.2985] [G loss: 2.9129] [GAN: 2.2121] [CONST: 0.0031] [TID: 0.3827] [TV: 0.2366]\n",
      "[Epoch 0/10] [Batch 29/1875] [D loss: 3.2998] [G loss: 2.9336] [GAN: 2.2112] [CONST: 0.0033] [TID: 0.3793] [TV: 0.2365]\n",
      "[Epoch 0/10] [Batch 30/1875] [D loss: 3.2969] [G loss: 2.8979] [GAN: 2.2105] [CONST: 0.0031] [TID: 0.3650] [TV: 0.2337]\n",
      "[Epoch 0/10] [Batch 31/1875] [D loss: 3.2987] [G loss: 2.8761] [GAN: 2.2098] [CONST: 0.0030] [TID: 0.3511] [TV: 0.2318]\n",
      "[Epoch 0/10] [Batch 32/1875] [D loss: 3.2981] [G loss: 2.8894] [GAN: 2.2088] [CONST: 0.0032] [TID: 0.3507] [TV: 0.2282]\n",
      "[Epoch 0/10] [Batch 33/1875] [D loss: 3.2981] [G loss: 2.8949] [GAN: 2.2082] [CONST: 0.0031] [TID: 0.3612] [TV: 0.2310]\n",
      "[Epoch 0/10] [Batch 34/1875] [D loss: 3.2983] [G loss: 2.8477] [GAN: 2.2075] [CONST: 0.0031] [TID: 0.3198] [TV: 0.2268]\n",
      "[Epoch 0/10] [Batch 35/1875] [D loss: 3.2978] [G loss: 2.8347] [GAN: 2.2069] [CONST: 0.0030] [TID: 0.3134] [TV: 0.2245]\n",
      "[Epoch 0/10] [Batch 36/1875] [D loss: 3.2978] [G loss: 2.8287] [GAN: 2.2063] [CONST: 0.0031] [TID: 0.3057] [TV: 0.2217]\n",
      "[Epoch 0/10] [Batch 37/1875] [D loss: 3.2973] [G loss: 2.8254] [GAN: 2.2058] [CONST: 0.0029] [TID: 0.3202] [TV: 0.2196]\n",
      "[Epoch 0/10] [Batch 38/1875] [D loss: 3.2972] [G loss: 2.8133] [GAN: 2.2051] [CONST: 0.0031] [TID: 0.2891] [TV: 0.2210]\n",
      "[Epoch 0/10] [Batch 39/1875] [D loss: 3.2971] [G loss: 2.8278] [GAN: 2.2047] [CONST: 0.0034] [TID: 0.2754] [TV: 0.2203]\n",
      "[Epoch 0/10] [Batch 40/1875] [D loss: 3.2974] [G loss: 2.7855] [GAN: 2.2042] [CONST: 0.0029] [TID: 0.2830] [TV: 0.2180]\n",
      "[Epoch 0/10] [Batch 41/1875] [D loss: 3.2976] [G loss: 2.7579] [GAN: 2.2038] [CONST: 0.0027] [TID: 0.2733] [TV: 0.2147]\n",
      "[Epoch 0/10] [Batch 42/1875] [D loss: 3.2992] [G loss: 2.7640] [GAN: 2.2033] [CONST: 0.0026] [TID: 0.2877] [TV: 0.2109]\n",
      "[Epoch 0/10] [Batch 43/1875] [D loss: 3.2976] [G loss: 2.7435] [GAN: 2.2029] [CONST: 0.0026] [TID: 0.2724] [TV: 0.2076]\n",
      "[Epoch 0/10] [Batch 44/1875] [D loss: 3.2970] [G loss: 2.7541] [GAN: 2.2026] [CONST: 0.0027] [TID: 0.2669] [TV: 0.2057]\n",
      "[Epoch 0/10] [Batch 45/1875] [D loss: 3.2970] [G loss: 2.7168] [GAN: 2.2023] [CONST: 0.0026] [TID: 0.2426] [TV: 0.2074]\n",
      "[Epoch 0/10] [Batch 46/1875] [D loss: 3.2970] [G loss: 2.7409] [GAN: 2.2019] [CONST: 0.0027] [TID: 0.2557] [TV: 0.2047]\n",
      "[Epoch 0/10] [Batch 47/1875] [D loss: 3.2972] [G loss: 2.7035] [GAN: 2.2016] [CONST: 0.0026] [TID: 0.2306] [TV: 0.2029]\n",
      "[Epoch 0/10] [Batch 48/1875] [D loss: 3.2968] [G loss: 2.7204] [GAN: 2.2013] [CONST: 0.0027] [TID: 0.2351] [TV: 0.2046]\n",
      "[Epoch 0/10] [Batch 49/1875] [D loss: 3.2974] [G loss: 2.6842] [GAN: 2.2010] [CONST: 0.0025] [TID: 0.2200] [TV: 0.1990]\n",
      "[Epoch 0/10] [Batch 50/1875] [D loss: 3.2965] [G loss: 2.6700] [GAN: 2.2009] [CONST: 0.0024] [TID: 0.2198] [TV: 0.2008]\n",
      "logged samples\n",
      "[Epoch 0/10] [Batch 51/1875] [D loss: 3.2963] [G loss: 2.6553] [GAN: 2.2006] [CONST: 0.0022] [TID: 0.2201] [TV: 0.2023]\n",
      "[Epoch 0/10] [Batch 52/1875] [D loss: 3.2969] [G loss: 2.7053] [GAN: 2.2004] [CONST: 0.0025] [TID: 0.2415] [TV: 0.2028]\n",
      "[Epoch 0/10] [Batch 53/1875] [D loss: 3.2968] [G loss: 2.6910] [GAN: 2.2002] [CONST: 0.0025] [TID: 0.2268] [TV: 0.2017]\n",
      "[Epoch 0/10] [Batch 54/1875] [D loss: 3.2970] [G loss: 2.7031] [GAN: 2.2000] [CONST: 0.0026] [TID: 0.2350] [TV: 0.2010]\n",
      "[Epoch 0/10] [Batch 55/1875] [D loss: 3.2973] [G loss: 2.6608] [GAN: 2.1998] [CONST: 0.0025] [TID: 0.2052] [TV: 0.1973]\n",
      "[Epoch 0/10] [Batch 56/1875] [D loss: 3.2977] [G loss: 2.6933] [GAN: 2.1997] [CONST: 0.0025] [TID: 0.2302] [TV: 0.1978]\n",
      "[Epoch 0/10] [Batch 57/1875] [D loss: 3.2970] [G loss: 2.6719] [GAN: 2.1996] [CONST: 0.0023] [TID: 0.2369] [TV: 0.1964]\n",
      "[Epoch 0/10] [Batch 58/1875] [D loss: 3.2965] [G loss: 2.6224] [GAN: 2.1994] [CONST: 0.0022] [TID: 0.1974] [TV: 0.1948]\n",
      "[Epoch 0/10] [Batch 59/1875] [D loss: 3.2972] [G loss: 2.6646] [GAN: 2.1993] [CONST: 0.0024] [TID: 0.2180] [TV: 0.1948]\n",
      "[Epoch 0/10] [Batch 60/1875] [D loss: 3.2972] [G loss: 2.6510] [GAN: 2.1992] [CONST: 0.0022] [TID: 0.2192] [TV: 0.1922]\n",
      "[Epoch 0/10] [Batch 61/1875] [D loss: 3.2965] [G loss: 2.6436] [GAN: 2.1990] [CONST: 0.0024] [TID: 0.1952] [TV: 0.1946]\n",
      "[Epoch 0/10] [Batch 62/1875] [D loss: 3.2966] [G loss: 2.6676] [GAN: 2.1990] [CONST: 0.0024] [TID: 0.2147] [TV: 0.1938]\n",
      "[Epoch 0/10] [Batch 63/1875] [D loss: 3.2964] [G loss: 2.6580] [GAN: 2.1988] [CONST: 0.0024] [TID: 0.2068] [TV: 0.1926]\n",
      "[Epoch 0/10] [Batch 64/1875] [D loss: 3.2971] [G loss: 2.6302] [GAN: 2.1987] [CONST: 0.0023] [TID: 0.1892] [TV: 0.1907]\n",
      "[Epoch 0/10] [Batch 65/1875] [D loss: 3.2967] [G loss: 2.6291] [GAN: 2.1987] [CONST: 0.0021] [TID: 0.2084] [TV: 0.1879]\n",
      "[Epoch 0/10] [Batch 66/1875] [D loss: 3.2968] [G loss: 2.6004] [GAN: 2.1986] [CONST: 0.0021] [TID: 0.1845] [TV: 0.1864]\n",
      "[Epoch 0/10] [Batch 67/1875] [D loss: 3.2966] [G loss: 2.6429] [GAN: 2.1985] [CONST: 0.0024] [TID: 0.1964] [TV: 0.1867]\n",
      "[Epoch 0/10] [Batch 68/1875] [D loss: 3.2964] [G loss: 2.6311] [GAN: 2.1984] [CONST: 0.0022] [TID: 0.2066] [TV: 0.1891]\n",
      "[Epoch 0/10] [Batch 69/1875] [D loss: 3.2965] [G loss: 2.6430] [GAN: 2.1983] [CONST: 0.0025] [TID: 0.1889] [TV: 0.1888]\n",
      "[Epoch 0/10] [Batch 70/1875] [D loss: 3.2969] [G loss: 2.6381] [GAN: 2.1983] [CONST: 0.0022] [TID: 0.2101] [TV: 0.1908]\n",
      "[Epoch 0/10] [Batch 71/1875] [D loss: 3.2965] [G loss: 2.6175] [GAN: 2.1983] [CONST: 0.0020] [TID: 0.2048] [TV: 0.1898]\n",
      "[Epoch 0/10] [Batch 72/1875] [D loss: 3.2964] [G loss: 2.5945] [GAN: 2.1982] [CONST: 0.0021] [TID: 0.1774] [TV: 0.1891]\n",
      "[Epoch 0/10] [Batch 73/1875] [D loss: 3.2968] [G loss: 2.6185] [GAN: 2.1981] [CONST: 0.0021] [TID: 0.1974] [TV: 0.1880]\n",
      "[Epoch 0/10] [Batch 74/1875] [D loss: 3.2967] [G loss: 2.5846] [GAN: 2.1983] [CONST: 0.0019] [TID: 0.1829] [TV: 0.1880]\n",
      "[Epoch 0/10] [Batch 75/1875] [D loss: 3.2966] [G loss: 2.5868] [GAN: 2.1980] [CONST: 0.0020] [TID: 0.1750] [TV: 0.1835]\n",
      "[Epoch 0/10] [Batch 76/1875] [D loss: 3.2965] [G loss: 2.5728] [GAN: 2.1980] [CONST: 0.0019] [TID: 0.1758] [TV: 0.1860]\n",
      "[Epoch 0/10] [Batch 77/1875] [D loss: 3.2964] [G loss: 2.5977] [GAN: 2.1979] [CONST: 0.0019] [TID: 0.1992] [TV: 0.1839]\n",
      "[Epoch 0/10] [Batch 78/1875] [D loss: 3.2965] [G loss: 2.5846] [GAN: 2.1979] [CONST: 0.0020] [TID: 0.1816] [TV: 0.1830]\n",
      "[Epoch 0/10] [Batch 79/1875] [D loss: 3.2965] [G loss: 2.5737] [GAN: 2.1979] [CONST: 0.0019] [TID: 0.1733] [TV: 0.1838]\n",
      "[Epoch 0/10] [Batch 80/1875] [D loss: 3.2964] [G loss: 2.5748] [GAN: 2.1979] [CONST: 0.0019] [TID: 0.1795] [TV: 0.1829]\n",
      "[Epoch 0/10] [Batch 81/1875] [D loss: 3.2966] [G loss: 2.5656] [GAN: 2.1978] [CONST: 0.0019] [TID: 0.1700] [TV: 0.1844]\n",
      "[Epoch 0/10] [Batch 82/1875] [D loss: 3.2965] [G loss: 2.5825] [GAN: 2.1978] [CONST: 0.0019] [TID: 0.1826] [TV: 0.1826]\n",
      "[Epoch 0/10] [Batch 83/1875] [D loss: 3.2966] [G loss: 2.5734] [GAN: 2.1978] [CONST: 0.0019] [TID: 0.1746] [TV: 0.1813]\n",
      "[Epoch 0/10] [Batch 84/1875] [D loss: 3.2970] [G loss: 2.5870] [GAN: 2.1977] [CONST: 0.0020] [TID: 0.1829] [TV: 0.1797]\n",
      "[Epoch 0/10] [Batch 85/1875] [D loss: 3.2964] [G loss: 2.5821] [GAN: 2.1977] [CONST: 0.0020] [TID: 0.1728] [TV: 0.1819]\n",
      "[Epoch 0/10] [Batch 86/1875] [D loss: 3.2966] [G loss: 2.5799] [GAN: 2.1978] [CONST: 0.0019] [TID: 0.1806] [TV: 0.1798]\n",
      "[Epoch 0/10] [Batch 87/1875] [D loss: 3.2963] [G loss: 2.5520] [GAN: 2.1977] [CONST: 0.0018] [TID: 0.1679] [TV: 0.1798]\n",
      "[Epoch 0/10] [Batch 88/1875] [D loss: 3.2965] [G loss: 2.5565] [GAN: 2.1977] [CONST: 0.0018] [TID: 0.1720] [TV: 0.1786]\n",
      "[Epoch 0/10] [Batch 89/1875] [D loss: 3.2962] [G loss: 2.5636] [GAN: 2.1977] [CONST: 0.0019] [TID: 0.1703] [TV: 0.1759]\n",
      "[Epoch 0/10] [Batch 90/1875] [D loss: 3.2966] [G loss: 2.6131] [GAN: 2.1976] [CONST: 0.0021] [TID: 0.1947] [TV: 0.1757]\n",
      "[Epoch 0/10] [Batch 91/1875] [D loss: 3.2964] [G loss: 2.5374] [GAN: 2.1976] [CONST: 0.0017] [TID: 0.1649] [TV: 0.1773]\n",
      "[Epoch 0/10] [Batch 92/1875] [D loss: 3.2965] [G loss: 2.5833] [GAN: 2.1976] [CONST: 0.0021] [TID: 0.1663] [TV: 0.1805]\n",
      "[Epoch 0/10] [Batch 93/1875] [D loss: 3.2964] [G loss: 2.5697] [GAN: 2.1976] [CONST: 0.0019] [TID: 0.1730] [TV: 0.1760]\n",
      "[Epoch 0/10] [Batch 94/1875] [D loss: 3.2966] [G loss: 2.5743] [GAN: 2.1976] [CONST: 0.0020] [TID: 0.1701] [TV: 0.1782]\n",
      "[Epoch 0/10] [Batch 95/1875] [D loss: 3.2962] [G loss: 2.5447] [GAN: 2.1976] [CONST: 0.0018] [TID: 0.1601] [TV: 0.1745]\n",
      "[Epoch 0/10] [Batch 96/1875] [D loss: 3.2962] [G loss: 2.5518] [GAN: 2.1976] [CONST: 0.0019] [TID: 0.1545] [TV: 0.1751]\n",
      "[Epoch 0/10] [Batch 97/1875] [D loss: 3.2963] [G loss: 2.5628] [GAN: 2.1975] [CONST: 0.0019] [TID: 0.1692] [TV: 0.1744]\n",
      "[Epoch 0/10] [Batch 98/1875] [D loss: 3.2961] [G loss: 2.5307] [GAN: 2.1975] [CONST: 0.0017] [TID: 0.1500] [TV: 0.1758]\n",
      "[Epoch 0/10] [Batch 99/1875] [D loss: 3.2963] [G loss: 2.5558] [GAN: 2.1975] [CONST: 0.0018] [TID: 0.1688] [TV: 0.1750]\n",
      "[Epoch 0/10] [Batch 100/1875] [D loss: 3.2962] [G loss: 2.5021] [GAN: 2.1974] [CONST: 0.0016] [TID: 0.1360] [TV: 0.1751]\n",
      "logged samples\n",
      "âœ… Checkpoint saved: checkpoints/dtn_checkpoint_batch_100.pth\n",
      "[Epoch 0/10] [Batch 101/1875] [D loss: 3.2968] [G loss: 2.5621] [GAN: 2.1974] [CONST: 0.0018] [TID: 0.1762] [TV: 0.1735]\n",
      "[Epoch 0/10] [Batch 102/1875] [D loss: 3.2967] [G loss: 2.5183] [GAN: 2.1974] [CONST: 0.0017] [TID: 0.1442] [TV: 0.1720]\n",
      "[Epoch 0/10] [Batch 103/1875] [D loss: 3.2967] [G loss: 2.5403] [GAN: 2.1975] [CONST: 0.0017] [TID: 0.1658] [TV: 0.1704]\n",
      "[Epoch 0/10] [Batch 104/1875] [D loss: 3.2969] [G loss: 2.5620] [GAN: 2.1976] [CONST: 0.0018] [TID: 0.1752] [TV: 0.1707]\n",
      "[Epoch 0/10] [Batch 105/1875] [D loss: 3.2962] [G loss: 2.5506] [GAN: 2.1975] [CONST: 0.0019] [TID: 0.1569] [TV: 0.1746]\n",
      "[Epoch 0/10] [Batch 106/1875] [D loss: 3.2962] [G loss: 2.5680] [GAN: 2.1975] [CONST: 0.0018] [TID: 0.1777] [TV: 0.1737]\n",
      "[Epoch 0/10] [Batch 107/1875] [D loss: 3.2963] [G loss: 2.5242] [GAN: 2.1974] [CONST: 0.0016] [TID: 0.1618] [TV: 0.1721]\n",
      "[Epoch 0/10] [Batch 108/1875] [D loss: 3.2962] [G loss: 2.5201] [GAN: 2.1974] [CONST: 0.0017] [TID: 0.1438] [TV: 0.1711]\n",
      "[Epoch 0/10] [Batch 109/1875] [D loss: 3.2961] [G loss: 2.5646] [GAN: 2.1974] [CONST: 0.0020] [TID: 0.1598] [TV: 0.1707]\n",
      "[Epoch 0/10] [Batch 110/1875] [D loss: 3.2964] [G loss: 2.5227] [GAN: 2.1974] [CONST: 0.0015] [TID: 0.1656] [TV: 0.1726]\n",
      "[Epoch 0/10] [Batch 111/1875] [D loss: 3.2963] [G loss: 2.5140] [GAN: 2.1974] [CONST: 0.0015] [TID: 0.1560] [TV: 0.1718]\n",
      "[Epoch 0/10] [Batch 112/1875] [D loss: 3.2963] [G loss: 2.5280] [GAN: 2.1974] [CONST: 0.0017] [TID: 0.1505] [TV: 0.1729]\n",
      "[Epoch 0/10] [Batch 113/1875] [D loss: 3.2962] [G loss: 2.5109] [GAN: 2.1974] [CONST: 0.0016] [TID: 0.1410] [TV: 0.1689]\n",
      "[Epoch 0/10] [Batch 114/1875] [D loss: 3.2962] [G loss: 2.5129] [GAN: 2.1974] [CONST: 0.0017] [TID: 0.1375] [TV: 0.1707]\n",
      "[Epoch 0/10] [Batch 115/1875] [D loss: 3.2961] [G loss: 2.4935] [GAN: 2.1973] [CONST: 0.0015] [TID: 0.1382] [TV: 0.1703]\n",
      "[Epoch 0/10] [Batch 116/1875] [D loss: 3.2960] [G loss: 2.5145] [GAN: 2.1974] [CONST: 0.0017] [TID: 0.1354] [TV: 0.1724]\n",
      "[Epoch 0/10] [Batch 117/1875] [D loss: 3.2961] [G loss: 2.5303] [GAN: 2.1974] [CONST: 0.0017] [TID: 0.1524] [TV: 0.1727]\n",
      "[Epoch 0/10] [Batch 118/1875] [D loss: 3.2962] [G loss: 2.5014] [GAN: 2.1974] [CONST: 0.0015] [TID: 0.1486] [TV: 0.1728]\n",
      "[Epoch 0/10] [Batch 119/1875] [D loss: 3.2961] [G loss: 2.4949] [GAN: 2.1973] [CONST: 0.0015] [TID: 0.1427] [TV: 0.1730]\n",
      "[Epoch 0/10] [Batch 120/1875] [D loss: 3.2964] [G loss: 2.4989] [GAN: 2.1974] [CONST: 0.0016] [TID: 0.1373] [TV: 0.1719]\n",
      "[Epoch 0/10] [Batch 121/1875] [D loss: 3.2965] [G loss: 2.5097] [GAN: 2.1974] [CONST: 0.0015] [TID: 0.1514] [TV: 0.1703]\n",
      "[Epoch 0/10] [Batch 122/1875] [D loss: 3.2963] [G loss: 2.5053] [GAN: 2.1974] [CONST: 0.0015] [TID: 0.1480] [TV: 0.1699]\n",
      "[Epoch 0/10] [Batch 123/1875] [D loss: 3.2963] [G loss: 2.4952] [GAN: 2.1973] [CONST: 0.0015] [TID: 0.1356] [TV: 0.1698]\n",
      "[Epoch 0/10] [Batch 124/1875] [D loss: 3.2962] [G loss: 2.5064] [GAN: 2.1973] [CONST: 0.0015] [TID: 0.1479] [TV: 0.1715]\n",
      "[Epoch 0/10] [Batch 125/1875] [D loss: 3.2963] [G loss: 2.5144] [GAN: 2.1973] [CONST: 0.0015] [TID: 0.1543] [TV: 0.1708]\n",
      "[Epoch 0/10] [Batch 126/1875] [D loss: 3.2963] [G loss: 2.4898] [GAN: 2.1973] [CONST: 0.0015] [TID: 0.1295] [TV: 0.1711]\n",
      "[Epoch 0/10] [Batch 127/1875] [D loss: 3.2961] [G loss: 2.5231] [GAN: 2.1973] [CONST: 0.0018] [TID: 0.1423] [TV: 0.1657]\n",
      "[Epoch 0/10] [Batch 128/1875] [D loss: 3.2965] [G loss: 2.5351] [GAN: 2.1974] [CONST: 0.0017] [TID: 0.1586] [TV: 0.1677]\n",
      "[Epoch 0/10] [Batch 129/1875] [D loss: 3.2961] [G loss: 2.4984] [GAN: 2.1974] [CONST: 0.0014] [TID: 0.1498] [TV: 0.1721]\n",
      "[Epoch 0/10] [Batch 130/1875] [D loss: 3.2963] [G loss: 2.5194] [GAN: 2.1974] [CONST: 0.0016] [TID: 0.1516] [TV: 0.1676]\n",
      "[Epoch 0/10] [Batch 131/1875] [D loss: 3.2961] [G loss: 2.4860] [GAN: 2.1974] [CONST: 0.0015] [TID: 0.1292] [TV: 0.1692]\n",
      "[Epoch 0/10] [Batch 132/1875] [D loss: 3.2963] [G loss: 2.5205] [GAN: 2.1975] [CONST: 0.0017] [TID: 0.1429] [TV: 0.1693]\n",
      "[Epoch 0/10] [Batch 133/1875] [D loss: 3.2963] [G loss: 2.4954] [GAN: 2.1974] [CONST: 0.0016] [TID: 0.1252] [TV: 0.1696]\n",
      "[Epoch 0/10] [Batch 134/1875] [D loss: 3.2961] [G loss: 2.4832] [GAN: 2.1974] [CONST: 0.0015] [TID: 0.1273] [TV: 0.1709]\n",
      "[Epoch 0/10] [Batch 135/1875] [D loss: 3.2962] [G loss: 2.5048] [GAN: 2.1974] [CONST: 0.0015] [TID: 0.1496] [TV: 0.1696]\n",
      "[Epoch 0/10] [Batch 136/1875] [D loss: 3.2964] [G loss: 2.5105] [GAN: 2.1974] [CONST: 0.0014] [TID: 0.1604] [TV: 0.1703]\n",
      "[Epoch 0/10] [Batch 137/1875] [D loss: 3.2961] [G loss: 2.4755] [GAN: 2.1973] [CONST: 0.0014] [TID: 0.1260] [TV: 0.1713]\n",
      "[Epoch 0/10] [Batch 138/1875] [D loss: 3.2961] [G loss: 2.5085] [GAN: 2.1973] [CONST: 0.0015] [TID: 0.1559] [TV: 0.1720]\n",
      "[Epoch 0/10] [Batch 139/1875] [D loss: 3.2961] [G loss: 2.4953] [GAN: 2.1973] [CONST: 0.0015] [TID: 0.1365] [TV: 0.1746]\n",
      "[Epoch 0/10] [Batch 140/1875] [D loss: 3.2964] [G loss: 2.5080] [GAN: 2.1974] [CONST: 0.0014] [TID: 0.1616] [TV: 0.1736]\n",
      "[Epoch 0/10] [Batch 141/1875] [D loss: 3.2960] [G loss: 2.4998] [GAN: 2.1973] [CONST: 0.0015] [TID: 0.1395] [TV: 0.1698]\n",
      "[Epoch 0/10] [Batch 142/1875] [D loss: 3.2961] [G loss: 2.4751] [GAN: 2.1973] [CONST: 0.0013] [TID: 0.1366] [TV: 0.1712]\n",
      "[Epoch 0/10] [Batch 143/1875] [D loss: 3.2964] [G loss: 2.4879] [GAN: 2.1973] [CONST: 0.0014] [TID: 0.1415] [TV: 0.1712]\n",
      "[Epoch 0/10] [Batch 144/1875] [D loss: 3.2961] [G loss: 2.5151] [GAN: 2.1973] [CONST: 0.0014] [TID: 0.1702] [TV: 0.1688]\n",
      "[Epoch 0/10] [Batch 145/1875] [D loss: 3.2962] [G loss: 2.4897] [GAN: 2.1973] [CONST: 0.0014] [TID: 0.1449] [TV: 0.1664]\n",
      "[Epoch 0/10] [Batch 146/1875] [D loss: 3.2963] [G loss: 2.4865] [GAN: 2.1973] [CONST: 0.0015] [TID: 0.1325] [TV: 0.1693]\n",
      "[Epoch 0/10] [Batch 147/1875] [D loss: 3.2961] [G loss: 2.5256] [GAN: 2.1974] [CONST: 0.0016] [TID: 0.1588] [TV: 0.1715]\n",
      "[Epoch 0/10] [Batch 148/1875] [D loss: 3.2961] [G loss: 2.5075] [GAN: 2.1973] [CONST: 0.0016] [TID: 0.1385] [TV: 0.1738]\n",
      "[Epoch 0/10] [Batch 149/1875] [D loss: 3.2961] [G loss: 2.4990] [GAN: 2.1974] [CONST: 0.0015] [TID: 0.1409] [TV: 0.1689]\n",
      "[Epoch 0/10] [Batch 150/1875] [D loss: 3.2960] [G loss: 2.4776] [GAN: 2.1973] [CONST: 0.0014] [TID: 0.1358] [TV: 0.1712]\n",
      "logged samples\n",
      "[Epoch 0/10] [Batch 151/1875] [D loss: 3.2961] [G loss: 2.4849] [GAN: 2.1973] [CONST: 0.0014] [TID: 0.1384] [TV: 0.1684]\n",
      "[Epoch 0/10] [Batch 152/1875] [D loss: 3.2962] [G loss: 2.4695] [GAN: 2.1973] [CONST: 0.0014] [TID: 0.1257] [TV: 0.1700]\n",
      "[Epoch 0/10] [Batch 153/1875] [D loss: 3.2960] [G loss: 2.4826] [GAN: 2.1973] [CONST: 0.0014] [TID: 0.1352] [TV: 0.1703]\n",
      "[Epoch 0/10] [Batch 154/1875] [D loss: 3.2960] [G loss: 2.4771] [GAN: 2.1973] [CONST: 0.0014] [TID: 0.1311] [TV: 0.1724]\n",
      "[Epoch 0/10] [Batch 155/1875] [D loss: 3.2960] [G loss: 2.4757] [GAN: 2.1973] [CONST: 0.0013] [TID: 0.1370] [TV: 0.1713]\n",
      "[Epoch 0/10] [Batch 156/1875] [D loss: 3.2962] [G loss: 2.4418] [GAN: 2.1973] [CONST: 0.0012] [TID: 0.1183] [TV: 0.1705]\n",
      "[Epoch 0/10] [Batch 157/1875] [D loss: 3.2960] [G loss: 2.4603] [GAN: 2.1973] [CONST: 0.0013] [TID: 0.1253] [TV: 0.1683]\n",
      "[Epoch 0/10] [Batch 158/1875] [D loss: 3.2960] [G loss: 2.4600] [GAN: 2.1973] [CONST: 0.0013] [TID: 0.1194] [TV: 0.1687]\n",
      "[Epoch 0/10] [Batch 159/1875] [D loss: 3.2961] [G loss: 2.4899] [GAN: 2.1973] [CONST: 0.0013] [TID: 0.1566] [TV: 0.1715]\n",
      "[Epoch 0/10] [Batch 160/1875] [D loss: 3.2960] [G loss: 2.4594] [GAN: 2.1973] [CONST: 0.0013] [TID: 0.1250] [TV: 0.1662]\n",
      "[Epoch 0/10] [Batch 161/1875] [D loss: 3.2961] [G loss: 2.5087] [GAN: 2.1973] [CONST: 0.0014] [TID: 0.1630] [TV: 0.1687]\n",
      "[Epoch 0/10] [Batch 162/1875] [D loss: 3.2961] [G loss: 2.4512] [GAN: 2.1973] [CONST: 0.0012] [TID: 0.1241] [TV: 0.1693]\n",
      "[Epoch 0/10] [Batch 163/1875] [D loss: 3.2963] [G loss: 2.5073] [GAN: 2.1974] [CONST: 0.0014] [TID: 0.1631] [TV: 0.1682]\n",
      "[Epoch 0/10] [Batch 164/1875] [D loss: 3.2960] [G loss: 2.4612] [GAN: 2.1973] [CONST: 0.0012] [TID: 0.1342] [TV: 0.1704]\n",
      "[Epoch 0/10] [Batch 165/1875] [D loss: 3.2960] [G loss: 2.4545] [GAN: 2.1973] [CONST: 0.0012] [TID: 0.1282] [TV: 0.1682]\n",
      "[Epoch 0/10] [Batch 166/1875] [D loss: 3.2960] [G loss: 2.4541] [GAN: 2.1973] [CONST: 0.0012] [TID: 0.1279] [TV: 0.1674]\n",
      "[Epoch 0/10] [Batch 167/1875] [D loss: 3.2961] [G loss: 2.4577] [GAN: 2.1973] [CONST: 0.0012] [TID: 0.1291] [TV: 0.1660]\n",
      "[Epoch 0/10] [Batch 168/1875] [D loss: 3.2962] [G loss: 2.4577] [GAN: 2.1973] [CONST: 0.0013] [TID: 0.1244] [TV: 0.1699]\n",
      "[Epoch 0/10] [Batch 169/1875] [D loss: 3.2960] [G loss: 2.4608] [GAN: 2.1973] [CONST: 0.0013] [TID: 0.1280] [TV: 0.1669]\n",
      "[Epoch 0/10] [Batch 170/1875] [D loss: 3.2960] [G loss: 2.4496] [GAN: 2.1973] [CONST: 0.0013] [TID: 0.1170] [TV: 0.1693]\n",
      "[Epoch 0/10] [Batch 171/1875] [D loss: 3.2961] [G loss: 2.4847] [GAN: 2.1973] [CONST: 0.0014] [TID: 0.1385] [TV: 0.1704]\n",
      "[Epoch 0/10] [Batch 172/1875] [D loss: 3.2961] [G loss: 2.4681] [GAN: 2.1973] [CONST: 0.0012] [TID: 0.1398] [TV: 0.1705]\n",
      "[Epoch 0/10] [Batch 173/1875] [D loss: 3.2960] [G loss: 2.4636] [GAN: 2.1973] [CONST: 0.0012] [TID: 0.1331] [TV: 0.1655]\n",
      "[Epoch 0/10] [Batch 174/1875] [D loss: 3.2961] [G loss: 2.4512] [GAN: 2.1974] [CONST: 0.0012] [TID: 0.1279] [TV: 0.1695]\n",
      "[Epoch 0/10] [Batch 175/1875] [D loss: 3.2960] [G loss: 2.4649] [GAN: 2.1973] [CONST: 0.0012] [TID: 0.1421] [TV: 0.1655]\n",
      "[Epoch 0/10] [Batch 176/1875] [D loss: 3.2960] [G loss: 2.4472] [GAN: 2.1973] [CONST: 0.0012] [TID: 0.1265] [TV: 0.1653]\n",
      "[Epoch 0/10] [Batch 177/1875] [D loss: 3.2960] [G loss: 2.4702] [GAN: 2.1973] [CONST: 0.0012] [TID: 0.1404] [TV: 0.1696]\n",
      "[Epoch 0/10] [Batch 178/1875] [D loss: 3.2961] [G loss: 2.4520] [GAN: 2.1973] [CONST: 0.0011] [TID: 0.1348] [TV: 0.1695]\n",
      "[Epoch 0/10] [Batch 179/1875] [D loss: 3.2961] [G loss: 2.4445] [GAN: 2.1973] [CONST: 0.0011] [TID: 0.1284] [TV: 0.1680]\n",
      "[Epoch 0/10] [Batch 180/1875] [D loss: 3.2961] [G loss: 2.4598] [GAN: 2.1973] [CONST: 0.0011] [TID: 0.1468] [TV: 0.1675]\n",
      "[Epoch 0/10] [Batch 181/1875] [D loss: 3.2962] [G loss: 2.4789] [GAN: 2.1973] [CONST: 0.0011] [TID: 0.1662] [TV: 0.1662]\n",
      "[Epoch 0/10] [Batch 182/1875] [D loss: 3.2961] [G loss: 2.4466] [GAN: 2.1973] [CONST: 0.0011] [TID: 0.1345] [TV: 0.1652]\n",
      "[Epoch 0/10] [Batch 183/1875] [D loss: 3.2964] [G loss: 2.4844] [GAN: 2.1973] [CONST: 0.0011] [TID: 0.1731] [TV: 0.1622]\n",
      "[Epoch 0/10] [Batch 184/1875] [D loss: 3.2967] [G loss: 2.4297] [GAN: 2.1975] [CONST: 0.0011] [TID: 0.1164] [TV: 0.1634]\n",
      "[Epoch 0/10] [Batch 185/1875] [D loss: 3.2961] [G loss: 2.4614] [GAN: 2.1974] [CONST: 0.0012] [TID: 0.1398] [TV: 0.1644]\n",
      "[Epoch 0/10] [Batch 186/1875] [D loss: 3.2962] [G loss: 2.4951] [GAN: 2.1973] [CONST: 0.0014] [TID: 0.1503] [TV: 0.1625]\n",
      "[Epoch 0/10] [Batch 187/1875] [D loss: 3.2962] [G loss: 2.4644] [GAN: 2.1973] [CONST: 0.0014] [TID: 0.1168] [TV: 0.1651]\n",
      "[Epoch 0/10] [Batch 188/1875] [D loss: 3.2964] [G loss: 2.5006] [GAN: 2.1973] [CONST: 0.0014] [TID: 0.1556] [TV: 0.1654]\n",
      "[Epoch 0/10] [Batch 189/1875] [D loss: 3.2960] [G loss: 2.4890] [GAN: 2.1973] [CONST: 0.0015] [TID: 0.1339] [TV: 0.1670]\n",
      "[Epoch 0/10] [Batch 190/1875] [D loss: 3.2961] [G loss: 2.4643] [GAN: 2.1973] [CONST: 0.0012] [TID: 0.1367] [TV: 0.1681]\n",
      "[Epoch 0/10] [Batch 191/1875] [D loss: 3.2961] [G loss: 2.4297] [GAN: 2.1973] [CONST: 0.0011] [TID: 0.1149] [TV: 0.1647]\n",
      "[Epoch 0/10] [Batch 192/1875] [D loss: 3.2959] [G loss: 2.4277] [GAN: 2.1973] [CONST: 0.0011] [TID: 0.1082] [TV: 0.1655]\n",
      "[Epoch 0/10] [Batch 193/1875] [D loss: 3.2961] [G loss: 2.4607] [GAN: 2.1973] [CONST: 0.0011] [TID: 0.1410] [TV: 0.1621]\n",
      "[Epoch 0/10] [Batch 194/1875] [D loss: 3.2960] [G loss: 2.4569] [GAN: 2.1973] [CONST: 0.0012] [TID: 0.1356] [TV: 0.1644]\n",
      "[Epoch 0/10] [Batch 195/1875] [D loss: 3.2960] [G loss: 2.4507] [GAN: 2.1973] [CONST: 0.0012] [TID: 0.1250] [TV: 0.1680]\n",
      "[Epoch 0/10] [Batch 196/1875] [D loss: 3.2960] [G loss: 2.4567] [GAN: 2.1973] [CONST: 0.0012] [TID: 0.1355] [TV: 0.1673]\n",
      "[Epoch 0/10] [Batch 197/1875] [D loss: 3.2960] [G loss: 2.4511] [GAN: 2.1973] [CONST: 0.0012] [TID: 0.1240] [TV: 0.1660]\n",
      "[Epoch 0/10] [Batch 198/1875] [D loss: 3.2960] [G loss: 2.4374] [GAN: 2.1973] [CONST: 0.0012] [TID: 0.1130] [TV: 0.1667]\n",
      "[Epoch 0/10] [Batch 199/1875] [D loss: 3.2962] [G loss: 2.4403] [GAN: 2.1974] [CONST: 0.0010] [TID: 0.1357] [TV: 0.1677]\n",
      "[Epoch 0/10] [Batch 200/1875] [D loss: 3.2964] [G loss: 2.4461] [GAN: 2.1975] [CONST: 0.0011] [TID: 0.1330] [TV: 0.1659]\n",
      "logged samples\n",
      "âœ… Checkpoint saved: checkpoints/dtn_checkpoint_batch_200.pth\n",
      "[Epoch 0/10] [Batch 201/1875] [D loss: 3.2961] [G loss: 2.4318] [GAN: 2.1973] [CONST: 0.0010] [TID: 0.1251] [TV: 0.1671]\n",
      "[Epoch 0/10] [Batch 202/1875] [D loss: 3.2963] [G loss: 2.4749] [GAN: 2.1973] [CONST: 0.0012] [TID: 0.1512] [TV: 0.1648]\n",
      "[Epoch 0/10] [Batch 203/1875] [D loss: 3.2960] [G loss: 2.4411] [GAN: 2.1973] [CONST: 0.0012] [TID: 0.1157] [TV: 0.1634]\n",
      "[Epoch 0/10] [Batch 204/1875] [D loss: 3.2960] [G loss: 2.4289] [GAN: 2.1973] [CONST: 0.0011] [TID: 0.1158] [TV: 0.1650]\n",
      "[Epoch 0/10] [Batch 205/1875] [D loss: 3.2962] [G loss: 2.4404] [GAN: 2.1973] [CONST: 0.0011] [TID: 0.1269] [TV: 0.1661]\n",
      "[Epoch 0/10] [Batch 206/1875] [D loss: 3.2960] [G loss: 2.4533] [GAN: 2.1972] [CONST: 0.0012] [TID: 0.1264] [TV: 0.1655]\n",
      "[Epoch 0/10] [Batch 207/1875] [D loss: 3.2959] [G loss: 2.4364] [GAN: 2.1973] [CONST: 0.0011] [TID: 0.1176] [TV: 0.1676]\n",
      "[Epoch 0/10] [Batch 208/1875] [D loss: 3.2960] [G loss: 2.4581] [GAN: 2.1973] [CONST: 0.0011] [TID: 0.1452] [TV: 0.1666]\n",
      "[Epoch 0/10] [Batch 209/1875] [D loss: 3.2960] [G loss: 2.4315] [GAN: 2.1973] [CONST: 0.0010] [TID: 0.1219] [TV: 0.1660]\n",
      "[Epoch 0/10] [Batch 210/1875] [D loss: 3.2960] [G loss: 2.4146] [GAN: 2.1973] [CONST: 0.0010] [TID: 0.1077] [TV: 0.1644]\n",
      "[Epoch 0/10] [Batch 211/1875] [D loss: 3.2962] [G loss: 2.4602] [GAN: 2.1973] [CONST: 0.0010] [TID: 0.1511] [TV: 0.1652]\n",
      "[Epoch 0/10] [Batch 212/1875] [D loss: 3.2960] [G loss: 2.4311] [GAN: 2.1973] [CONST: 0.0010] [TID: 0.1231] [TV: 0.1647]\n",
      "[Epoch 0/10] [Batch 213/1875] [D loss: 3.2960] [G loss: 2.4357] [GAN: 2.1973] [CONST: 0.0010] [TID: 0.1300] [TV: 0.1663]\n",
      "[Epoch 0/10] [Batch 214/1875] [D loss: 3.2961] [G loss: 2.4560] [GAN: 2.1974] [CONST: 0.0012] [TID: 0.1320] [TV: 0.1635]\n",
      "[Epoch 0/10] [Batch 215/1875] [D loss: 3.2962] [G loss: 2.4848] [GAN: 2.1973] [CONST: 0.0013] [TID: 0.1519] [TV: 0.1631]\n",
      "[Epoch 0/10] [Batch 216/1875] [D loss: 3.2961] [G loss: 2.4390] [GAN: 2.1974] [CONST: 0.0010] [TID: 0.1331] [TV: 0.1674]\n",
      "[Epoch 0/10] [Batch 217/1875] [D loss: 3.2962] [G loss: 2.4506] [GAN: 2.1973] [CONST: 0.0010] [TID: 0.1448] [TV: 0.1677]\n",
      "[Epoch 0/10] [Batch 218/1875] [D loss: 3.2964] [G loss: 2.4324] [GAN: 2.1975] [CONST: 0.0010] [TID: 0.1220] [TV: 0.1668]\n",
      "[Epoch 0/10] [Batch 219/1875] [D loss: 3.2960] [G loss: 2.4316] [GAN: 2.1973] [CONST: 0.0010] [TID: 0.1274] [TV: 0.1641]\n",
      "[Epoch 0/10] [Batch 220/1875] [D loss: 3.2961] [G loss: 2.4150] [GAN: 2.1973] [CONST: 0.0009] [TID: 0.1159] [TV: 0.1642]\n",
      "[Epoch 0/10] [Batch 221/1875] [D loss: 3.2960] [G loss: 2.4399] [GAN: 2.1973] [CONST: 0.0011] [TID: 0.1288] [TV: 0.1622]\n",
      "[Epoch 0/10] [Batch 222/1875] [D loss: 3.2961] [G loss: 2.4369] [GAN: 2.1973] [CONST: 0.0009] [TID: 0.1389] [TV: 0.1665]\n",
      "[Epoch 0/10] [Batch 223/1875] [D loss: 3.2960] [G loss: 2.4229] [GAN: 2.1973] [CONST: 0.0010] [TID: 0.1188] [TV: 0.1657]\n",
      "[Epoch 0/10] [Batch 224/1875] [D loss: 3.2960] [G loss: 2.4615] [GAN: 2.1973] [CONST: 0.0011] [TID: 0.1478] [TV: 0.1642]\n",
      "[Epoch 0/10] [Batch 225/1875] [D loss: 3.2960] [G loss: 2.4317] [GAN: 2.1973] [CONST: 0.0010] [TID: 0.1218] [TV: 0.1670]\n",
      "[Epoch 0/10] [Batch 226/1875] [D loss: 3.2960] [G loss: 2.4340] [GAN: 2.1973] [CONST: 0.0010] [TID: 0.1295] [TV: 0.1688]\n",
      "[Epoch 0/10] [Batch 227/1875] [D loss: 3.2960] [G loss: 2.4161] [GAN: 2.1973] [CONST: 0.0010] [TID: 0.1115] [TV: 0.1677]\n",
      "[Epoch 0/10] [Batch 228/1875] [D loss: 3.2960] [G loss: 2.4336] [GAN: 2.1973] [CONST: 0.0012] [TID: 0.1057] [TV: 0.1659]\n",
      "[Epoch 0/10] [Batch 229/1875] [D loss: 3.2960] [G loss: 2.4193] [GAN: 2.1973] [CONST: 0.0010] [TID: 0.1184] [TV: 0.1653]\n",
      "[Epoch 0/10] [Batch 230/1875] [D loss: 3.2960] [G loss: 2.4026] [GAN: 2.1973] [CONST: 0.0009] [TID: 0.1096] [TV: 0.1656]\n",
      "[Epoch 0/10] [Batch 231/1875] [D loss: 3.2959] [G loss: 2.4249] [GAN: 2.1973] [CONST: 0.0009] [TID: 0.1250] [TV: 0.1653]\n",
      "[Epoch 0/10] [Batch 232/1875] [D loss: 3.2960] [G loss: 2.4200] [GAN: 2.1973] [CONST: 0.0009] [TID: 0.1225] [TV: 0.1642]\n",
      "[Epoch 0/10] [Batch 233/1875] [D loss: 3.2961] [G loss: 2.4157] [GAN: 2.1973] [CONST: 0.0008] [TID: 0.1307] [TV: 0.1646]\n",
      "[Epoch 0/10] [Batch 234/1875] [D loss: 3.2960] [G loss: 2.4354] [GAN: 2.1973] [CONST: 0.0009] [TID: 0.1387] [TV: 0.1619]\n",
      "[Epoch 0/10] [Batch 235/1875] [D loss: 3.2960] [G loss: 2.4242] [GAN: 2.1973] [CONST: 0.0010] [TID: 0.1202] [TV: 0.1647]\n",
      "[Epoch 0/10] [Batch 236/1875] [D loss: 3.2961] [G loss: 2.4343] [GAN: 2.1973] [CONST: 0.0010] [TID: 0.1338] [TV: 0.1623]\n",
      "[Epoch 0/10] [Batch 237/1875] [D loss: 3.2961] [G loss: 2.4210] [GAN: 2.1973] [CONST: 0.0009] [TID: 0.1301] [TV: 0.1621]\n",
      "[Epoch 0/10] [Batch 238/1875] [D loss: 3.2960] [G loss: 2.4178] [GAN: 2.1973] [CONST: 0.0010] [TID: 0.1127] [TV: 0.1633]\n",
      "[Epoch 0/10] [Batch 239/1875] [D loss: 3.2961] [G loss: 2.4634] [GAN: 2.1973] [CONST: 0.0010] [TID: 0.1606] [TV: 0.1629]\n",
      "[Epoch 0/10] [Batch 240/1875] [D loss: 3.2961] [G loss: 2.4112] [GAN: 2.1974] [CONST: 0.0010] [TID: 0.1098] [TV: 0.1637]\n",
      "[Epoch 0/10] [Batch 241/1875] [D loss: 3.2960] [G loss: 2.4123] [GAN: 2.1973] [CONST: 0.0009] [TID: 0.1216] [TV: 0.1636]\n",
      "[Epoch 0/10] [Batch 242/1875] [D loss: 3.2963] [G loss: 2.4553] [GAN: 2.1973] [CONST: 0.0009] [TID: 0.1628] [TV: 0.1646]\n",
      "[Epoch 0/10] [Batch 243/1875] [D loss: 3.2961] [G loss: 2.4094] [GAN: 2.1973] [CONST: 0.0009] [TID: 0.1174] [TV: 0.1642]\n",
      "[Epoch 0/10] [Batch 244/1875] [D loss: 3.2961] [G loss: 2.4072] [GAN: 2.1974] [CONST: 0.0008] [TID: 0.1168] [TV: 0.1627]\n",
      "[Epoch 0/10] [Batch 245/1875] [D loss: 3.2961] [G loss: 2.4081] [GAN: 2.1974] [CONST: 0.0009] [TID: 0.1102] [TV: 0.1608]\n",
      "[Epoch 0/10] [Batch 246/1875] [D loss: 3.2962] [G loss: 2.4400] [GAN: 2.1973] [CONST: 0.0010] [TID: 0.1346] [TV: 0.1628]\n",
      "[Epoch 0/10] [Batch 247/1875] [D loss: 3.2961] [G loss: 2.4217] [GAN: 2.1974] [CONST: 0.0009] [TID: 0.1241] [TV: 0.1618]\n",
      "[Epoch 0/10] [Batch 248/1875] [D loss: 3.2961] [G loss: 2.4195] [GAN: 2.1974] [CONST: 0.0009] [TID: 0.1285] [TV: 0.1655]\n",
      "[Epoch 0/10] [Batch 249/1875] [D loss: 3.2960] [G loss: 2.4010] [GAN: 2.1974] [CONST: 0.0008] [TID: 0.1166] [TV: 0.1627]\n",
      "[Epoch 0/10] [Batch 250/1875] [D loss: 3.2959] [G loss: 2.4219] [GAN: 2.1973] [CONST: 0.0010] [TID: 0.1162] [TV: 0.1642]\n",
      "logged samples\n",
      "[Epoch 0/10] [Batch 251/1875] [D loss: 3.2960] [G loss: 2.3952] [GAN: 2.1974] [CONST: 0.0008] [TID: 0.1059] [TV: 0.1621]\n",
      "[Epoch 0/10] [Batch 252/1875] [D loss: 3.2959] [G loss: 2.4138] [GAN: 2.1973] [CONST: 0.0009] [TID: 0.1140] [TV: 0.1636]\n",
      "[Epoch 0/10] [Batch 253/1875] [D loss: 3.2961] [G loss: 2.4236] [GAN: 2.1974] [CONST: 0.0009] [TID: 0.1329] [TV: 0.1639]\n",
      "[Epoch 0/10] [Batch 254/1875] [D loss: 3.2961] [G loss: 2.4374] [GAN: 2.1974] [CONST: 0.0010] [TID: 0.1302] [TV: 0.1631]\n",
      "[Epoch 0/10] [Batch 255/1875] [D loss: 3.2960] [G loss: 2.4078] [GAN: 2.1974] [CONST: 0.0009] [TID: 0.1124] [TV: 0.1652]\n",
      "[Epoch 0/10] [Batch 256/1875] [D loss: 3.2960] [G loss: 2.3925] [GAN: 2.1973] [CONST: 0.0008] [TID: 0.1073] [TV: 0.1633]\n",
      "[Epoch 0/10] [Batch 257/1875] [D loss: 3.2959] [G loss: 2.4091] [GAN: 2.1973] [CONST: 0.0008] [TID: 0.1247] [TV: 0.1620]\n",
      "[Epoch 0/10] [Batch 258/1875] [D loss: 3.2959] [G loss: 2.4153] [GAN: 2.1973] [CONST: 0.0010] [TID: 0.1136] [TV: 0.1606]\n",
      "[Epoch 0/10] [Batch 259/1875] [D loss: 3.2960] [G loss: 2.4521] [GAN: 2.1973] [CONST: 0.0010] [TID: 0.1422] [TV: 0.1628]\n",
      "[Epoch 0/10] [Batch 260/1875] [D loss: 3.2961] [G loss: 2.4190] [GAN: 2.1973] [CONST: 0.0009] [TID: 0.1280] [TV: 0.1630]\n",
      "[Epoch 0/10] [Batch 261/1875] [D loss: 3.2961] [G loss: 2.3983] [GAN: 2.1974] [CONST: 0.0008] [TID: 0.1095] [TV: 0.1654]\n",
      "[Epoch 0/10] [Batch 262/1875] [D loss: 3.2960] [G loss: 2.4141] [GAN: 2.1973] [CONST: 0.0009] [TID: 0.1160] [TV: 0.1647]\n",
      "[Epoch 0/10] [Batch 263/1875] [D loss: 3.2961] [G loss: 2.4069] [GAN: 2.1974] [CONST: 0.0008] [TID: 0.1232] [TV: 0.1634]\n",
      "[Epoch 0/10] [Batch 264/1875] [D loss: 3.2960] [G loss: 2.3919] [GAN: 2.1974] [CONST: 0.0007] [TID: 0.1160] [TV: 0.1646]\n",
      "[Epoch 0/10] [Batch 265/1875] [D loss: 3.2961] [G loss: 2.3875] [GAN: 2.1975] [CONST: 0.0008] [TID: 0.1063] [TV: 0.1646]\n",
      "[Epoch 0/10] [Batch 266/1875] [D loss: 3.2960] [G loss: 2.3954] [GAN: 2.1974] [CONST: 0.0008] [TID: 0.1056] [TV: 0.1638]\n",
      "[Epoch 0/10] [Batch 267/1875] [D loss: 3.2962] [G loss: 2.4151] [GAN: 2.1973] [CONST: 0.0008] [TID: 0.1291] [TV: 0.1654]\n",
      "[Epoch 0/10] [Batch 268/1875] [D loss: 3.2961] [G loss: 2.4293] [GAN: 2.1973] [CONST: 0.0009] [TID: 0.1375] [TV: 0.1653]\n",
      "[Epoch 0/10] [Batch 269/1875] [D loss: 3.2960] [G loss: 2.4187] [GAN: 2.1973] [CONST: 0.0010] [TID: 0.1141] [TV: 0.1647]\n",
      "[Epoch 0/10] [Batch 270/1875] [D loss: 3.2960] [G loss: 2.4018] [GAN: 2.1973] [CONST: 0.0008] [TID: 0.1183] [TV: 0.1626]\n",
      "[Epoch 0/10] [Batch 271/1875] [D loss: 3.2960] [G loss: 2.4001] [GAN: 2.1973] [CONST: 0.0008] [TID: 0.1147] [TV: 0.1625]\n",
      "[Epoch 0/10] [Batch 272/1875] [D loss: 3.2960] [G loss: 2.4298] [GAN: 2.1973] [CONST: 0.0010] [TID: 0.1289] [TV: 0.1653]\n",
      "[Epoch 0/10] [Batch 273/1875] [D loss: 3.2961] [G loss: 2.4266] [GAN: 2.1973] [CONST: 0.0010] [TID: 0.1221] [TV: 0.1667]\n",
      "[Epoch 0/10] [Batch 274/1875] [D loss: 3.2961] [G loss: 2.4125] [GAN: 2.1973] [CONST: 0.0008] [TID: 0.1289] [TV: 0.1635]\n",
      "[Epoch 0/10] [Batch 275/1875] [D loss: 3.2960] [G loss: 2.3963] [GAN: 2.1973] [CONST: 0.0008] [TID: 0.1137] [TV: 0.1660]\n",
      "[Epoch 0/10] [Batch 276/1875] [D loss: 3.2960] [G loss: 2.3903] [GAN: 2.1973] [CONST: 0.0007] [TID: 0.1173] [TV: 0.1669]\n",
      "[Epoch 0/10] [Batch 277/1875] [D loss: 3.2962] [G loss: 2.4297] [GAN: 2.1973] [CONST: 0.0010] [TID: 0.1262] [TV: 0.1626]\n",
      "[Epoch 0/10] [Batch 278/1875] [D loss: 3.2960] [G loss: 2.4087] [GAN: 2.1973] [CONST: 0.0008] [TID: 0.1225] [TV: 0.1658]\n",
      "[Epoch 0/10] [Batch 279/1875] [D loss: 3.2960] [G loss: 2.3764] [GAN: 2.1973] [CONST: 0.0007] [TID: 0.0988] [TV: 0.1657]\n",
      "[Epoch 0/10] [Batch 280/1875] [D loss: 3.2960] [G loss: 2.4216] [GAN: 2.1973] [CONST: 0.0009] [TID: 0.1308] [TV: 0.1668]\n",
      "[Epoch 0/10] [Batch 281/1875] [D loss: 3.2959] [G loss: 2.4170] [GAN: 2.1972] [CONST: 0.0008] [TID: 0.1335] [TV: 0.1640]\n",
      "[Epoch 0/10] [Batch 282/1875] [D loss: 3.2960] [G loss: 2.4027] [GAN: 2.1972] [CONST: 0.0007] [TID: 0.1234] [TV: 0.1640]\n",
      "[Epoch 0/10] [Batch 283/1875] [D loss: 3.2960] [G loss: 2.3988] [GAN: 2.1972] [CONST: 0.0007] [TID: 0.1186] [TV: 0.1643]\n",
      "[Epoch 0/10] [Batch 284/1875] [D loss: 3.2960] [G loss: 2.4150] [GAN: 2.1972] [CONST: 0.0008] [TID: 0.1330] [TV: 0.1636]\n",
      "[Epoch 0/10] [Batch 285/1875] [D loss: 3.2959] [G loss: 2.3634] [GAN: 2.1972] [CONST: 0.0006] [TID: 0.0964] [TV: 0.1631]\n",
      "[Epoch 0/10] [Batch 286/1875] [D loss: 3.2960] [G loss: 2.3951] [GAN: 2.1972] [CONST: 0.0007] [TID: 0.1169] [TV: 0.1632]\n",
      "[Epoch 0/10] [Batch 287/1875] [D loss: 3.2960] [G loss: 2.4091] [GAN: 2.1972] [CONST: 0.0007] [TID: 0.1327] [TV: 0.1634]\n",
      "[Epoch 0/10] [Batch 288/1875] [D loss: 3.2960] [G loss: 2.4015] [GAN: 2.1973] [CONST: 0.0007] [TID: 0.1293] [TV: 0.1609]\n",
      "[Epoch 0/10] [Batch 289/1875] [D loss: 3.2960] [G loss: 2.4043] [GAN: 2.1973] [CONST: 0.0008] [TID: 0.1176] [TV: 0.1595]\n",
      "[Epoch 0/10] [Batch 290/1875] [D loss: 3.2960] [G loss: 2.3898] [GAN: 2.1973] [CONST: 0.0008] [TID: 0.1066] [TV: 0.1629]\n",
      "[Epoch 0/10] [Batch 291/1875] [D loss: 3.2960] [G loss: 2.3933] [GAN: 2.1973] [CONST: 0.0008] [TID: 0.1098] [TV: 0.1607]\n",
      "[Epoch 0/10] [Batch 292/1875] [D loss: 3.2960] [G loss: 2.4123] [GAN: 2.1973] [CONST: 0.0009] [TID: 0.1171] [TV: 0.1621]\n",
      "[Epoch 0/10] [Batch 293/1875] [D loss: 3.2959] [G loss: 2.3951] [GAN: 2.1972] [CONST: 0.0007] [TID: 0.1156] [TV: 0.1622]\n",
      "[Epoch 0/10] [Batch 294/1875] [D loss: 3.2960] [G loss: 2.3953] [GAN: 2.1973] [CONST: 0.0008] [TID: 0.1049] [TV: 0.1645]\n",
      "[Epoch 0/10] [Batch 295/1875] [D loss: 3.2960] [G loss: 2.3891] [GAN: 2.1972] [CONST: 0.0007] [TID: 0.1106] [TV: 0.1642]\n",
      "[Epoch 0/10] [Batch 296/1875] [D loss: 3.2960] [G loss: 2.3883] [GAN: 2.1973] [CONST: 0.0007] [TID: 0.1120] [TV: 0.1662]\n",
      "[Epoch 0/10] [Batch 297/1875] [D loss: 3.2959] [G loss: 2.3955] [GAN: 2.1972] [CONST: 0.0007] [TID: 0.1161] [TV: 0.1645]\n",
      "[Epoch 0/10] [Batch 298/1875] [D loss: 3.2959] [G loss: 2.4256] [GAN: 2.1972] [CONST: 0.0010] [TID: 0.1154] [TV: 0.1627]\n",
      "[Epoch 0/10] [Batch 299/1875] [D loss: 3.2959] [G loss: 2.3917] [GAN: 2.1973] [CONST: 0.0008] [TID: 0.1109] [TV: 0.1609]\n",
      "[Epoch 0/10] [Batch 300/1875] [D loss: 3.2959] [G loss: 2.3858] [GAN: 2.1972] [CONST: 0.0007] [TID: 0.1085] [TV: 0.1588]\n",
      "logged samples\n",
      "âœ… Checkpoint saved: checkpoints/dtn_checkpoint_batch_300.pth\n",
      "[Epoch 0/10] [Batch 301/1875] [D loss: 3.2959] [G loss: 2.3898] [GAN: 2.1972] [CONST: 0.0007] [TID: 0.1138] [TV: 0.1581]\n",
      "[Epoch 0/10] [Batch 302/1875] [D loss: 3.2960] [G loss: 2.4112] [GAN: 2.1972] [CONST: 0.0007] [TID: 0.1327] [TV: 0.1607]\n",
      "[Epoch 0/10] [Batch 303/1875] [D loss: 3.2959] [G loss: 2.3986] [GAN: 2.1973] [CONST: 0.0008] [TID: 0.1124] [TV: 0.1623]\n",
      "[Epoch 0/10] [Batch 304/1875] [D loss: 3.2960] [G loss: 2.3932] [GAN: 2.1973] [CONST: 0.0008] [TID: 0.1112] [TV: 0.1626]\n",
      "[Epoch 0/10] [Batch 305/1875] [D loss: 3.2960] [G loss: 2.4105] [GAN: 2.1973] [CONST: 0.0008] [TID: 0.1240] [TV: 0.1618]\n",
      "[Epoch 0/10] [Batch 306/1875] [D loss: 3.2959] [G loss: 2.3985] [GAN: 2.1973] [CONST: 0.0006] [TID: 0.1292] [TV: 0.1611]\n",
      "[Epoch 0/10] [Batch 307/1875] [D loss: 3.2959] [G loss: 2.3971] [GAN: 2.1972] [CONST: 0.0008] [TID: 0.1133] [TV: 0.1605]\n",
      "[Epoch 0/10] [Batch 308/1875] [D loss: 3.2960] [G loss: 2.4432] [GAN: 2.1972] [CONST: 0.0010] [TID: 0.1390] [TV: 0.1600]\n",
      "[Epoch 0/10] [Batch 309/1875] [D loss: 3.2960] [G loss: 2.3923] [GAN: 2.1973] [CONST: 0.0008] [TID: 0.1103] [TV: 0.1624]\n",
      "[Epoch 0/10] [Batch 310/1875] [D loss: 3.2961] [G loss: 2.4026] [GAN: 2.1973] [CONST: 0.0008] [TID: 0.1137] [TV: 0.1647]\n",
      "[Epoch 0/10] [Batch 311/1875] [D loss: 3.2960] [G loss: 2.3974] [GAN: 2.1973] [CONST: 0.0008] [TID: 0.1138] [TV: 0.1652]\n",
      "[Epoch 0/10] [Batch 312/1875] [D loss: 3.2959] [G loss: 2.3845] [GAN: 2.1972] [CONST: 0.0007] [TID: 0.1050] [TV: 0.1661]\n",
      "[Epoch 0/10] [Batch 313/1875] [D loss: 3.2960] [G loss: 2.4222] [GAN: 2.1972] [CONST: 0.0007] [TID: 0.1469] [TV: 0.1650]\n",
      "[Epoch 0/10] [Batch 314/1875] [D loss: 3.2960] [G loss: 2.3878] [GAN: 2.1972] [CONST: 0.0007] [TID: 0.1125] [TV: 0.1647]\n",
      "[Epoch 0/10] [Batch 315/1875] [D loss: 3.2960] [G loss: 2.4014] [GAN: 2.1972] [CONST: 0.0006] [TID: 0.1324] [TV: 0.1632]\n",
      "[Epoch 0/10] [Batch 316/1875] [D loss: 3.2961] [G loss: 2.3927] [GAN: 2.1973] [CONST: 0.0007] [TID: 0.1180] [TV: 0.1632]\n",
      "[Epoch 0/10] [Batch 317/1875] [D loss: 3.2959] [G loss: 2.3803] [GAN: 2.1972] [CONST: 0.0008] [TID: 0.0987] [TV: 0.1612]\n",
      "[Epoch 0/10] [Batch 318/1875] [D loss: 3.2961] [G loss: 2.4009] [GAN: 2.1973] [CONST: 0.0007] [TID: 0.1286] [TV: 0.1647]\n",
      "[Epoch 0/10] [Batch 319/1875] [D loss: 3.2960] [G loss: 2.3910] [GAN: 2.1973] [CONST: 0.0008] [TID: 0.1075] [TV: 0.1631]\n",
      "[Epoch 0/10] [Batch 320/1875] [D loss: 3.2960] [G loss: 2.3772] [GAN: 2.1972] [CONST: 0.0006] [TID: 0.1074] [TV: 0.1662]\n",
      "[Epoch 0/10] [Batch 321/1875] [D loss: 3.2959] [G loss: 2.3804] [GAN: 2.1972] [CONST: 0.0006] [TID: 0.1161] [TV: 0.1642]\n",
      "[Epoch 0/10] [Batch 322/1875] [D loss: 3.2960] [G loss: 2.4025] [GAN: 2.1972] [CONST: 0.0007] [TID: 0.1269] [TV: 0.1635]\n",
      "[Epoch 0/10] [Batch 323/1875] [D loss: 3.2960] [G loss: 2.3712] [GAN: 2.1973] [CONST: 0.0007] [TID: 0.0993] [TV: 0.1656]\n",
      "[Epoch 0/10] [Batch 324/1875] [D loss: 3.2961] [G loss: 2.4108] [GAN: 2.1972] [CONST: 0.0008] [TID: 0.1245] [TV: 0.1651]\n",
      "[Epoch 0/10] [Batch 325/1875] [D loss: 3.2961] [G loss: 2.4028] [GAN: 2.1972] [CONST: 0.0008] [TID: 0.1223] [TV: 0.1632]\n",
      "[Epoch 0/10] [Batch 326/1875] [D loss: 3.2960] [G loss: 2.4045] [GAN: 2.1973] [CONST: 0.0009] [TID: 0.1047] [TV: 0.1622]\n",
      "[Epoch 0/10] [Batch 327/1875] [D loss: 3.2960] [G loss: 2.3578] [GAN: 2.1972] [CONST: 0.0006] [TID: 0.0900] [TV: 0.1624]\n",
      "[Epoch 0/10] [Batch 328/1875] [D loss: 3.2959] [G loss: 2.4094] [GAN: 2.1972] [CONST: 0.0008] [TID: 0.1269] [TV: 0.1616]\n",
      "[Epoch 0/10] [Batch 329/1875] [D loss: 3.2960] [G loss: 2.3799] [GAN: 2.1973] [CONST: 0.0006] [TID: 0.1095] [TV: 0.1641]\n",
      "[Epoch 0/10] [Batch 330/1875] [D loss: 3.2959] [G loss: 2.3955] [GAN: 2.1972] [CONST: 0.0006] [TID: 0.1276] [TV: 0.1626]\n",
      "[Epoch 0/10] [Batch 331/1875] [D loss: 3.2960] [G loss: 2.3965] [GAN: 2.1972] [CONST: 0.0007] [TID: 0.1187] [TV: 0.1633]\n",
      "[Epoch 0/10] [Batch 332/1875] [D loss: 3.2960] [G loss: 2.4170] [GAN: 2.1972] [CONST: 0.0008] [TID: 0.1312] [TV: 0.1644]\n",
      "[Epoch 0/10] [Batch 333/1875] [D loss: 3.2959] [G loss: 2.3951] [GAN: 2.1972] [CONST: 0.0008] [TID: 0.1117] [TV: 0.1614]\n",
      "[Epoch 0/10] [Batch 334/1875] [D loss: 3.2959] [G loss: 2.4007] [GAN: 2.1972] [CONST: 0.0009] [TID: 0.1077] [TV: 0.1616]\n",
      "[Epoch 0/10] [Batch 335/1875] [D loss: 3.2960] [G loss: 2.4000] [GAN: 2.1972] [CONST: 0.0009] [TID: 0.1085] [TV: 0.1619]\n",
      "[Epoch 0/10] [Batch 336/1875] [D loss: 3.2959] [G loss: 2.3901] [GAN: 2.1972] [CONST: 0.0007] [TID: 0.1149] [TV: 0.1606]\n",
      "[Epoch 0/10] [Batch 337/1875] [D loss: 3.2959] [G loss: 2.3754] [GAN: 2.1972] [CONST: 0.0007] [TID: 0.1013] [TV: 0.1637]\n",
      "[Epoch 0/10] [Batch 338/1875] [D loss: 3.2959] [G loss: 2.4013] [GAN: 2.1972] [CONST: 0.0007] [TID: 0.1250] [TV: 0.1626]\n",
      "[Epoch 0/10] [Batch 339/1875] [D loss: 3.2959] [G loss: 2.3755] [GAN: 2.1972] [CONST: 0.0006] [TID: 0.1087] [TV: 0.1620]\n",
      "[Epoch 0/10] [Batch 340/1875] [D loss: 3.2959] [G loss: 2.3824] [GAN: 2.1972] [CONST: 0.0006] [TID: 0.1146] [TV: 0.1617]\n",
      "[Epoch 0/10] [Batch 341/1875] [D loss: 3.2959] [G loss: 2.3973] [GAN: 2.1972] [CONST: 0.0007] [TID: 0.1234] [TV: 0.1604]\n",
      "[Epoch 0/10] [Batch 342/1875] [D loss: 3.2960] [G loss: 2.3944] [GAN: 2.1972] [CONST: 0.0007] [TID: 0.1237] [TV: 0.1608]\n",
      "[Epoch 0/10] [Batch 343/1875] [D loss: 3.2960] [G loss: 2.4156] [GAN: 2.1973] [CONST: 0.0007] [TID: 0.1361] [TV: 0.1614]\n",
      "[Epoch 0/10] [Batch 344/1875] [D loss: 3.2959] [G loss: 2.3877] [GAN: 2.1973] [CONST: 0.0007] [TID: 0.1161] [TV: 0.1598]\n",
      "[Epoch 0/10] [Batch 345/1875] [D loss: 3.2959] [G loss: 2.3927] [GAN: 2.1973] [CONST: 0.0008] [TID: 0.1117] [TV: 0.1632]\n",
      "[Epoch 0/10] [Batch 346/1875] [D loss: 3.2960] [G loss: 2.4004] [GAN: 2.1973] [CONST: 0.0008] [TID: 0.1195] [TV: 0.1612]\n",
      "[Epoch 0/10] [Batch 347/1875] [D loss: 3.2960] [G loss: 2.3948] [GAN: 2.1973] [CONST: 0.0006] [TID: 0.1273] [TV: 0.1623]\n",
      "[Epoch 0/10] [Batch 348/1875] [D loss: 3.2959] [G loss: 2.3735] [GAN: 2.1972] [CONST: 0.0007] [TID: 0.0994] [TV: 0.1641]\n",
      "[Epoch 0/10] [Batch 349/1875] [D loss: 3.2959] [G loss: 2.3872] [GAN: 2.1973] [CONST: 0.0007] [TID: 0.1108] [TV: 0.1608]\n",
      "[Epoch 0/10] [Batch 350/1875] [D loss: 3.2959] [G loss: 2.3740] [GAN: 2.1973] [CONST: 0.0005] [TID: 0.1139] [TV: 0.1612]\n",
      "logged samples\n",
      "[Epoch 0/10] [Batch 351/1875] [D loss: 3.2960] [G loss: 2.3827] [GAN: 2.1973] [CONST: 0.0006] [TID: 0.1179] [TV: 0.1619]\n",
      "[Epoch 0/10] [Batch 352/1875] [D loss: 3.2959] [G loss: 2.3719] [GAN: 2.1973] [CONST: 0.0006] [TID: 0.1044] [TV: 0.1623]\n",
      "[Epoch 0/10] [Batch 353/1875] [D loss: 3.2960] [G loss: 2.3835] [GAN: 2.1973] [CONST: 0.0007] [TID: 0.1080] [TV: 0.1605]\n",
      "[Epoch 0/10] [Batch 354/1875] [D loss: 3.2960] [G loss: 2.3732] [GAN: 2.1973] [CONST: 0.0005] [TID: 0.1148] [TV: 0.1618]\n",
      "[Epoch 0/10] [Batch 355/1875] [D loss: 3.2960] [G loss: 2.3767] [GAN: 2.1973] [CONST: 0.0006] [TID: 0.1104] [TV: 0.1596]\n",
      "[Epoch 0/10] [Batch 356/1875] [D loss: 3.2960] [G loss: 2.3830] [GAN: 2.1973] [CONST: 0.0006] [TID: 0.1223] [TV: 0.1630]\n",
      "[Epoch 0/10] [Batch 357/1875] [D loss: 3.2961] [G loss: 2.3810] [GAN: 2.1974] [CONST: 0.0006] [TID: 0.1155] [TV: 0.1638]\n",
      "[Epoch 0/10] [Batch 358/1875] [D loss: 3.2960] [G loss: 2.3753] [GAN: 2.1973] [CONST: 0.0007] [TID: 0.1045] [TV: 0.1583]\n",
      "[Epoch 0/10] [Batch 359/1875] [D loss: 3.2960] [G loss: 2.3880] [GAN: 2.1973] [CONST: 0.0006] [TID: 0.1195] [TV: 0.1625]\n",
      "[Epoch 0/10] [Batch 360/1875] [D loss: 3.2959] [G loss: 2.3816] [GAN: 2.1973] [CONST: 0.0006] [TID: 0.1146] [TV: 0.1633]\n",
      "[Epoch 0/10] [Batch 361/1875] [D loss: 3.2960] [G loss: 2.4008] [GAN: 2.1973] [CONST: 0.0006] [TID: 0.1318] [TV: 0.1613]\n",
      "[Epoch 0/10] [Batch 362/1875] [D loss: 3.2960] [G loss: 2.3800] [GAN: 2.1973] [CONST: 0.0006] [TID: 0.1135] [TV: 0.1611]\n",
      "[Epoch 0/10] [Batch 363/1875] [D loss: 3.2960] [G loss: 2.3708] [GAN: 2.1973] [CONST: 0.0006] [TID: 0.1081] [TV: 0.1606]\n",
      "[Epoch 0/10] [Batch 364/1875] [D loss: 3.2960] [G loss: 2.3804] [GAN: 2.1973] [CONST: 0.0006] [TID: 0.1112] [TV: 0.1597]\n",
      "[Epoch 0/10] [Batch 365/1875] [D loss: 3.2960] [G loss: 2.3689] [GAN: 2.1973] [CONST: 0.0005] [TID: 0.1101] [TV: 0.1605]\n",
      "[Epoch 0/10] [Batch 366/1875] [D loss: 3.2961] [G loss: 2.4121] [GAN: 2.1973] [CONST: 0.0005] [TID: 0.1574] [TV: 0.1606]\n",
      "[Epoch 0/10] [Batch 367/1875] [D loss: 3.2964] [G loss: 2.3644] [GAN: 2.1975] [CONST: 0.0006] [TID: 0.1034] [TV: 0.1612]\n",
      "[Epoch 0/10] [Batch 368/1875] [D loss: 3.2960] [G loss: 2.3778] [GAN: 2.1973] [CONST: 0.0006] [TID: 0.1078] [TV: 0.1596]\n",
      "[Epoch 0/10] [Batch 369/1875] [D loss: 3.2960] [G loss: 2.3780] [GAN: 2.1972] [CONST: 0.0006] [TID: 0.1101] [TV: 0.1623]\n",
      "[Epoch 0/10] [Batch 370/1875] [D loss: 3.2960] [G loss: 2.3650] [GAN: 2.1972] [CONST: 0.0005] [TID: 0.1096] [TV: 0.1612]\n",
      "[Epoch 0/10] [Batch 371/1875] [D loss: 3.2962] [G loss: 2.3720] [GAN: 2.1972] [CONST: 0.0005] [TID: 0.1168] [TV: 0.1596]\n",
      "[Epoch 0/10] [Batch 372/1875] [D loss: 3.2959] [G loss: 2.3709] [GAN: 2.1972] [CONST: 0.0006] [TID: 0.1047] [TV: 0.1593]\n",
      "[Epoch 0/10] [Batch 373/1875] [D loss: 3.2961] [G loss: 2.3930] [GAN: 2.1972] [CONST: 0.0008] [TID: 0.1108] [TV: 0.1606]\n",
      "[Epoch 0/10] [Batch 374/1875] [D loss: 3.2961] [G loss: 2.3828] [GAN: 2.1973] [CONST: 0.0007] [TID: 0.1031] [TV: 0.1597]\n",
      "[Epoch 0/10] [Batch 375/1875] [D loss: 3.2962] [G loss: 2.3871] [GAN: 2.1973] [CONST: 0.0008] [TID: 0.1027] [TV: 0.1564]\n",
      "[Epoch 0/10] [Batch 376/1875] [D loss: 3.2962] [G loss: 2.3953] [GAN: 2.1972] [CONST: 0.0007] [TID: 0.1189] [TV: 0.1593]\n",
      "[Epoch 0/10] [Batch 377/1875] [D loss: 3.2960] [G loss: 2.3709] [GAN: 2.1973] [CONST: 0.0006] [TID: 0.1073] [TV: 0.1593]\n",
      "[Epoch 0/10] [Batch 378/1875] [D loss: 3.2965] [G loss: 2.4170] [GAN: 2.1973] [CONST: 0.0006] [TID: 0.1469] [TV: 0.1593]\n",
      "[Epoch 0/10] [Batch 379/1875] [D loss: 3.2965] [G loss: 2.3664] [GAN: 2.1976] [CONST: 0.0006] [TID: 0.1040] [TV: 0.1612]\n",
      "[Epoch 0/10] [Batch 380/1875] [D loss: 3.2960] [G loss: 2.3660] [GAN: 2.1973] [CONST: 0.0006] [TID: 0.1009] [TV: 0.1612]\n",
      "[Epoch 0/10] [Batch 381/1875] [D loss: 3.2965] [G loss: 2.3885] [GAN: 2.1974] [CONST: 0.0007] [TID: 0.1168] [TV: 0.1617]\n",
      "[Epoch 0/10] [Batch 382/1875] [D loss: 3.2990] [G loss: 2.3769] [GAN: 2.1979] [CONST: 0.0006] [TID: 0.1114] [TV: 0.1632]\n",
      "[Epoch 0/10] [Batch 383/1875] [D loss: 3.2999] [G loss: 2.3792] [GAN: 2.1984] [CONST: 0.0006] [TID: 0.1175] [TV: 0.1620]\n",
      "[Epoch 0/10] [Batch 384/1875] [D loss: 3.2963] [G loss: 2.3751] [GAN: 2.1979] [CONST: 0.0006] [TID: 0.1097] [TV: 0.1633]\n",
      "[Epoch 0/10] [Batch 385/1875] [D loss: 3.2970] [G loss: 2.3766] [GAN: 2.1984] [CONST: 0.0006] [TID: 0.1127] [TV: 0.1609]\n",
      "[Epoch 0/10] [Batch 386/1875] [D loss: 3.2961] [G loss: 2.3909] [GAN: 2.1979] [CONST: 0.0006] [TID: 0.1241] [TV: 0.1590]\n",
      "[Epoch 0/10] [Batch 387/1875] [D loss: 3.2963] [G loss: 2.3623] [GAN: 2.1978] [CONST: 0.0005] [TID: 0.1028] [TV: 0.1623]\n",
      "[Epoch 0/10] [Batch 388/1875] [D loss: 3.2963] [G loss: 2.3950] [GAN: 2.1978] [CONST: 0.0006] [TID: 0.1340] [TV: 0.1603]\n",
      "[Epoch 0/10] [Batch 389/1875] [D loss: 3.2961] [G loss: 2.3779] [GAN: 2.1977] [CONST: 0.0005] [TID: 0.1191] [TV: 0.1594]\n",
      "[Epoch 0/10] [Batch 390/1875] [D loss: 3.2962] [G loss: 2.3696] [GAN: 2.1977] [CONST: 0.0005] [TID: 0.1111] [TV: 0.1603]\n",
      "[Epoch 0/10] [Batch 391/1875] [D loss: 3.2964] [G loss: 2.3880] [GAN: 2.1975] [CONST: 0.0007] [TID: 0.1172] [TV: 0.1597]\n",
      "[Epoch 0/10] [Batch 392/1875] [D loss: 3.2962] [G loss: 2.3760] [GAN: 2.1976] [CONST: 0.0005] [TID: 0.1169] [TV: 0.1595]\n",
      "[Epoch 0/10] [Batch 393/1875] [D loss: 3.2961] [G loss: 2.3570] [GAN: 2.1974] [CONST: 0.0006] [TID: 0.0952] [TV: 0.1584]\n",
      "[Epoch 0/10] [Batch 394/1875] [D loss: 3.2960] [G loss: 2.3842] [GAN: 2.1974] [CONST: 0.0006] [TID: 0.1239] [TV: 0.1586]\n",
      "[Epoch 0/10] [Batch 395/1875] [D loss: 3.2960] [G loss: 2.3623] [GAN: 2.1974] [CONST: 0.0006] [TID: 0.0997] [TV: 0.1597]\n",
      "[Epoch 0/10] [Batch 396/1875] [D loss: 3.2959] [G loss: 2.3790] [GAN: 2.1974] [CONST: 0.0008] [TID: 0.0953] [TV: 0.1605]\n",
      "[Epoch 0/10] [Batch 397/1875] [D loss: 3.2961] [G loss: 2.4003] [GAN: 2.1973] [CONST: 0.0006] [TID: 0.1357] [TV: 0.1592]\n",
      "[Epoch 0/10] [Batch 398/1875] [D loss: 3.2960] [G loss: 2.3808] [GAN: 2.1973] [CONST: 0.0006] [TID: 0.1180] [TV: 0.1601]\n",
      "[Epoch 0/10] [Batch 399/1875] [D loss: 3.2960] [G loss: 2.3730] [GAN: 2.1973] [CONST: 0.0005] [TID: 0.1133] [TV: 0.1597]\n",
      "[Epoch 0/10] [Batch 400/1875] [D loss: 3.2959] [G loss: 2.3581] [GAN: 2.1973] [CONST: 0.0005] [TID: 0.1016] [TV: 0.1580]\n",
      "logged samples\n",
      "âœ… Checkpoint saved: checkpoints/dtn_checkpoint_batch_400.pth\n",
      "[Epoch 0/10] [Batch 401/1875] [D loss: 3.2961] [G loss: 2.3679] [GAN: 2.1973] [CONST: 0.0006] [TID: 0.1071] [TV: 0.1575]\n",
      "[Epoch 0/10] [Batch 402/1875] [D loss: 3.2960] [G loss: 2.3822] [GAN: 2.1973] [CONST: 0.0007] [TID: 0.1100] [TV: 0.1572]\n",
      "[Epoch 0/10] [Batch 403/1875] [D loss: 3.2962] [G loss: 2.4035] [GAN: 2.1973] [CONST: 0.0007] [TID: 0.1272] [TV: 0.1596]\n",
      "[Epoch 0/10] [Batch 404/1875] [D loss: 3.2963] [G loss: 2.3778] [GAN: 2.1975] [CONST: 0.0008] [TID: 0.0962] [TV: 0.1588]\n",
      "[Epoch 0/10] [Batch 405/1875] [D loss: 3.2960] [G loss: 2.3831] [GAN: 2.1973] [CONST: 0.0008] [TID: 0.1020] [TV: 0.1601]\n",
      "[Epoch 0/10] [Batch 406/1875] [D loss: 3.2960] [G loss: 2.3785] [GAN: 2.1973] [CONST: 0.0007] [TID: 0.1053] [TV: 0.1570]\n",
      "[Epoch 0/10] [Batch 407/1875] [D loss: 3.2961] [G loss: 2.3707] [GAN: 2.1973] [CONST: 0.0005] [TID: 0.1169] [TV: 0.1585]\n",
      "[Epoch 0/10] [Batch 408/1875] [D loss: 3.2960] [G loss: 2.3891] [GAN: 2.1972] [CONST: 0.0006] [TID: 0.1237] [TV: 0.1611]\n",
      "[Epoch 0/10] [Batch 409/1875] [D loss: 3.2959] [G loss: 2.3502] [GAN: 2.1972] [CONST: 0.0005] [TID: 0.0935] [TV: 0.1624]\n",
      "[Epoch 0/10] [Batch 410/1875] [D loss: 3.2960] [G loss: 2.3549] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.1064] [TV: 0.1607]\n",
      "[Epoch 0/10] [Batch 411/1875] [D loss: 3.2961] [G loss: 2.3636] [GAN: 2.1972] [CONST: 0.0006] [TID: 0.0999] [TV: 0.1603]\n",
      "[Epoch 0/10] [Batch 412/1875] [D loss: 3.2959] [G loss: 2.3907] [GAN: 2.1973] [CONST: 0.0007] [TID: 0.1177] [TV: 0.1586]\n",
      "[Epoch 0/10] [Batch 413/1875] [D loss: 3.2959] [G loss: 2.3677] [GAN: 2.1972] [CONST: 0.0006] [TID: 0.1010] [TV: 0.1603]\n",
      "[Epoch 0/10] [Batch 414/1875] [D loss: 3.2959] [G loss: 2.3649] [GAN: 2.1972] [CONST: 0.0006] [TID: 0.1032] [TV: 0.1590]\n",
      "[Epoch 0/10] [Batch 415/1875] [D loss: 3.2959] [G loss: 2.3561] [GAN: 2.1972] [CONST: 0.0006] [TID: 0.0880] [TV: 0.1615]\n",
      "[Epoch 0/10] [Batch 416/1875] [D loss: 3.2959] [G loss: 2.3545] [GAN: 2.1972] [CONST: 0.0005] [TID: 0.0979] [TV: 0.1607]\n",
      "[Epoch 0/10] [Batch 417/1875] [D loss: 3.2959] [G loss: 2.3645] [GAN: 2.1972] [CONST: 0.0005] [TID: 0.1046] [TV: 0.1592]\n",
      "[Epoch 0/10] [Batch 418/1875] [D loss: 3.2959] [G loss: 2.3679] [GAN: 2.1972] [CONST: 0.0005] [TID: 0.1077] [TV: 0.1611]\n",
      "[Epoch 0/10] [Batch 419/1875] [D loss: 3.2959] [G loss: 2.3678] [GAN: 2.1972] [CONST: 0.0006] [TID: 0.0982] [TV: 0.1591]\n",
      "[Epoch 0/10] [Batch 420/1875] [D loss: 3.2960] [G loss: 2.3662] [GAN: 2.1972] [CONST: 0.0005] [TID: 0.1122] [TV: 0.1594]\n",
      "[Epoch 0/10] [Batch 421/1875] [D loss: 3.2959] [G loss: 2.3725] [GAN: 2.1972] [CONST: 0.0005] [TID: 0.1167] [TV: 0.1614]\n",
      "[Epoch 0/10] [Batch 422/1875] [D loss: 3.2960] [G loss: 2.3592] [GAN: 2.1973] [CONST: 0.0005] [TID: 0.1073] [TV: 0.1595]\n",
      "[Epoch 0/10] [Batch 423/1875] [D loss: 3.2959] [G loss: 2.3574] [GAN: 2.1972] [CONST: 0.0005] [TID: 0.0997] [TV: 0.1601]\n",
      "[Epoch 0/10] [Batch 424/1875] [D loss: 3.2960] [G loss: 2.3717] [GAN: 2.1973] [CONST: 0.0005] [TID: 0.1116] [TV: 0.1598]\n",
      "[Epoch 0/10] [Batch 425/1875] [D loss: 3.2961] [G loss: 2.3701] [GAN: 2.1973] [CONST: 0.0006] [TID: 0.1088] [TV: 0.1567]\n",
      "[Epoch 0/10] [Batch 426/1875] [D loss: 3.2960] [G loss: 2.3575] [GAN: 2.1973] [CONST: 0.0005] [TID: 0.1016] [TV: 0.1599]\n",
      "[Epoch 0/10] [Batch 427/1875] [D loss: 3.2960] [G loss: 2.3680] [GAN: 2.1972] [CONST: 0.0006] [TID: 0.1049] [TV: 0.1594]\n",
      "[Epoch 0/10] [Batch 428/1875] [D loss: 3.2960] [G loss: 2.3608] [GAN: 2.1973] [CONST: 0.0006] [TID: 0.0949] [TV: 0.1594]\n",
      "[Epoch 0/10] [Batch 429/1875] [D loss: 3.2959] [G loss: 2.3576] [GAN: 2.1972] [CONST: 0.0006] [TID: 0.0946] [TV: 0.1598]\n",
      "[Epoch 0/10] [Batch 430/1875] [D loss: 3.2960] [G loss: 2.3654] [GAN: 2.1973] [CONST: 0.0005] [TID: 0.1061] [TV: 0.1591]\n",
      "[Epoch 0/10] [Batch 431/1875] [D loss: 3.2959] [G loss: 2.3846] [GAN: 2.1973] [CONST: 0.0005] [TID: 0.1252] [TV: 0.1584]\n",
      "[Epoch 0/10] [Batch 432/1875] [D loss: 3.2959] [G loss: 2.3722] [GAN: 2.1973] [CONST: 0.0006] [TID: 0.1111] [TV: 0.1580]\n",
      "[Epoch 0/10] [Batch 433/1875] [D loss: 3.2960] [G loss: 2.3614] [GAN: 2.1973] [CONST: 0.0005] [TID: 0.1029] [TV: 0.1609]\n",
      "[Epoch 0/10] [Batch 434/1875] [D loss: 3.2961] [G loss: 2.3775] [GAN: 2.1973] [CONST: 0.0005] [TID: 0.1200] [TV: 0.1582]\n",
      "[Epoch 0/10] [Batch 435/1875] [D loss: 3.2959] [G loss: 2.3681] [GAN: 2.1972] [CONST: 0.0005] [TID: 0.1106] [TV: 0.1580]\n",
      "[Epoch 0/10] [Batch 436/1875] [D loss: 3.2959] [G loss: 2.3667] [GAN: 2.1972] [CONST: 0.0005] [TID: 0.1115] [TV: 0.1594]\n",
      "[Epoch 0/10] [Batch 437/1875] [D loss: 3.2959] [G loss: 2.3661] [GAN: 2.1972] [CONST: 0.0006] [TID: 0.1054] [TV: 0.1572]\n",
      "[Epoch 0/10] [Batch 438/1875] [D loss: 3.2959] [G loss: 2.3561] [GAN: 2.1972] [CONST: 0.0005] [TID: 0.1050] [TV: 0.1584]\n",
      "[Epoch 0/10] [Batch 439/1875] [D loss: 3.2960] [G loss: 2.3593] [GAN: 2.1972] [CONST: 0.0005] [TID: 0.1073] [TV: 0.1611]\n",
      "[Epoch 0/10] [Batch 440/1875] [D loss: 3.2960] [G loss: 2.3742] [GAN: 2.1972] [CONST: 0.0005] [TID: 0.1205] [TV: 0.1611]\n",
      "[Epoch 0/10] [Batch 441/1875] [D loss: 3.2960] [G loss: 2.3615] [GAN: 2.1973] [CONST: 0.0006] [TID: 0.0993] [TV: 0.1613]\n",
      "[Epoch 0/10] [Batch 442/1875] [D loss: 3.2961] [G loss: 2.3655] [GAN: 2.1972] [CONST: 0.0005] [TID: 0.1108] [TV: 0.1603]\n",
      "[Epoch 0/10] [Batch 443/1875] [D loss: 3.2967] [G loss: 2.3749] [GAN: 2.1975] [CONST: 0.0005] [TID: 0.1167] [TV: 0.1590]\n",
      "[Epoch 0/10] [Batch 444/1875] [D loss: 3.2970] [G loss: 2.3588] [GAN: 2.1977] [CONST: 0.0005] [TID: 0.1071] [TV: 0.1581]\n",
      "[Epoch 0/10] [Batch 445/1875] [D loss: 3.2960] [G loss: 2.3638] [GAN: 2.1975] [CONST: 0.0005] [TID: 0.1109] [TV: 0.1586]\n",
      "[Epoch 0/10] [Batch 446/1875] [D loss: 3.2961] [G loss: 2.3642] [GAN: 2.1976] [CONST: 0.0004] [TID: 0.1149] [TV: 0.1578]\n",
      "[Epoch 0/10] [Batch 447/1875] [D loss: 3.2960] [G loss: 2.3392] [GAN: 2.1975] [CONST: 0.0005] [TID: 0.0850] [TV: 0.1569]\n",
      "[Epoch 0/10] [Batch 448/1875] [D loss: 3.2960] [G loss: 2.3736] [GAN: 2.1975] [CONST: 0.0006] [TID: 0.1054] [TV: 0.1564]\n",
      "[Epoch 0/10] [Batch 449/1875] [D loss: 3.2960] [G loss: 2.3549] [GAN: 2.1974] [CONST: 0.0005] [TID: 0.0978] [TV: 0.1558]\n",
      "[Epoch 0/10] [Batch 450/1875] [D loss: 3.2959] [G loss: 2.3572] [GAN: 2.1974] [CONST: 0.0004] [TID: 0.1108] [TV: 0.1583]\n",
      "logged samples\n",
      "[Epoch 0/10] [Batch 451/1875] [D loss: 3.2960] [G loss: 2.3525] [GAN: 2.1975] [CONST: 0.0004] [TID: 0.1022] [TV: 0.1582]\n",
      "[Epoch 0/10] [Batch 452/1875] [D loss: 3.2960] [G loss: 2.3461] [GAN: 2.1974] [CONST: 0.0004] [TID: 0.0997] [TV: 0.1573]\n",
      "[Epoch 0/10] [Batch 453/1875] [D loss: 3.2960] [G loss: 2.3675] [GAN: 2.1974] [CONST: 0.0005] [TID: 0.1165] [TV: 0.1591]\n",
      "[Epoch 0/10] [Batch 454/1875] [D loss: 3.2960] [G loss: 2.3681] [GAN: 2.1974] [CONST: 0.0005] [TID: 0.1146] [TV: 0.1572]\n",
      "[Epoch 0/10] [Batch 455/1875] [D loss: 3.2960] [G loss: 2.3616] [GAN: 2.1973] [CONST: 0.0005] [TID: 0.1066] [TV: 0.1580]\n",
      "[Epoch 0/10] [Batch 456/1875] [D loss: 3.2959] [G loss: 2.3698] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.1209] [TV: 0.1586]\n",
      "[Epoch 0/10] [Batch 457/1875] [D loss: 3.2959] [G loss: 2.3745] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.1286] [TV: 0.1567]\n",
      "[Epoch 0/10] [Batch 458/1875] [D loss: 3.2960] [G loss: 2.3481] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.0985] [TV: 0.1569]\n",
      "[Epoch 0/10] [Batch 459/1875] [D loss: 3.2961] [G loss: 2.3629] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.1148] [TV: 0.1559]\n",
      "[Epoch 0/10] [Batch 460/1875] [D loss: 3.2961] [G loss: 2.3568] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.1127] [TV: 0.1581]\n",
      "[Epoch 0/10] [Batch 461/1875] [D loss: 3.2960] [G loss: 2.3498] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.1067] [TV: 0.1583]\n",
      "[Epoch 0/10] [Batch 462/1875] [D loss: 3.2961] [G loss: 2.3517] [GAN: 2.1974] [CONST: 0.0004] [TID: 0.1019] [TV: 0.1565]\n",
      "[Epoch 0/10] [Batch 463/1875] [D loss: 3.2960] [G loss: 2.3519] [GAN: 2.1973] [CONST: 0.0005] [TID: 0.0979] [TV: 0.1568]\n",
      "[Epoch 0/10] [Batch 464/1875] [D loss: 3.2962] [G loss: 2.3758] [GAN: 2.1973] [CONST: 0.0005] [TID: 0.1179] [TV: 0.1559]\n",
      "[Epoch 0/10] [Batch 465/1875] [D loss: 3.2960] [G loss: 2.3511] [GAN: 2.1974] [CONST: 0.0005] [TID: 0.0966] [TV: 0.1584]\n",
      "[Epoch 0/10] [Batch 466/1875] [D loss: 3.2961] [G loss: 2.3577] [GAN: 2.1974] [CONST: 0.0005] [TID: 0.0996] [TV: 0.1587]\n",
      "[Epoch 0/10] [Batch 467/1875] [D loss: 3.2961] [G loss: 2.3753] [GAN: 2.1973] [CONST: 0.0006] [TID: 0.1085] [TV: 0.1570]\n",
      "[Epoch 0/10] [Batch 468/1875] [D loss: 3.2960] [G loss: 2.3741] [GAN: 2.1974] [CONST: 0.0007] [TID: 0.0982] [TV: 0.1584]\n",
      "[Epoch 0/10] [Batch 469/1875] [D loss: 3.2960] [G loss: 2.3725] [GAN: 2.1974] [CONST: 0.0006] [TID: 0.1044] [TV: 0.1585]\n",
      "[Epoch 0/10] [Batch 470/1875] [D loss: 3.2959] [G loss: 2.3541] [GAN: 2.1973] [CONST: 0.0005] [TID: 0.0980] [TV: 0.1577]\n",
      "[Epoch 0/10] [Batch 471/1875] [D loss: 3.2959] [G loss: 2.3891] [GAN: 2.1973] [CONST: 0.0006] [TID: 0.1262] [TV: 0.1573]\n",
      "[Epoch 0/10] [Batch 472/1875] [D loss: 3.2959] [G loss: 2.3476] [GAN: 2.1973] [CONST: 0.0005] [TID: 0.0943] [TV: 0.1598]\n",
      "[Epoch 0/10] [Batch 473/1875] [D loss: 3.2959] [G loss: 2.3682] [GAN: 2.1973] [CONST: 0.0006] [TID: 0.1070] [TV: 0.1589]\n",
      "[Epoch 0/10] [Batch 474/1875] [D loss: 3.2959] [G loss: 2.3454] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.0966] [TV: 0.1595]\n",
      "[Epoch 0/10] [Batch 475/1875] [D loss: 3.2960] [G loss: 2.3675] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.1189] [TV: 0.1587]\n",
      "[Epoch 0/10] [Batch 476/1875] [D loss: 3.2959] [G loss: 2.3404] [GAN: 2.1973] [CONST: 0.0005] [TID: 0.0872] [TV: 0.1592]\n",
      "[Epoch 0/10] [Batch 477/1875] [D loss: 3.2959] [G loss: 2.3673] [GAN: 2.1973] [CONST: 0.0005] [TID: 0.1121] [TV: 0.1586]\n",
      "[Epoch 0/10] [Batch 478/1875] [D loss: 3.2959] [G loss: 2.3578] [GAN: 2.1973] [CONST: 0.0005] [TID: 0.1015] [TV: 0.1578]\n",
      "[Epoch 0/10] [Batch 479/1875] [D loss: 3.2959] [G loss: 2.3478] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.0981] [TV: 0.1562]\n",
      "[Epoch 0/10] [Batch 480/1875] [D loss: 3.2959] [G loss: 2.3626] [GAN: 2.1973] [CONST: 0.0006] [TID: 0.0974] [TV: 0.1580]\n",
      "[Epoch 0/10] [Batch 481/1875] [D loss: 3.2959] [G loss: 2.3584] [GAN: 2.1973] [CONST: 0.0006] [TID: 0.0945] [TV: 0.1569]\n",
      "[Epoch 0/10] [Batch 482/1875] [D loss: 3.2961] [G loss: 2.3740] [GAN: 2.1973] [CONST: 0.0005] [TID: 0.1168] [TV: 0.1578]\n",
      "[Epoch 0/10] [Batch 483/1875] [D loss: 3.2962] [G loss: 2.3420] [GAN: 2.1975] [CONST: 0.0005] [TID: 0.0909] [TV: 0.1577]\n",
      "[Epoch 0/10] [Batch 484/1875] [D loss: 3.2962] [G loss: 2.3641] [GAN: 2.1974] [CONST: 0.0005] [TID: 0.1108] [TV: 0.1575]\n",
      "[Epoch 0/10] [Batch 485/1875] [D loss: 3.2961] [G loss: 2.3519] [GAN: 2.1975] [CONST: 0.0004] [TID: 0.1018] [TV: 0.1585]\n",
      "[Epoch 0/10] [Batch 486/1875] [D loss: 3.2959] [G loss: 2.3666] [GAN: 2.1974] [CONST: 0.0004] [TID: 0.1206] [TV: 0.1585]\n",
      "[Epoch 0/10] [Batch 487/1875] [D loss: 3.2959] [G loss: 2.3508] [GAN: 2.1974] [CONST: 0.0005] [TID: 0.0952] [TV: 0.1571]\n",
      "[Epoch 0/10] [Batch 488/1875] [D loss: 3.2959] [G loss: 2.3776] [GAN: 2.1973] [CONST: 0.0005] [TID: 0.1272] [TV: 0.1589]\n",
      "[Epoch 0/10] [Batch 489/1875] [D loss: 3.2959] [G loss: 2.3510] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.1017] [TV: 0.1570]\n",
      "[Epoch 0/10] [Batch 490/1875] [D loss: 3.2959] [G loss: 2.3597] [GAN: 2.1973] [CONST: 0.0005] [TID: 0.1024] [TV: 0.1584]\n",
      "[Epoch 0/10] [Batch 491/1875] [D loss: 3.2959] [G loss: 2.3558] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.1063] [TV: 0.1603]\n",
      "[Epoch 0/10] [Batch 492/1875] [D loss: 3.2959] [G loss: 2.3670] [GAN: 2.1973] [CONST: 0.0005] [TID: 0.1129] [TV: 0.1581]\n",
      "[Epoch 0/10] [Batch 493/1875] [D loss: 3.2959] [G loss: 2.3593] [GAN: 2.1973] [CONST: 0.0005] [TID: 0.1039] [TV: 0.1605]\n",
      "[Epoch 0/10] [Batch 494/1875] [D loss: 3.2959] [G loss: 2.3551] [GAN: 2.1973] [CONST: 0.0005] [TID: 0.1034] [TV: 0.1585]\n",
      "[Epoch 0/10] [Batch 495/1875] [D loss: 3.2959] [G loss: 2.3361] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.0871] [TV: 0.1571]\n",
      "[Epoch 0/10] [Batch 496/1875] [D loss: 3.2959] [G loss: 2.3406] [GAN: 2.1972] [CONST: 0.0005] [TID: 0.0901] [TV: 0.1554]\n",
      "[Epoch 0/10] [Batch 497/1875] [D loss: 3.2959] [G loss: 2.3468] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.1032] [TV: 0.1579]\n",
      "[Epoch 0/10] [Batch 498/1875] [D loss: 3.2959] [G loss: 2.3494] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.0998] [TV: 0.1572]\n",
      "[Epoch 0/10] [Batch 499/1875] [D loss: 3.2959] [G loss: 2.3568] [GAN: 2.1972] [CONST: 0.0005] [TID: 0.1066] [TV: 0.1573]\n",
      "[Epoch 0/10] [Batch 500/1875] [D loss: 3.2959] [G loss: 2.3331] [GAN: 2.1972] [CONST: 0.0005] [TID: 0.0813] [TV: 0.1573]\n",
      "logged samples\n",
      "âœ… Checkpoint saved: checkpoints/dtn_checkpoint_batch_500.pth\n",
      "[Epoch 0/10] [Batch 501/1875] [D loss: 3.2959] [G loss: 2.3427] [GAN: 2.1972] [CONST: 0.0005] [TID: 0.0918] [TV: 0.1576]\n",
      "[Epoch 0/10] [Batch 502/1875] [D loss: 3.2959] [G loss: 2.3605] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.1182] [TV: 0.1580]\n",
      "[Epoch 0/10] [Batch 503/1875] [D loss: 3.2959] [G loss: 2.3459] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.1030] [TV: 0.1575]\n",
      "[Epoch 0/10] [Batch 504/1875] [D loss: 3.2959] [G loss: 2.3701] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.1246] [TV: 0.1589]\n",
      "[Epoch 0/10] [Batch 505/1875] [D loss: 3.2961] [G loss: 2.3756] [GAN: 2.1972] [CONST: 0.0005] [TID: 0.1252] [TV: 0.1566]\n",
      "[Epoch 0/10] [Batch 506/1875] [D loss: 3.2960] [G loss: 2.3435] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.0981] [TV: 0.1579]\n",
      "[Epoch 0/10] [Batch 507/1875] [D loss: 3.2964] [G loss: 2.3456] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.0967] [TV: 0.1573]\n",
      "[Epoch 0/10] [Batch 508/1875] [D loss: 3.2963] [G loss: 2.3592] [GAN: 2.1973] [CONST: 0.0005] [TID: 0.1078] [TV: 0.1590]\n",
      "[Epoch 0/10] [Batch 509/1875] [D loss: 3.2960] [G loss: 2.3429] [GAN: 2.1973] [CONST: 0.0005] [TID: 0.0865] [TV: 0.1573]\n",
      "[Epoch 0/10] [Batch 510/1875] [D loss: 3.2959] [G loss: 2.3488] [GAN: 2.1973] [CONST: 0.0005] [TID: 0.0967] [TV: 0.1581]\n",
      "[Epoch 0/10] [Batch 511/1875] [D loss: 3.2959] [G loss: 2.3613] [GAN: 2.1973] [CONST: 0.0006] [TID: 0.0946] [TV: 0.1576]\n",
      "[Epoch 0/10] [Batch 512/1875] [D loss: 3.2959] [G loss: 2.3540] [GAN: 2.1972] [CONST: 0.0005] [TID: 0.0968] [TV: 0.1591]\n",
      "[Epoch 0/10] [Batch 513/1875] [D loss: 3.2959] [G loss: 2.3644] [GAN: 2.1972] [CONST: 0.0005] [TID: 0.1057] [TV: 0.1566]\n",
      "[Epoch 0/10] [Batch 514/1875] [D loss: 3.2959] [G loss: 2.3555] [GAN: 2.1972] [CONST: 0.0005] [TID: 0.0961] [TV: 0.1596]\n",
      "[Epoch 0/10] [Batch 515/1875] [D loss: 3.2960] [G loss: 2.3663] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.1194] [TV: 0.1561]\n",
      "[Epoch 0/10] [Batch 516/1875] [D loss: 3.2959] [G loss: 2.3660] [GAN: 2.1972] [CONST: 0.0005] [TID: 0.1070] [TV: 0.1554]\n",
      "[Epoch 0/10] [Batch 517/1875] [D loss: 3.2959] [G loss: 2.3602] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.1107] [TV: 0.1561]\n",
      "[Epoch 0/10] [Batch 518/1875] [D loss: 3.2959] [G loss: 2.3370] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.0898] [TV: 0.1575]\n",
      "[Epoch 0/10] [Batch 519/1875] [D loss: 3.2959] [G loss: 2.3861] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.1364] [TV: 0.1570]\n",
      "[Epoch 0/10] [Batch 520/1875] [D loss: 3.2959] [G loss: 2.3501] [GAN: 2.1972] [CONST: 0.0005] [TID: 0.0999] [TV: 0.1560]\n",
      "[Epoch 0/10] [Batch 521/1875] [D loss: 3.2959] [G loss: 2.3544] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.1056] [TV: 0.1579]\n",
      "[Epoch 0/10] [Batch 522/1875] [D loss: 3.2959] [G loss: 2.3395] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.0961] [TV: 0.1589]\n",
      "[Epoch 0/10] [Batch 523/1875] [D loss: 3.2959] [G loss: 2.3415] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.1000] [TV: 0.1578]\n",
      "[Epoch 0/10] [Batch 524/1875] [D loss: 3.2959] [G loss: 2.3364] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.0914] [TV: 0.1571]\n",
      "[Epoch 0/10] [Batch 525/1875] [D loss: 3.2959] [G loss: 2.3523] [GAN: 2.1972] [CONST: 0.0005] [TID: 0.0989] [TV: 0.1564]\n",
      "[Epoch 0/10] [Batch 526/1875] [D loss: 3.2959] [G loss: 2.3565] [GAN: 2.1972] [CONST: 0.0005] [TID: 0.1013] [TV: 0.1578]\n",
      "[Epoch 0/10] [Batch 527/1875] [D loss: 3.2959] [G loss: 2.3503] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.1043] [TV: 0.1567]\n",
      "[Epoch 0/10] [Batch 528/1875] [D loss: 3.2959] [G loss: 2.3650] [GAN: 2.1972] [CONST: 0.0005] [TID: 0.1122] [TV: 0.1570]\n",
      "[Epoch 0/10] [Batch 529/1875] [D loss: 3.2959] [G loss: 2.3366] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.0961] [TV: 0.1571]\n",
      "[Epoch 0/10] [Batch 530/1875] [D loss: 3.2959] [G loss: 2.3654] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.1211] [TV: 0.1571]\n",
      "[Epoch 0/10] [Batch 531/1875] [D loss: 3.2959] [G loss: 2.3486] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.1002] [TV: 0.1564]\n",
      "[Epoch 0/10] [Batch 532/1875] [D loss: 3.2960] [G loss: 2.3535] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.1142] [TV: 0.1570]\n",
      "[Epoch 0/10] [Batch 533/1875] [D loss: 3.2959] [G loss: 2.3439] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.0968] [TV: 0.1580]\n",
      "[Epoch 0/10] [Batch 534/1875] [D loss: 3.2960] [G loss: 2.3603] [GAN: 2.1972] [CONST: 0.0005] [TID: 0.1048] [TV: 0.1563]\n",
      "[Epoch 0/10] [Batch 535/1875] [D loss: 3.2960] [G loss: 2.3457] [GAN: 2.1972] [CONST: 0.0005] [TID: 0.0903] [TV: 0.1576]\n",
      "[Epoch 0/10] [Batch 536/1875] [D loss: 3.2959] [G loss: 2.3449] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.1027] [TV: 0.1576]\n",
      "[Epoch 0/10] [Batch 537/1875] [D loss: 3.2959] [G loss: 2.3500] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.1007] [TV: 0.1569]\n",
      "[Epoch 0/10] [Batch 538/1875] [D loss: 3.2959] [G loss: 2.3575] [GAN: 2.1972] [CONST: 0.0005] [TID: 0.1004] [TV: 0.1579]\n",
      "[Epoch 0/10] [Batch 539/1875] [D loss: 3.2959] [G loss: 2.3841] [GAN: 2.1972] [CONST: 0.0006] [TID: 0.1211] [TV: 0.1587]\n",
      "[Epoch 0/10] [Batch 540/1875] [D loss: 3.2959] [G loss: 2.3828] [GAN: 2.1972] [CONST: 0.0005] [TID: 0.1307] [TV: 0.1581]\n",
      "[Epoch 0/10] [Batch 541/1875] [D loss: 3.2959] [G loss: 2.3360] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.0914] [TV: 0.1588]\n",
      "[Epoch 0/10] [Batch 542/1875] [D loss: 3.2959] [G loss: 2.3524] [GAN: 2.1972] [CONST: 0.0005] [TID: 0.1017] [TV: 0.1571]\n",
      "[Epoch 0/10] [Batch 543/1875] [D loss: 3.2959] [G loss: 2.3548] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.1063] [TV: 0.1571]\n",
      "[Epoch 0/10] [Batch 544/1875] [D loss: 3.2959] [G loss: 2.3666] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.1209] [TV: 0.1597]\n",
      "[Epoch 0/10] [Batch 545/1875] [D loss: 3.2960] [G loss: 2.3375] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.0949] [TV: 0.1573]\n",
      "[Epoch 0/10] [Batch 546/1875] [D loss: 3.2960] [G loss: 2.3491] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.1049] [TV: 0.1577]\n",
      "[Epoch 0/10] [Batch 547/1875] [D loss: 3.2960] [G loss: 2.3370] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.0939] [TV: 0.1556]\n",
      "[Epoch 0/10] [Batch 548/1875] [D loss: 3.2960] [G loss: 2.3709] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.1246] [TV: 0.1568]\n",
      "[Epoch 0/10] [Batch 549/1875] [D loss: 3.2960] [G loss: 2.3752] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.1315] [TV: 0.1577]\n",
      "[Epoch 0/10] [Batch 550/1875] [D loss: 3.2959] [G loss: 2.3412] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1020] [TV: 0.1575]\n",
      "logged samples\n",
      "[Epoch 0/10] [Batch 551/1875] [D loss: 3.2959] [G loss: 2.3449] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.1027] [TV: 0.1558]\n",
      "[Epoch 0/10] [Batch 552/1875] [D loss: 3.2962] [G loss: 2.3648] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.1214] [TV: 0.1558]\n",
      "[Epoch 0/10] [Batch 553/1875] [D loss: 3.2974] [G loss: 2.3514] [GAN: 2.1981] [CONST: 0.0004] [TID: 0.1103] [TV: 0.1567]\n",
      "[Epoch 0/10] [Batch 554/1875] [D loss: 3.2962] [G loss: 2.3563] [GAN: 2.1975] [CONST: 0.0004] [TID: 0.1123] [TV: 0.1566]\n",
      "[Epoch 0/10] [Batch 555/1875] [D loss: 3.2960] [G loss: 2.3556] [GAN: 2.1976] [CONST: 0.0005] [TID: 0.1015] [TV: 0.1569]\n",
      "[Epoch 0/10] [Batch 556/1875] [D loss: 3.2960] [G loss: 2.3497] [GAN: 2.1975] [CONST: 0.0005] [TID: 0.0942] [TV: 0.1558]\n",
      "[Epoch 0/10] [Batch 557/1875] [D loss: 3.2960] [G loss: 2.3368] [GAN: 2.1975] [CONST: 0.0005] [TID: 0.0864] [TV: 0.1565]\n",
      "[Epoch 0/10] [Batch 558/1875] [D loss: 3.2959] [G loss: 2.3401] [GAN: 2.1975] [CONST: 0.0004] [TID: 0.0957] [TV: 0.1573]\n",
      "[Epoch 0/10] [Batch 559/1875] [D loss: 3.2959] [G loss: 2.3600] [GAN: 2.1974] [CONST: 0.0005] [TID: 0.1057] [TV: 0.1569]\n",
      "[Epoch 0/10] [Batch 560/1875] [D loss: 3.2959] [G loss: 2.3360] [GAN: 2.1974] [CONST: 0.0004] [TID: 0.0943] [TV: 0.1587]\n",
      "[Epoch 0/10] [Batch 561/1875] [D loss: 3.2960] [G loss: 2.3824] [GAN: 2.1974] [CONST: 0.0004] [TID: 0.1379] [TV: 0.1589]\n",
      "[Epoch 0/10] [Batch 562/1875] [D loss: 3.2963] [G loss: 2.3690] [GAN: 2.1974] [CONST: 0.0004] [TID: 0.1191] [TV: 0.1585]\n",
      "[Epoch 0/10] [Batch 563/1875] [D loss: 3.2961] [G loss: 2.3383] [GAN: 2.1975] [CONST: 0.0003] [TID: 0.0982] [TV: 0.1590]\n",
      "[Epoch 0/10] [Batch 564/1875] [D loss: 3.2966] [G loss: 2.3344] [GAN: 2.1975] [CONST: 0.0004] [TID: 0.0903] [TV: 0.1586]\n",
      "[Epoch 0/10] [Batch 565/1875] [D loss: 3.2968] [G loss: 2.3415] [GAN: 2.1978] [CONST: 0.0004] [TID: 0.0955] [TV: 0.1572]\n",
      "[Epoch 0/10] [Batch 566/1875] [D loss: 3.2961] [G loss: 2.3552] [GAN: 2.1974] [CONST: 0.0004] [TID: 0.1110] [TV: 0.1573]\n",
      "[Epoch 0/10] [Batch 567/1875] [D loss: 3.2963] [G loss: 2.3715] [GAN: 2.1975] [CONST: 0.0005] [TID: 0.1172] [TV: 0.1566]\n",
      "[Epoch 0/10] [Batch 568/1875] [D loss: 3.2963] [G loss: 2.3528] [GAN: 2.1976] [CONST: 0.0004] [TID: 0.1099] [TV: 0.1565]\n",
      "[Epoch 0/10] [Batch 569/1875] [D loss: 3.2962] [G loss: 2.3499] [GAN: 2.1976] [CONST: 0.0004] [TID: 0.1031] [TV: 0.1547]\n",
      "[Epoch 0/10] [Batch 570/1875] [D loss: 3.2959] [G loss: 2.3284] [GAN: 2.1975] [CONST: 0.0004] [TID: 0.0873] [TV: 0.1552]\n",
      "[Epoch 0/10] [Batch 571/1875] [D loss: 3.2960] [G loss: 2.3503] [GAN: 2.1974] [CONST: 0.0004] [TID: 0.1055] [TV: 0.1545]\n",
      "[Epoch 0/10] [Batch 572/1875] [D loss: 3.2959] [G loss: 2.3628] [GAN: 2.1974] [CONST: 0.0004] [TID: 0.1160] [TV: 0.1575]\n",
      "[Epoch 0/10] [Batch 573/1875] [D loss: 3.2959] [G loss: 2.3308] [GAN: 2.1974] [CONST: 0.0003] [TID: 0.0917] [TV: 0.1553]\n",
      "[Epoch 0/10] [Batch 574/1875] [D loss: 3.2960] [G loss: 2.3417] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.0982] [TV: 0.1560]\n",
      "[Epoch 0/10] [Batch 575/1875] [D loss: 3.2960] [G loss: 2.3557] [GAN: 2.1974] [CONST: 0.0003] [TID: 0.1173] [TV: 0.1546]\n",
      "[Epoch 0/10] [Batch 576/1875] [D loss: 3.2960] [G loss: 2.3412] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.0976] [TV: 0.1547]\n",
      "[Epoch 0/10] [Batch 577/1875] [D loss: 3.2959] [G loss: 2.3531] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.1069] [TV: 0.1547]\n",
      "[Epoch 0/10] [Batch 578/1875] [D loss: 3.2959] [G loss: 2.3400] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.0984] [TV: 0.1545]\n",
      "[Epoch 0/10] [Batch 579/1875] [D loss: 3.2960] [G loss: 2.3506] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.1070] [TV: 0.1548]\n",
      "[Epoch 0/10] [Batch 580/1875] [D loss: 3.2961] [G loss: 2.3458] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.1001] [TV: 0.1568]\n",
      "[Epoch 0/10] [Batch 581/1875] [D loss: 3.2960] [G loss: 2.3474] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.1009] [TV: 0.1573]\n",
      "[Epoch 0/10] [Batch 582/1875] [D loss: 3.2959] [G loss: 2.3318] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.0865] [TV: 0.1564]\n",
      "[Epoch 0/10] [Batch 583/1875] [D loss: 3.2959] [G loss: 2.3675] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.1251] [TV: 0.1558]\n",
      "[Epoch 0/10] [Batch 584/1875] [D loss: 3.2959] [G loss: 2.3513] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.1026] [TV: 0.1541]\n",
      "[Epoch 0/10] [Batch 585/1875] [D loss: 3.2959] [G loss: 2.3512] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.1107] [TV: 0.1543]\n",
      "[Epoch 0/10] [Batch 586/1875] [D loss: 3.2959] [G loss: 2.3318] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.0826] [TV: 0.1560]\n",
      "[Epoch 0/10] [Batch 587/1875] [D loss: 3.2959] [G loss: 2.3268] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.0893] [TV: 0.1549]\n",
      "[Epoch 0/10] [Batch 588/1875] [D loss: 3.2959] [G loss: 2.3332] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.0879] [TV: 0.1560]\n",
      "[Epoch 0/10] [Batch 589/1875] [D loss: 3.2959] [G loss: 2.3466] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.1003] [TV: 0.1561]\n",
      "[Epoch 0/10] [Batch 590/1875] [D loss: 3.2959] [G loss: 2.3347] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0966] [TV: 0.1574]\n",
      "[Epoch 0/10] [Batch 591/1875] [D loss: 3.2959] [G loss: 2.3314] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.0902] [TV: 0.1567]\n",
      "[Epoch 0/10] [Batch 592/1875] [D loss: 3.2959] [G loss: 2.3792] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.1383] [TV: 0.1562]\n",
      "[Epoch 0/10] [Batch 593/1875] [D loss: 3.2959] [G loss: 2.3395] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.1038] [TV: 0.1570]\n",
      "[Epoch 0/10] [Batch 594/1875] [D loss: 3.2962] [G loss: 2.3584] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.1159] [TV: 0.1563]\n",
      "[Epoch 0/10] [Batch 595/1875] [D loss: 3.2960] [G loss: 2.3320] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.0866] [TV: 0.1547]\n",
      "[Epoch 0/10] [Batch 596/1875] [D loss: 3.2962] [G loss: 2.3284] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.0876] [TV: 0.1563]\n",
      "[Epoch 0/10] [Batch 597/1875] [D loss: 3.2967] [G loss: 2.3622] [GAN: 2.1974] [CONST: 0.0004] [TID: 0.1195] [TV: 0.1549]\n",
      "[Epoch 0/10] [Batch 598/1875] [D loss: 3.2961] [G loss: 2.3578] [GAN: 2.1973] [CONST: 0.0005] [TID: 0.1067] [TV: 0.1556]\n",
      "[Epoch 0/10] [Batch 599/1875] [D loss: 3.2961] [G loss: 2.3682] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.1211] [TV: 0.1563]\n",
      "[Epoch 0/10] [Batch 600/1875] [D loss: 3.2959] [G loss: 2.3289] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0945] [TV: 0.1559]\n",
      "logged samples\n",
      "âœ… Checkpoint saved: checkpoints/dtn_checkpoint_batch_600.pth\n",
      "[Epoch 0/10] [Batch 601/1875] [D loss: 3.2961] [G loss: 2.3429] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.1035] [TV: 0.1548]\n",
      "[Epoch 0/10] [Batch 602/1875] [D loss: 3.2960] [G loss: 2.3091] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.0711] [TV: 0.1555]\n",
      "[Epoch 0/10] [Batch 603/1875] [D loss: 3.2962] [G loss: 2.3413] [GAN: 2.1974] [CONST: 0.0004] [TID: 0.1005] [TV: 0.1541]\n",
      "[Epoch 0/10] [Batch 604/1875] [D loss: 3.2962] [G loss: 2.3412] [GAN: 2.1974] [CONST: 0.0004] [TID: 0.0926] [TV: 0.1561]\n",
      "[Epoch 0/10] [Batch 605/1875] [D loss: 3.2959] [G loss: 2.3445] [GAN: 2.1973] [CONST: 0.0005] [TID: 0.0936] [TV: 0.1566]\n",
      "[Epoch 0/10] [Batch 606/1875] [D loss: 3.2960] [G loss: 2.3392] [GAN: 2.1974] [CONST: 0.0004] [TID: 0.0953] [TV: 0.1564]\n",
      "[Epoch 0/10] [Batch 607/1875] [D loss: 3.2960] [G loss: 2.3594] [GAN: 2.1974] [CONST: 0.0004] [TID: 0.1099] [TV: 0.1555]\n",
      "[Epoch 0/10] [Batch 608/1875] [D loss: 3.2959] [G loss: 2.3670] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.1246] [TV: 0.1567]\n",
      "[Epoch 0/10] [Batch 609/1875] [D loss: 3.2960] [G loss: 2.3472] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.1024] [TV: 0.1570]\n",
      "[Epoch 0/10] [Batch 610/1875] [D loss: 3.2960] [G loss: 2.3502] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.1034] [TV: 0.1570]\n",
      "[Epoch 0/10] [Batch 611/1875] [D loss: 3.2959] [G loss: 2.3412] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.0989] [TV: 0.1566]\n",
      "[Epoch 0/10] [Batch 612/1875] [D loss: 3.2959] [G loss: 2.3446] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.1016] [TV: 0.1575]\n",
      "[Epoch 0/10] [Batch 613/1875] [D loss: 3.2959] [G loss: 2.3439] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.1099] [TV: 0.1554]\n",
      "[Epoch 0/10] [Batch 614/1875] [D loss: 3.2959] [G loss: 2.3618] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.1217] [TV: 0.1555]\n",
      "[Epoch 0/10] [Batch 615/1875] [D loss: 3.2959] [G loss: 2.3354] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.0905] [TV: 0.1564]\n",
      "[Epoch 0/10] [Batch 616/1875] [D loss: 3.2959] [G loss: 2.3479] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.1013] [TV: 0.1561]\n",
      "[Epoch 0/10] [Batch 617/1875] [D loss: 3.2959] [G loss: 2.3427] [GAN: 2.1973] [CONST: 0.0005] [TID: 0.0910] [TV: 0.1576]\n",
      "[Epoch 0/10] [Batch 618/1875] [D loss: 3.2959] [G loss: 2.3383] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.0963] [TV: 0.1575]\n",
      "[Epoch 0/10] [Batch 619/1875] [D loss: 3.2959] [G loss: 2.3340] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.0928] [TV: 0.1589]\n",
      "[Epoch 0/10] [Batch 620/1875] [D loss: 3.2959] [G loss: 2.3416] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1026] [TV: 0.1577]\n",
      "[Epoch 0/10] [Batch 621/1875] [D loss: 3.2959] [G loss: 2.3372] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.0966] [TV: 0.1583]\n",
      "[Epoch 0/10] [Batch 622/1875] [D loss: 3.2959] [G loss: 2.3334] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.0929] [TV: 0.1582]\n",
      "[Epoch 0/10] [Batch 623/1875] [D loss: 3.2959] [G loss: 2.3515] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.1044] [TV: 0.1584]\n",
      "[Epoch 0/10] [Batch 624/1875] [D loss: 3.2959] [G loss: 2.3336] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.0845] [TV: 0.1585]\n",
      "[Epoch 0/10] [Batch 625/1875] [D loss: 3.2959] [G loss: 2.3402] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.0959] [TV: 0.1584]\n",
      "[Epoch 0/10] [Batch 626/1875] [D loss: 3.2959] [G loss: 2.3450] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.1046] [TV: 0.1558]\n",
      "[Epoch 0/10] [Batch 627/1875] [D loss: 3.2959] [G loss: 2.3456] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1074] [TV: 0.1565]\n",
      "[Epoch 0/10] [Batch 628/1875] [D loss: 3.2959] [G loss: 2.3354] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0990] [TV: 0.1574]\n",
      "[Epoch 0/10] [Batch 629/1875] [D loss: 3.2959] [G loss: 2.3290] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0923] [TV: 0.1578]\n",
      "[Epoch 0/10] [Batch 630/1875] [D loss: 3.2959] [G loss: 2.3418] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1035] [TV: 0.1575]\n",
      "[Epoch 0/10] [Batch 631/1875] [D loss: 3.2959] [G loss: 2.3308] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0938] [TV: 0.1559]\n",
      "[Epoch 0/10] [Batch 632/1875] [D loss: 3.2959] [G loss: 2.3443] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1114] [TV: 0.1561]\n",
      "[Epoch 0/10] [Batch 633/1875] [D loss: 3.2959] [G loss: 2.3365] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.0921] [TV: 0.1565]\n",
      "[Epoch 0/10] [Batch 634/1875] [D loss: 3.2959] [G loss: 2.3392] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.0939] [TV: 0.1549]\n",
      "[Epoch 0/10] [Batch 635/1875] [D loss: 3.2959] [G loss: 2.3329] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.0911] [TV: 0.1541]\n",
      "[Epoch 0/10] [Batch 636/1875] [D loss: 3.2959] [G loss: 2.3621] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.1163] [TV: 0.1573]\n",
      "[Epoch 0/10] [Batch 637/1875] [D loss: 3.2959] [G loss: 2.3563] [GAN: 2.1972] [CONST: 0.0005] [TID: 0.1040] [TV: 0.1587]\n",
      "[Epoch 0/10] [Batch 638/1875] [D loss: 3.2959] [G loss: 2.3472] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.1014] [TV: 0.1574]\n",
      "[Epoch 0/10] [Batch 639/1875] [D loss: 3.2959] [G loss: 2.3345] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.0892] [TV: 0.1579]\n",
      "[Epoch 0/10] [Batch 640/1875] [D loss: 3.2960] [G loss: 2.3567] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1182] [TV: 0.1581]\n",
      "[Epoch 0/10] [Batch 641/1875] [D loss: 3.2960] [G loss: 2.3387] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.1003] [TV: 0.1572]\n",
      "[Epoch 0/10] [Batch 642/1875] [D loss: 3.2960] [G loss: 2.3374] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.1038] [TV: 0.1571]\n",
      "[Epoch 0/10] [Batch 643/1875] [D loss: 3.2961] [G loss: 2.3284] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.0941] [TV: 0.1568]\n",
      "[Epoch 0/10] [Batch 644/1875] [D loss: 3.2962] [G loss: 2.3743] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.1371] [TV: 0.1562]\n",
      "[Epoch 0/10] [Batch 645/1875] [D loss: 3.2961] [G loss: 2.3399] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.0945] [TV: 0.1554]\n",
      "[Epoch 0/10] [Batch 646/1875] [D loss: 3.2960] [G loss: 2.3346] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.0961] [TV: 0.1551]\n",
      "[Epoch 0/10] [Batch 647/1875] [D loss: 3.2961] [G loss: 2.3476] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.1094] [TV: 0.1543]\n",
      "[Epoch 0/10] [Batch 648/1875] [D loss: 3.2962] [G loss: 2.3385] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.0969] [TV: 0.1540]\n",
      "[Epoch 0/10] [Batch 649/1875] [D loss: 3.2961] [G loss: 2.3717] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1325] [TV: 0.1541]\n",
      "[Epoch 0/10] [Batch 650/1875] [D loss: 3.2963] [G loss: 2.3617] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.1177] [TV: 0.1572]\n",
      "logged samples\n",
      "[Epoch 0/10] [Batch 651/1875] [D loss: 3.2962] [G loss: 2.3562] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1180] [TV: 0.1559]\n",
      "[Epoch 0/10] [Batch 652/1875] [D loss: 3.2959] [G loss: 2.3262] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.0852] [TV: 0.1535]\n",
      "[Epoch 0/10] [Batch 653/1875] [D loss: 3.2961] [G loss: 2.3581] [GAN: 2.1971] [CONST: 0.0004] [TID: 0.1181] [TV: 0.1530]\n",
      "[Epoch 0/10] [Batch 654/1875] [D loss: 3.2967] [G loss: 2.3626] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.1211] [TV: 0.1537]\n",
      "[Epoch 0/10] [Batch 655/1875] [D loss: 3.2964] [G loss: 2.3305] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.0936] [TV: 0.1540]\n",
      "[Epoch 0/10] [Batch 656/1875] [D loss: 3.2962] [G loss: 2.3374] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1010] [TV: 0.1549]\n",
      "[Epoch 0/10] [Batch 657/1875] [D loss: 3.2961] [G loss: 2.3317] [GAN: 2.1971] [CONST: 0.0004] [TID: 0.0918] [TV: 0.1556]\n",
      "[Epoch 0/10] [Batch 658/1875] [D loss: 3.2961] [G loss: 2.3389] [GAN: 2.1970] [CONST: 0.0004] [TID: 0.0962] [TV: 0.1563]\n",
      "[Epoch 0/10] [Batch 659/1875] [D loss: 3.2959] [G loss: 2.3368] [GAN: 2.1970] [CONST: 0.0004] [TID: 0.0905] [TV: 0.1541]\n",
      "[Epoch 0/10] [Batch 660/1875] [D loss: 3.2960] [G loss: 2.3352] [GAN: 2.1971] [CONST: 0.0003] [TID: 0.1008] [TV: 0.1547]\n",
      "[Epoch 0/10] [Batch 661/1875] [D loss: 3.2961] [G loss: 2.3382] [GAN: 2.1971] [CONST: 0.0003] [TID: 0.1012] [TV: 0.1548]\n",
      "[Epoch 0/10] [Batch 662/1875] [D loss: 3.2960] [G loss: 2.3280] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0904] [TV: 0.1539]\n",
      "[Epoch 0/10] [Batch 663/1875] [D loss: 3.2960] [G loss: 2.3357] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.0946] [TV: 0.1544]\n",
      "[Epoch 0/10] [Batch 664/1875] [D loss: 3.2962] [G loss: 2.3516] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.1062] [TV: 0.1535]\n",
      "[Epoch 0/10] [Batch 665/1875] [D loss: 3.2961] [G loss: 2.3362] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0967] [TV: 0.1536]\n",
      "[Epoch 0/10] [Batch 666/1875] [D loss: 3.2959] [G loss: 2.3235] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.0827] [TV: 0.1542]\n",
      "[Epoch 0/10] [Batch 667/1875] [D loss: 3.2960] [G loss: 2.3338] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.0870] [TV: 0.1529]\n",
      "[Epoch 0/10] [Batch 668/1875] [D loss: 3.2959] [G loss: 2.3408] [GAN: 2.1972] [CONST: 0.0005] [TID: 0.0847] [TV: 0.1543]\n",
      "[Epoch 0/10] [Batch 669/1875] [D loss: 3.2959] [G loss: 2.3423] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.0969] [TV: 0.1562]\n",
      "[Epoch 0/10] [Batch 670/1875] [D loss: 3.2959] [G loss: 2.3284] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0917] [TV: 0.1556]\n",
      "[Epoch 0/10] [Batch 671/1875] [D loss: 3.2960] [G loss: 2.3597] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1250] [TV: 0.1560]\n",
      "[Epoch 0/10] [Batch 672/1875] [D loss: 3.2960] [G loss: 2.3217] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.0836] [TV: 0.1552]\n",
      "[Epoch 0/10] [Batch 673/1875] [D loss: 3.2960] [G loss: 2.3416] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.1035] [TV: 0.1561]\n",
      "[Epoch 0/10] [Batch 674/1875] [D loss: 3.2962] [G loss: 2.3477] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.1104] [TV: 0.1548]\n",
      "[Epoch 0/10] [Batch 675/1875] [D loss: 3.2963] [G loss: 2.3364] [GAN: 2.1974] [CONST: 0.0004] [TID: 0.0950] [TV: 0.1553]\n",
      "[Epoch 0/10] [Batch 676/1875] [D loss: 3.2960] [G loss: 2.3329] [GAN: 2.1971] [CONST: 0.0004] [TID: 0.0905] [TV: 0.1551]\n",
      "[Epoch 0/10] [Batch 677/1875] [D loss: 3.2960] [G loss: 2.3407] [GAN: 2.1971] [CONST: 0.0003] [TID: 0.1056] [TV: 0.1557]\n",
      "[Epoch 0/10] [Batch 678/1875] [D loss: 3.2961] [G loss: 2.3506] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.1100] [TV: 0.1536]\n",
      "[Epoch 0/10] [Batch 679/1875] [D loss: 3.2959] [G loss: 2.3330] [GAN: 2.1971] [CONST: 0.0004] [TID: 0.0922] [TV: 0.1549]\n",
      "[Epoch 0/10] [Batch 680/1875] [D loss: 3.2960] [G loss: 2.3430] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.0994] [TV: 0.1565]\n",
      "[Epoch 0/10] [Batch 681/1875] [D loss: 3.2960] [G loss: 2.3391] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.0977] [TV: 0.1552]\n",
      "[Epoch 0/10] [Batch 682/1875] [D loss: 3.2959] [G loss: 2.3331] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0989] [TV: 0.1548]\n",
      "[Epoch 0/10] [Batch 683/1875] [D loss: 3.2959] [G loss: 2.3439] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1077] [TV: 0.1539]\n",
      "[Epoch 0/10] [Batch 684/1875] [D loss: 3.2959] [G loss: 2.3481] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1098] [TV: 0.1531]\n",
      "[Epoch 0/10] [Batch 685/1875] [D loss: 3.2959] [G loss: 2.3359] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0991] [TV: 0.1531]\n",
      "[Epoch 0/10] [Batch 686/1875] [D loss: 3.2959] [G loss: 2.3492] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.1082] [TV: 0.1518]\n",
      "[Epoch 0/10] [Batch 687/1875] [D loss: 3.2959] [G loss: 2.3341] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0998] [TV: 0.1536]\n",
      "[Epoch 0/10] [Batch 688/1875] [D loss: 3.2959] [G loss: 2.3364] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0998] [TV: 0.1543]\n",
      "[Epoch 0/10] [Batch 689/1875] [D loss: 3.2960] [G loss: 2.3435] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1119] [TV: 0.1527]\n",
      "[Epoch 0/10] [Batch 690/1875] [D loss: 3.2959] [G loss: 2.3385] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1023] [TV: 0.1539]\n",
      "[Epoch 0/10] [Batch 691/1875] [D loss: 3.2959] [G loss: 2.3308] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0913] [TV: 0.1540]\n",
      "[Epoch 0/10] [Batch 692/1875] [D loss: 3.2959] [G loss: 2.3212] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0855] [TV: 0.1533]\n",
      "[Epoch 0/10] [Batch 693/1875] [D loss: 3.2959] [G loss: 2.3628] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1267] [TV: 0.1552]\n",
      "[Epoch 0/10] [Batch 694/1875] [D loss: 3.2959] [G loss: 2.3411] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1024] [TV: 0.1535]\n",
      "[Epoch 0/10] [Batch 695/1875] [D loss: 3.2959] [G loss: 2.3526] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1164] [TV: 0.1523]\n",
      "[Epoch 0/10] [Batch 696/1875] [D loss: 3.2959] [G loss: 2.3383] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.0985] [TV: 0.1512]\n",
      "[Epoch 0/10] [Batch 697/1875] [D loss: 3.2959] [G loss: 2.3323] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.0890] [TV: 0.1518]\n",
      "[Epoch 0/10] [Batch 698/1875] [D loss: 3.2959] [G loss: 2.3373] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0998] [TV: 0.1509]\n",
      "[Epoch 0/10] [Batch 699/1875] [D loss: 3.2959] [G loss: 2.3324] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0996] [TV: 0.1511]\n",
      "[Epoch 0/10] [Batch 700/1875] [D loss: 3.2959] [G loss: 2.3647] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1323] [TV: 0.1514]\n",
      "logged samples\n",
      "âœ… Checkpoint saved: checkpoints/dtn_checkpoint_batch_700.pth\n",
      "[Epoch 0/10] [Batch 701/1875] [D loss: 3.2959] [G loss: 2.3306] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0982] [TV: 0.1539]\n",
      "[Epoch 0/10] [Batch 702/1875] [D loss: 3.2960] [G loss: 2.3538] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.1128] [TV: 0.1530]\n",
      "[Epoch 0/10] [Batch 703/1875] [D loss: 3.2959] [G loss: 2.3362] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.0991] [TV: 0.1516]\n",
      "[Epoch 0/10] [Batch 704/1875] [D loss: 3.2959] [G loss: 2.3531] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1136] [TV: 0.1535]\n",
      "[Epoch 0/10] [Batch 705/1875] [D loss: 3.2959] [G loss: 2.3301] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.0942] [TV: 0.1520]\n",
      "[Epoch 0/10] [Batch 706/1875] [D loss: 3.2959] [G loss: 2.3367] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0979] [TV: 0.1508]\n",
      "[Epoch 0/10] [Batch 707/1875] [D loss: 3.2959] [G loss: 2.3163] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0820] [TV: 0.1522]\n",
      "[Epoch 0/10] [Batch 708/1875] [D loss: 3.2959] [G loss: 2.3426] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1094] [TV: 0.1524]\n",
      "[Epoch 0/10] [Batch 709/1875] [D loss: 3.2959] [G loss: 2.3326] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0964] [TV: 0.1530]\n",
      "[Epoch 0/10] [Batch 710/1875] [D loss: 3.2959] [G loss: 2.3216] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0895] [TV: 0.1525]\n",
      "[Epoch 0/10] [Batch 711/1875] [D loss: 3.2959] [G loss: 2.3254] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0880] [TV: 0.1525]\n",
      "[Epoch 0/10] [Batch 712/1875] [D loss: 3.2959] [G loss: 2.3259] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.0858] [TV: 0.1538]\n",
      "[Epoch 0/10] [Batch 713/1875] [D loss: 3.2959] [G loss: 2.3322] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0941] [TV: 0.1534]\n",
      "[Epoch 0/10] [Batch 714/1875] [D loss: 3.2959] [G loss: 2.3262] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0894] [TV: 0.1545]\n",
      "[Epoch 0/10] [Batch 715/1875] [D loss: 3.2959] [G loss: 2.3286] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0969] [TV: 0.1543]\n",
      "[Epoch 0/10] [Batch 716/1875] [D loss: 3.2959] [G loss: 2.3234] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0896] [TV: 0.1542]\n",
      "[Epoch 0/10] [Batch 717/1875] [D loss: 3.2959] [G loss: 2.3577] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1204] [TV: 0.1536]\n",
      "[Epoch 0/10] [Batch 718/1875] [D loss: 3.2959] [G loss: 2.3389] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1036] [TV: 0.1541]\n",
      "[Epoch 0/10] [Batch 719/1875] [D loss: 3.2959] [G loss: 2.3702] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1375] [TV: 0.1538]\n",
      "[Epoch 0/10] [Batch 720/1875] [D loss: 3.2959] [G loss: 2.3377] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1023] [TV: 0.1523]\n",
      "[Epoch 0/10] [Batch 721/1875] [D loss: 3.2959] [G loss: 2.3205] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0884] [TV: 0.1536]\n",
      "[Epoch 0/10] [Batch 722/1875] [D loss: 3.2959] [G loss: 2.3357] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0993] [TV: 0.1531]\n",
      "[Epoch 0/10] [Batch 723/1875] [D loss: 3.2959] [G loss: 2.3259] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0918] [TV: 0.1529]\n",
      "[Epoch 0/10] [Batch 724/1875] [D loss: 3.2959] [G loss: 2.3353] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0983] [TV: 0.1530]\n",
      "[Epoch 0/10] [Batch 725/1875] [D loss: 3.2959] [G loss: 2.3273] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.0852] [TV: 0.1528]\n",
      "[Epoch 0/10] [Batch 726/1875] [D loss: 3.2959] [G loss: 2.3447] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1055] [TV: 0.1521]\n",
      "[Epoch 0/10] [Batch 727/1875] [D loss: 3.2959] [G loss: 2.3392] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1041] [TV: 0.1529]\n",
      "[Epoch 0/10] [Batch 728/1875] [D loss: 3.2959] [G loss: 2.3484] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1103] [TV: 0.1525]\n",
      "[Epoch 0/10] [Batch 729/1875] [D loss: 3.2959] [G loss: 2.3491] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.1090] [TV: 0.1546]\n",
      "[Epoch 0/10] [Batch 730/1875] [D loss: 3.2959] [G loss: 2.3462] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1134] [TV: 0.1547]\n",
      "[Epoch 0/10] [Batch 731/1875] [D loss: 3.2959] [G loss: 2.3650] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1294] [TV: 0.1540]\n",
      "[Epoch 0/10] [Batch 732/1875] [D loss: 3.2959] [G loss: 2.3324] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0931] [TV: 0.1536]\n",
      "[Epoch 0/10] [Batch 733/1875] [D loss: 3.2961] [G loss: 2.3544] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.1166] [TV: 0.1534]\n",
      "[Epoch 0/10] [Batch 734/1875] [D loss: 3.2962] [G loss: 2.3216] [GAN: 2.1974] [CONST: 0.0003] [TID: 0.0862] [TV: 0.1539]\n",
      "[Epoch 0/10] [Batch 735/1875] [D loss: 3.2960] [G loss: 2.3381] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.1016] [TV: 0.1535]\n",
      "[Epoch 0/10] [Batch 736/1875] [D loss: 3.2960] [G loss: 2.3461] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.1102] [TV: 0.1538]\n",
      "[Epoch 0/10] [Batch 737/1875] [D loss: 3.2959] [G loss: 2.3313] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0958] [TV: 0.1550]\n",
      "[Epoch 0/10] [Batch 738/1875] [D loss: 3.2959] [G loss: 2.3308] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.0941] [TV: 0.1546]\n",
      "[Epoch 0/10] [Batch 739/1875] [D loss: 3.2959] [G loss: 2.3388] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.0996] [TV: 0.1528]\n",
      "[Epoch 0/10] [Batch 740/1875] [D loss: 3.2959] [G loss: 2.3589] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.1249] [TV: 0.1549]\n",
      "[Epoch 0/10] [Batch 741/1875] [D loss: 3.2959] [G loss: 2.3414] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.1008] [TV: 0.1533]\n",
      "[Epoch 0/10] [Batch 742/1875] [D loss: 3.2959] [G loss: 2.3272] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0923] [TV: 0.1538]\n",
      "[Epoch 0/10] [Batch 743/1875] [D loss: 3.2959] [G loss: 2.3326] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0929] [TV: 0.1531]\n",
      "[Epoch 0/10] [Batch 744/1875] [D loss: 3.2959] [G loss: 2.3202] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0887] [TV: 0.1520]\n",
      "[Epoch 0/10] [Batch 745/1875] [D loss: 3.2959] [G loss: 2.3458] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1142] [TV: 0.1513]\n",
      "[Epoch 0/10] [Batch 746/1875] [D loss: 3.2959] [G loss: 2.3269] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0949] [TV: 0.1504]\n",
      "[Epoch 0/10] [Batch 747/1875] [D loss: 3.2959] [G loss: 2.3273] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0913] [TV: 0.1520]\n",
      "[Epoch 0/10] [Batch 748/1875] [D loss: 3.2959] [G loss: 2.3318] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0931] [TV: 0.1506]\n",
      "[Epoch 0/10] [Batch 749/1875] [D loss: 3.2959] [G loss: 2.3275] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.0876] [TV: 0.1525]\n",
      "[Epoch 0/10] [Batch 750/1875] [D loss: 3.2959] [G loss: 2.3326] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1001] [TV: 0.1533]\n",
      "logged samples\n",
      "[Epoch 0/10] [Batch 751/1875] [D loss: 3.2960] [G loss: 2.3404] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1049] [TV: 0.1530]\n",
      "[Epoch 0/10] [Batch 752/1875] [D loss: 3.2960] [G loss: 2.3545] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1205] [TV: 0.1547]\n",
      "[Epoch 0/10] [Batch 753/1875] [D loss: 3.2961] [G loss: 2.3379] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.1014] [TV: 0.1528]\n",
      "[Epoch 0/10] [Batch 754/1875] [D loss: 3.2959] [G loss: 2.3267] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0936] [TV: 0.1545]\n",
      "[Epoch 0/10] [Batch 755/1875] [D loss: 3.2961] [G loss: 2.3284] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.0934] [TV: 0.1538]\n",
      "[Epoch 0/10] [Batch 756/1875] [D loss: 3.2960] [G loss: 2.3403] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.1062] [TV: 0.1543]\n",
      "[Epoch 0/10] [Batch 757/1875] [D loss: 3.2959] [G loss: 2.3184] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.0859] [TV: 0.1536]\n",
      "[Epoch 0/10] [Batch 758/1875] [D loss: 3.2959] [G loss: 2.3150] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0816] [TV: 0.1549]\n",
      "[Epoch 0/10] [Batch 759/1875] [D loss: 3.2960] [G loss: 2.3463] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1111] [TV: 0.1532]\n",
      "[Epoch 0/10] [Batch 760/1875] [D loss: 3.2962] [G loss: 2.3319] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.0981] [TV: 0.1519]\n",
      "[Epoch 0/10] [Batch 761/1875] [D loss: 3.2960] [G loss: 2.3503] [GAN: 2.1973] [CONST: 0.0002] [TID: 0.1208] [TV: 0.1540]\n",
      "[Epoch 0/10] [Batch 762/1875] [D loss: 3.2960] [G loss: 2.3285] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0931] [TV: 0.1538]\n",
      "[Epoch 0/10] [Batch 763/1875] [D loss: 3.2961] [G loss: 2.3327] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.0994] [TV: 0.1538]\n",
      "[Epoch 0/10] [Batch 764/1875] [D loss: 3.2965] [G loss: 2.3543] [GAN: 2.1975] [CONST: 0.0004] [TID: 0.1132] [TV: 0.1531]\n",
      "[Epoch 0/10] [Batch 765/1875] [D loss: 3.2959] [G loss: 2.3591] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1235] [TV: 0.1527]\n",
      "[Epoch 0/10] [Batch 766/1875] [D loss: 3.2960] [G loss: 2.3258] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0893] [TV: 0.1535]\n",
      "[Epoch 0/10] [Batch 767/1875] [D loss: 3.2962] [G loss: 2.3251] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.0905] [TV: 0.1540]\n",
      "[Epoch 0/10] [Batch 768/1875] [D loss: 3.2959] [G loss: 2.3431] [GAN: 2.1971] [CONST: 0.0003] [TID: 0.1053] [TV: 0.1520]\n",
      "[Epoch 0/10] [Batch 769/1875] [D loss: 3.2959] [G loss: 2.3268] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0884] [TV: 0.1540]\n",
      "[Epoch 0/10] [Batch 770/1875] [D loss: 3.2960] [G loss: 2.3517] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1127] [TV: 0.1526]\n",
      "[Epoch 0/10] [Batch 771/1875] [D loss: 3.2960] [G loss: 2.3232] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0844] [TV: 0.1529]\n",
      "[Epoch 0/10] [Batch 772/1875] [D loss: 3.2959] [G loss: 2.3244] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0867] [TV: 0.1538]\n",
      "[Epoch 0/10] [Batch 773/1875] [D loss: 3.2960] [G loss: 2.3454] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1079] [TV: 0.1525]\n",
      "[Epoch 0/10] [Batch 774/1875] [D loss: 3.2960] [G loss: 2.3387] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.0976] [TV: 0.1561]\n",
      "[Epoch 0/10] [Batch 775/1875] [D loss: 3.2959] [G loss: 2.3117] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0773] [TV: 0.1545]\n",
      "[Epoch 0/10] [Batch 776/1875] [D loss: 3.2959] [G loss: 2.3244] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0861] [TV: 0.1534]\n",
      "[Epoch 0/10] [Batch 777/1875] [D loss: 3.2959] [G loss: 2.3332] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.0924] [TV: 0.1536]\n",
      "[Epoch 0/10] [Batch 778/1875] [D loss: 3.2959] [G loss: 2.3311] [GAN: 2.1973] [CONST: 0.0004] [TID: 0.0863] [TV: 0.1550]\n",
      "[Epoch 0/10] [Batch 779/1875] [D loss: 3.2959] [G loss: 2.3379] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.0973] [TV: 0.1536]\n",
      "[Epoch 0/10] [Batch 780/1875] [D loss: 3.2959] [G loss: 2.3209] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0876] [TV: 0.1518]\n",
      "[Epoch 0/10] [Batch 781/1875] [D loss: 3.2959] [G loss: 2.3317] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0919] [TV: 0.1519]\n",
      "[Epoch 0/10] [Batch 782/1875] [D loss: 3.2959] [G loss: 2.3296] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0938] [TV: 0.1529]\n",
      "[Epoch 0/10] [Batch 783/1875] [D loss: 3.2959] [G loss: 2.3162] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0882] [TV: 0.1511]\n",
      "[Epoch 0/10] [Batch 784/1875] [D loss: 3.2959] [G loss: 2.3353] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1004] [TV: 0.1523]\n",
      "[Epoch 0/10] [Batch 785/1875] [D loss: 3.2959] [G loss: 2.3137] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0782] [TV: 0.1533]\n",
      "[Epoch 0/10] [Batch 786/1875] [D loss: 3.2959] [G loss: 2.3573] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.1123] [TV: 0.1522]\n",
      "[Epoch 0/10] [Batch 787/1875] [D loss: 3.2959] [G loss: 2.3353] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0976] [TV: 0.1525]\n",
      "[Epoch 0/10] [Batch 788/1875] [D loss: 3.2959] [G loss: 2.3196] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0853] [TV: 0.1535]\n",
      "[Epoch 0/10] [Batch 789/1875] [D loss: 3.2959] [G loss: 2.3189] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0861] [TV: 0.1524]\n",
      "[Epoch 0/10] [Batch 790/1875] [D loss: 3.2959] [G loss: 2.3302] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0922] [TV: 0.1518]\n",
      "[Epoch 0/10] [Batch 791/1875] [D loss: 3.2959] [G loss: 2.3420] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1028] [TV: 0.1530]\n",
      "[Epoch 0/10] [Batch 792/1875] [D loss: 3.2959] [G loss: 2.3291] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0904] [TV: 0.1523]\n",
      "[Epoch 0/10] [Batch 793/1875] [D loss: 3.2959] [G loss: 2.3454] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.1038] [TV: 0.1518]\n",
      "[Epoch 0/10] [Batch 794/1875] [D loss: 3.2959] [G loss: 2.3503] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1155] [TV: 0.1537]\n",
      "[Epoch 0/10] [Batch 795/1875] [D loss: 3.2959] [G loss: 2.3329] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1010] [TV: 0.1533]\n",
      "[Epoch 0/10] [Batch 796/1875] [D loss: 3.2959] [G loss: 2.3505] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1160] [TV: 0.1523]\n",
      "[Epoch 0/10] [Batch 797/1875] [D loss: 3.2959] [G loss: 2.3298] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.0944] [TV: 0.1527]\n",
      "[Epoch 0/10] [Batch 798/1875] [D loss: 3.2959] [G loss: 2.3149] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0811] [TV: 0.1536]\n",
      "[Epoch 0/10] [Batch 799/1875] [D loss: 3.2959] [G loss: 2.3332] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1006] [TV: 0.1524]\n",
      "[Epoch 0/10] [Batch 800/1875] [D loss: 3.2959] [G loss: 2.3443] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1105] [TV: 0.1523]\n",
      "logged samples\n",
      "âœ… Checkpoint saved: checkpoints/dtn_checkpoint_batch_800.pth\n",
      "[Epoch 0/10] [Batch 801/1875] [D loss: 3.2959] [G loss: 2.3272] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0927] [TV: 0.1519]\n",
      "[Epoch 0/10] [Batch 802/1875] [D loss: 3.2959] [G loss: 2.3215] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0916] [TV: 0.1515]\n",
      "[Epoch 0/10] [Batch 803/1875] [D loss: 3.2959] [G loss: 2.3507] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1164] [TV: 0.1527]\n",
      "[Epoch 0/10] [Batch 804/1875] [D loss: 3.2959] [G loss: 2.3206] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0873] [TV: 0.1515]\n",
      "[Epoch 0/10] [Batch 805/1875] [D loss: 3.2959] [G loss: 2.3241] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0929] [TV: 0.1507]\n",
      "[Epoch 0/10] [Batch 806/1875] [D loss: 3.2959] [G loss: 2.4197] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1898] [TV: 0.1513]\n",
      "[Epoch 0/10] [Batch 807/1875] [D loss: 3.2959] [G loss: 2.3312] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.0938] [TV: 0.1525]\n",
      "[Epoch 0/10] [Batch 808/1875] [D loss: 3.2959] [G loss: 2.3297] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.0956] [TV: 0.1526]\n",
      "[Epoch 0/10] [Batch 809/1875] [D loss: 3.2959] [G loss: 2.3278] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.0938] [TV: 0.1528]\n",
      "[Epoch 0/10] [Batch 810/1875] [D loss: 3.2959] [G loss: 2.3114] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.0775] [TV: 0.1528]\n",
      "[Epoch 0/10] [Batch 811/1875] [D loss: 3.2959] [G loss: 2.3200] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.0826] [TV: 0.1543]\n",
      "[Epoch 0/10] [Batch 812/1875] [D loss: 3.2959] [G loss: 2.3236] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0856] [TV: 0.1521]\n",
      "[Epoch 0/10] [Batch 813/1875] [D loss: 3.2959] [G loss: 2.3345] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.0965] [TV: 0.1538]\n",
      "[Epoch 0/10] [Batch 814/1875] [D loss: 3.2959] [G loss: 2.3113] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0777] [TV: 0.1530]\n",
      "[Epoch 0/10] [Batch 815/1875] [D loss: 3.2959] [G loss: 2.3131] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0787] [TV: 0.1519]\n",
      "[Epoch 0/10] [Batch 816/1875] [D loss: 3.2959] [G loss: 2.3431] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.0996] [TV: 0.1534]\n",
      "[Epoch 0/10] [Batch 817/1875] [D loss: 3.2959] [G loss: 2.3399] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1069] [TV: 0.1523]\n",
      "[Epoch 0/10] [Batch 818/1875] [D loss: 3.2959] [G loss: 2.3306] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.0900] [TV: 0.1527]\n",
      "[Epoch 0/10] [Batch 819/1875] [D loss: 3.2959] [G loss: 2.3523] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1132] [TV: 0.1527]\n",
      "[Epoch 0/10] [Batch 820/1875] [D loss: 3.2959] [G loss: 2.3267] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0967] [TV: 0.1516]\n",
      "[Epoch 0/10] [Batch 821/1875] [D loss: 3.2959] [G loss: 2.3236] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0923] [TV: 0.1534]\n",
      "[Epoch 0/10] [Batch 822/1875] [D loss: 3.2959] [G loss: 2.3301] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0957] [TV: 0.1520]\n",
      "[Epoch 0/10] [Batch 823/1875] [D loss: 3.2959] [G loss: 2.3120] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0833] [TV: 0.1534]\n",
      "[Epoch 0/10] [Batch 824/1875] [D loss: 3.2959] [G loss: 2.3178] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0844] [TV: 0.1544]\n",
      "[Epoch 0/10] [Batch 825/1875] [D loss: 3.2959] [G loss: 2.3171] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0840] [TV: 0.1537]\n",
      "[Epoch 0/10] [Batch 826/1875] [D loss: 3.2959] [G loss: 2.3346] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0987] [TV: 0.1533]\n",
      "[Epoch 0/10] [Batch 827/1875] [D loss: 3.2959] [G loss: 2.3250] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0929] [TV: 0.1528]\n",
      "[Epoch 0/10] [Batch 828/1875] [D loss: 3.2959] [G loss: 2.3145] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0758] [TV: 0.1527]\n",
      "[Epoch 0/10] [Batch 829/1875] [D loss: 3.2959] [G loss: 2.3197] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0871] [TV: 0.1521]\n",
      "[Epoch 0/10] [Batch 830/1875] [D loss: 3.2959] [G loss: 2.3517] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.1223] [TV: 0.1519]\n",
      "[Epoch 0/10] [Batch 831/1875] [D loss: 3.2959] [G loss: 2.3225] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0921] [TV: 0.1517]\n",
      "[Epoch 0/10] [Batch 832/1875] [D loss: 3.2959] [G loss: 2.3207] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0923] [TV: 0.1531]\n",
      "[Epoch 0/10] [Batch 833/1875] [D loss: 3.2959] [G loss: 2.3399] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1099] [TV: 0.1519]\n",
      "[Epoch 0/10] [Batch 834/1875] [D loss: 3.2959] [G loss: 2.3420] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.1125] [TV: 0.1515]\n",
      "[Epoch 0/10] [Batch 835/1875] [D loss: 3.2959] [G loss: 2.3349] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.1069] [TV: 0.1530]\n",
      "[Epoch 0/10] [Batch 836/1875] [D loss: 3.2958] [G loss: 2.3137] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0836] [TV: 0.1520]\n",
      "[Epoch 0/10] [Batch 837/1875] [D loss: 3.2959] [G loss: 2.3114] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0777] [TV: 0.1514]\n",
      "[Epoch 0/10] [Batch 838/1875] [D loss: 3.2959] [G loss: 2.3112] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0781] [TV: 0.1516]\n",
      "[Epoch 0/10] [Batch 839/1875] [D loss: 3.2960] [G loss: 2.3484] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1146] [TV: 0.1512]\n",
      "[Epoch 0/10] [Batch 840/1875] [D loss: 3.2959] [G loss: 2.3437] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1114] [TV: 0.1518]\n",
      "[Epoch 0/10] [Batch 841/1875] [D loss: 3.2959] [G loss: 2.3313] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0965] [TV: 0.1517]\n",
      "[Epoch 0/10] [Batch 842/1875] [D loss: 3.2959] [G loss: 2.3209] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0866] [TV: 0.1511]\n",
      "[Epoch 0/10] [Batch 843/1875] [D loss: 3.2959] [G loss: 2.3474] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1114] [TV: 0.1528]\n",
      "[Epoch 0/10] [Batch 844/1875] [D loss: 3.2959] [G loss: 2.3272] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0934] [TV: 0.1532]\n",
      "[Epoch 0/10] [Batch 845/1875] [D loss: 3.2959] [G loss: 2.3177] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0883] [TV: 0.1530]\n",
      "[Epoch 0/10] [Batch 846/1875] [D loss: 3.2959] [G loss: 2.3302] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0999] [TV: 0.1535]\n",
      "[Epoch 0/10] [Batch 847/1875] [D loss: 3.2959] [G loss: 2.3276] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0961] [TV: 0.1519]\n",
      "[Epoch 0/10] [Batch 848/1875] [D loss: 3.2959] [G loss: 2.3155] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0829] [TV: 0.1531]\n",
      "[Epoch 0/10] [Batch 849/1875] [D loss: 3.2959] [G loss: 2.3075] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0751] [TV: 0.1523]\n",
      "[Epoch 0/10] [Batch 850/1875] [D loss: 3.2959] [G loss: 2.3502] [GAN: 2.1972] [CONST: 0.0004] [TID: 0.1045] [TV: 0.1523]\n",
      "logged samples\n",
      "[Epoch 0/10] [Batch 851/1875] [D loss: 3.2959] [G loss: 2.3405] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1022] [TV: 0.1523]\n",
      "[Epoch 0/10] [Batch 852/1875] [D loss: 3.2959] [G loss: 2.3107] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0829] [TV: 0.1525]\n",
      "[Epoch 0/10] [Batch 853/1875] [D loss: 3.2960] [G loss: 2.3421] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1048] [TV: 0.1515]\n",
      "[Epoch 0/10] [Batch 854/1875] [D loss: 3.2959] [G loss: 2.3552] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1218] [TV: 0.1515]\n",
      "[Epoch 0/10] [Batch 855/1875] [D loss: 3.2959] [G loss: 2.3361] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.1076] [TV: 0.1532]\n",
      "[Epoch 0/10] [Batch 856/1875] [D loss: 3.2959] [G loss: 2.3283] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0954] [TV: 0.1517]\n",
      "[Epoch 0/10] [Batch 857/1875] [D loss: 3.2959] [G loss: 2.3210] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0908] [TV: 0.1528]\n",
      "[Epoch 0/10] [Batch 858/1875] [D loss: 3.2959] [G loss: 2.3238] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0896] [TV: 0.1525]\n",
      "[Epoch 0/10] [Batch 859/1875] [D loss: 3.2959] [G loss: 2.3262] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0931] [TV: 0.1523]\n",
      "[Epoch 0/10] [Batch 860/1875] [D loss: 3.2959] [G loss: 2.3020] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0729] [TV: 0.1528]\n",
      "[Epoch 0/10] [Batch 861/1875] [D loss: 3.2959] [G loss: 2.3254] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0962] [TV: 0.1526]\n",
      "[Epoch 0/10] [Batch 862/1875] [D loss: 3.2959] [G loss: 2.3140] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0838] [TV: 0.1531]\n",
      "[Epoch 0/10] [Batch 863/1875] [D loss: 3.2959] [G loss: 2.3218] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0920] [TV: 0.1534]\n",
      "[Epoch 0/10] [Batch 864/1875] [D loss: 3.2959] [G loss: 2.3024] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0725] [TV: 0.1535]\n",
      "[Epoch 0/10] [Batch 865/1875] [D loss: 3.2959] [G loss: 2.3135] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0843] [TV: 0.1526]\n",
      "[Epoch 0/10] [Batch 866/1875] [D loss: 3.2959] [G loss: 2.2970] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0673] [TV: 0.1509]\n",
      "[Epoch 0/10] [Batch 867/1875] [D loss: 3.2959] [G loss: 2.3190] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0895] [TV: 0.1512]\n",
      "[Epoch 0/10] [Batch 868/1875] [D loss: 3.2959] [G loss: 2.3290] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0962] [TV: 0.1513]\n",
      "[Epoch 0/10] [Batch 869/1875] [D loss: 3.2959] [G loss: 2.3269] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0938] [TV: 0.1508]\n",
      "[Epoch 0/10] [Batch 870/1875] [D loss: 3.2959] [G loss: 2.3379] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1061] [TV: 0.1523]\n",
      "[Epoch 0/10] [Batch 871/1875] [D loss: 3.2959] [G loss: 2.3189] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0859] [TV: 0.1519]\n",
      "[Epoch 0/10] [Batch 872/1875] [D loss: 3.2960] [G loss: 2.3471] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1160] [TV: 0.1510]\n",
      "[Epoch 0/10] [Batch 873/1875] [D loss: 3.2961] [G loss: 2.3325] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.0978] [TV: 0.1507]\n",
      "[Epoch 0/10] [Batch 874/1875] [D loss: 3.2959] [G loss: 2.3153] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.0808] [TV: 0.1500]\n",
      "[Epoch 0/10] [Batch 875/1875] [D loss: 3.2959] [G loss: 2.3342] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1034] [TV: 0.1508]\n",
      "[Epoch 0/10] [Batch 876/1875] [D loss: 3.2959] [G loss: 2.3379] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.1087] [TV: 0.1508]\n",
      "[Epoch 0/10] [Batch 877/1875] [D loss: 3.2959] [G loss: 2.3191] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0872] [TV: 0.1498]\n",
      "[Epoch 0/10] [Batch 878/1875] [D loss: 3.2960] [G loss: 2.3533] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1214] [TV: 0.1508]\n",
      "[Epoch 0/10] [Batch 879/1875] [D loss: 3.2960] [G loss: 2.3504] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1186] [TV: 0.1507]\n",
      "[Epoch 0/10] [Batch 880/1875] [D loss: 3.2960] [G loss: 2.3136] [GAN: 2.1973] [CONST: 0.0002] [TID: 0.0841] [TV: 0.1510]\n",
      "[Epoch 0/10] [Batch 881/1875] [D loss: 3.2961] [G loss: 2.3634] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.1354] [TV: 0.1508]\n",
      "[Epoch 0/10] [Batch 882/1875] [D loss: 3.2960] [G loss: 2.3085] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0814] [TV: 0.1509]\n",
      "[Epoch 0/10] [Batch 883/1875] [D loss: 3.2960] [G loss: 2.3324] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1002] [TV: 0.1518]\n",
      "[Epoch 0/10] [Batch 884/1875] [D loss: 3.2959] [G loss: 2.3294] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0944] [TV: 0.1510]\n",
      "[Epoch 0/10] [Batch 885/1875] [D loss: 3.2959] [G loss: 2.3191] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0921] [TV: 0.1514]\n",
      "[Epoch 0/10] [Batch 886/1875] [D loss: 3.2959] [G loss: 2.3353] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1044] [TV: 0.1522]\n",
      "[Epoch 0/10] [Batch 887/1875] [D loss: 3.2960] [G loss: 2.3417] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.1126] [TV: 0.1514]\n",
      "[Epoch 0/10] [Batch 888/1875] [D loss: 3.2959] [G loss: 2.3520] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.1250] [TV: 0.1506]\n",
      "[Epoch 0/10] [Batch 889/1875] [D loss: 3.2959] [G loss: 2.3272] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0978] [TV: 0.1510]\n",
      "[Epoch 0/10] [Batch 890/1875] [D loss: 3.2959] [G loss: 2.3490] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1167] [TV: 0.1523]\n",
      "[Epoch 0/10] [Batch 891/1875] [D loss: 3.2959] [G loss: 2.3712] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1374] [TV: 0.1520]\n",
      "[Epoch 0/10] [Batch 892/1875] [D loss: 3.2959] [G loss: 2.3475] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1161] [TV: 0.1510]\n",
      "[Epoch 0/10] [Batch 893/1875] [D loss: 3.2960] [G loss: 2.3270] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0924] [TV: 0.1518]\n",
      "[Epoch 0/10] [Batch 894/1875] [D loss: 3.2960] [G loss: 2.3402] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1052] [TV: 0.1514]\n",
      "[Epoch 0/10] [Batch 895/1875] [D loss: 3.2959] [G loss: 2.3324] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0959] [TV: 0.1526]\n",
      "[Epoch 0/10] [Batch 896/1875] [D loss: 3.2959] [G loss: 2.3278] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0973] [TV: 0.1537]\n",
      "[Epoch 0/10] [Batch 897/1875] [D loss: 3.2960] [G loss: 2.3209] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0900] [TV: 0.1536]\n",
      "[Epoch 0/10] [Batch 898/1875] [D loss: 3.2959] [G loss: 2.3266] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0893] [TV: 0.1530]\n",
      "[Epoch 0/10] [Batch 899/1875] [D loss: 3.2959] [G loss: 2.3330] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.1042] [TV: 0.1550]\n",
      "[Epoch 0/10] [Batch 900/1875] [D loss: 3.2959] [G loss: 2.3312] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0993] [TV: 0.1541]\n",
      "logged samples\n",
      "âœ… Checkpoint saved: checkpoints/dtn_checkpoint_batch_900.pth\n",
      "[Epoch 0/10] [Batch 901/1875] [D loss: 3.2960] [G loss: 2.3310] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0943] [TV: 0.1540]\n",
      "[Epoch 0/10] [Batch 902/1875] [D loss: 3.2960] [G loss: 2.3383] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1048] [TV: 0.1542]\n",
      "[Epoch 0/10] [Batch 903/1875] [D loss: 3.2960] [G loss: 2.3165] [GAN: 2.1973] [CONST: 0.0002] [TID: 0.0884] [TV: 0.1524]\n",
      "[Epoch 0/10] [Batch 904/1875] [D loss: 3.2959] [G loss: 2.3090] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0804] [TV: 0.1513]\n",
      "[Epoch 0/10] [Batch 905/1875] [D loss: 3.2960] [G loss: 2.3321] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.1043] [TV: 0.1504]\n",
      "[Epoch 0/10] [Batch 906/1875] [D loss: 3.2959] [G loss: 2.3162] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0882] [TV: 0.1509]\n",
      "[Epoch 0/10] [Batch 907/1875] [D loss: 3.2962] [G loss: 2.3477] [GAN: 2.1973] [CONST: 0.0002] [TID: 0.1193] [TV: 0.1508]\n",
      "[Epoch 0/10] [Batch 908/1875] [D loss: 3.2961] [G loss: 2.3222] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0944] [TV: 0.1518]\n",
      "[Epoch 0/10] [Batch 909/1875] [D loss: 3.2968] [G loss: 2.3314] [GAN: 2.1974] [CONST: 0.0003] [TID: 0.0973] [TV: 0.1509]\n",
      "[Epoch 0/10] [Batch 910/1875] [D loss: 3.2961] [G loss: 2.3093] [GAN: 2.1971] [CONST: 0.0002] [TID: 0.0799] [TV: 0.1520]\n",
      "[Epoch 0/10] [Batch 911/1875] [D loss: 3.2966] [G loss: 2.3185] [GAN: 2.1973] [CONST: 0.0002] [TID: 0.0913] [TV: 0.1509]\n",
      "[Epoch 0/10] [Batch 912/1875] [D loss: 3.2960] [G loss: 2.3134] [GAN: 2.1970] [CONST: 0.0002] [TID: 0.0878] [TV: 0.1517]\n",
      "[Epoch 0/10] [Batch 913/1875] [D loss: 3.2966] [G loss: 2.3169] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.0869] [TV: 0.1520]\n",
      "[Epoch 0/10] [Batch 914/1875] [D loss: 3.2959] [G loss: 2.2977] [GAN: 2.1969] [CONST: 0.0002] [TID: 0.0705] [TV: 0.1516]\n",
      "[Epoch 0/10] [Batch 915/1875] [D loss: 3.2961] [G loss: 2.3181] [GAN: 2.1971] [CONST: 0.0002] [TID: 0.0900] [TV: 0.1507]\n",
      "[Epoch 0/10] [Batch 916/1875] [D loss: 3.2961] [G loss: 2.3187] [GAN: 2.1970] [CONST: 0.0002] [TID: 0.0921] [TV: 0.1523]\n",
      "[Epoch 0/10] [Batch 917/1875] [D loss: 3.2960] [G loss: 2.3041] [GAN: 2.1971] [CONST: 0.0002] [TID: 0.0759] [TV: 0.1522]\n",
      "[Epoch 0/10] [Batch 918/1875] [D loss: 3.2962] [G loss: 2.3266] [GAN: 2.1971] [CONST: 0.0002] [TID: 0.0980] [TV: 0.1514]\n",
      "[Epoch 0/10] [Batch 919/1875] [D loss: 3.2961] [G loss: 2.3202] [GAN: 2.1971] [CONST: 0.0003] [TID: 0.0902] [TV: 0.1522]\n",
      "[Epoch 0/10] [Batch 920/1875] [D loss: 3.2960] [G loss: 2.3140] [GAN: 2.1971] [CONST: 0.0003] [TID: 0.0820] [TV: 0.1497]\n",
      "[Epoch 0/10] [Batch 921/1875] [D loss: 3.2959] [G loss: 2.3197] [GAN: 2.1971] [CONST: 0.0003] [TID: 0.0885] [TV: 0.1523]\n",
      "[Epoch 0/10] [Batch 922/1875] [D loss: 3.2960] [G loss: 2.3188] [GAN: 2.1971] [CONST: 0.0002] [TID: 0.0896] [TV: 0.1530]\n",
      "[Epoch 0/10] [Batch 923/1875] [D loss: 3.2959] [G loss: 2.3411] [GAN: 2.1971] [CONST: 0.0003] [TID: 0.1060] [TV: 0.1523]\n",
      "[Epoch 0/10] [Batch 924/1875] [D loss: 3.2959] [G loss: 2.3239] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0905] [TV: 0.1517]\n",
      "[Epoch 0/10] [Batch 925/1875] [D loss: 3.2960] [G loss: 2.3305] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.1010] [TV: 0.1531]\n",
      "[Epoch 0/10] [Batch 926/1875] [D loss: 3.2960] [G loss: 2.3241] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0954] [TV: 0.1520]\n",
      "[Epoch 0/10] [Batch 927/1875] [D loss: 3.2959] [G loss: 2.3188] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0940] [TV: 0.1516]\n",
      "[Epoch 0/10] [Batch 928/1875] [D loss: 3.2959] [G loss: 2.3111] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0807] [TV: 0.1502]\n",
      "[Epoch 0/10] [Batch 929/1875] [D loss: 3.2961] [G loss: 2.3486] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1181] [TV: 0.1519]\n",
      "[Epoch 0/10] [Batch 930/1875] [D loss: 3.2963] [G loss: 2.3363] [GAN: 2.1974] [CONST: 0.0003] [TID: 0.1043] [TV: 0.1505]\n",
      "[Epoch 0/10] [Batch 931/1875] [D loss: 3.2959] [G loss: 2.3246] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.0917] [TV: 0.1508]\n",
      "[Epoch 0/10] [Batch 932/1875] [D loss: 3.2960] [G loss: 2.3215] [GAN: 2.1974] [CONST: 0.0003] [TID: 0.0893] [TV: 0.1500]\n",
      "[Epoch 0/10] [Batch 933/1875] [D loss: 3.2960] [G loss: 2.3281] [GAN: 2.1974] [CONST: 0.0004] [TID: 0.0858] [TV: 0.1493]\n",
      "[Epoch 0/10] [Batch 934/1875] [D loss: 3.2959] [G loss: 2.3281] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.0904] [TV: 0.1505]\n",
      "[Epoch 0/10] [Batch 935/1875] [D loss: 3.2959] [G loss: 2.3190] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.0856] [TV: 0.1509]\n",
      "[Epoch 0/10] [Batch 936/1875] [D loss: 3.2959] [G loss: 2.3211] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.0874] [TV: 0.1500]\n",
      "[Epoch 0/10] [Batch 937/1875] [D loss: 3.2959] [G loss: 2.3384] [GAN: 2.1973] [CONST: 0.0002] [TID: 0.1089] [TV: 0.1511]\n",
      "[Epoch 0/10] [Batch 938/1875] [D loss: 3.2959] [G loss: 2.3326] [GAN: 2.1973] [CONST: 0.0002] [TID: 0.1035] [TV: 0.1522]\n",
      "[Epoch 0/10] [Batch 939/1875] [D loss: 3.2960] [G loss: 2.3355] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1031] [TV: 0.1521]\n",
      "[Epoch 0/10] [Batch 940/1875] [D loss: 3.2960] [G loss: 2.3211] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.0878] [TV: 0.1522]\n",
      "[Epoch 0/10] [Batch 941/1875] [D loss: 3.2959] [G loss: 2.3216] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0893] [TV: 0.1513]\n",
      "[Epoch 0/10] [Batch 942/1875] [D loss: 3.2959] [G loss: 2.3080] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0759] [TV: 0.1521]\n",
      "[Epoch 0/10] [Batch 943/1875] [D loss: 3.2959] [G loss: 2.3218] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0869] [TV: 0.1534]\n",
      "[Epoch 0/10] [Batch 944/1875] [D loss: 3.2959] [G loss: 2.3332] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1012] [TV: 0.1513]\n",
      "[Epoch 0/10] [Batch 945/1875] [D loss: 3.2959] [G loss: 2.3483] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1153] [TV: 0.1530]\n",
      "[Epoch 0/10] [Batch 946/1875] [D loss: 3.2959] [G loss: 2.3198] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0833] [TV: 0.1520]\n",
      "[Epoch 0/10] [Batch 947/1875] [D loss: 3.2959] [G loss: 2.3302] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0980] [TV: 0.1528]\n",
      "[Epoch 0/10] [Batch 948/1875] [D loss: 3.2959] [G loss: 2.3104] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0793] [TV: 0.1525]\n",
      "[Epoch 0/10] [Batch 949/1875] [D loss: 3.2959] [G loss: 2.3138] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0842] [TV: 0.1518]\n",
      "[Epoch 0/10] [Batch 950/1875] [D loss: 3.2959] [G loss: 2.3297] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0980] [TV: 0.1531]\n",
      "logged samples\n",
      "[Epoch 0/10] [Batch 951/1875] [D loss: 3.2959] [G loss: 2.3184] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0906] [TV: 0.1537]\n",
      "[Epoch 0/10] [Batch 952/1875] [D loss: 3.2959] [G loss: 2.3288] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0963] [TV: 0.1544]\n",
      "[Epoch 0/10] [Batch 953/1875] [D loss: 3.2959] [G loss: 2.3276] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0953] [TV: 0.1545]\n",
      "[Epoch 0/10] [Batch 954/1875] [D loss: 3.2959] [G loss: 2.3085] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0821] [TV: 0.1530]\n",
      "[Epoch 0/10] [Batch 955/1875] [D loss: 3.2959] [G loss: 2.3253] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0973] [TV: 0.1511]\n",
      "[Epoch 0/10] [Batch 956/1875] [D loss: 3.2959] [G loss: 2.3117] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0840] [TV: 0.1527]\n",
      "[Epoch 0/10] [Batch 957/1875] [D loss: 3.2959] [G loss: 2.3428] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.1131] [TV: 0.1524]\n",
      "[Epoch 0/10] [Batch 958/1875] [D loss: 3.2960] [G loss: 2.3134] [GAN: 2.1973] [CONST: 0.0002] [TID: 0.0894] [TV: 0.1519]\n",
      "[Epoch 0/10] [Batch 959/1875] [D loss: 3.2959] [G loss: 2.3291] [GAN: 2.1973] [CONST: 0.0002] [TID: 0.1015] [TV: 0.1515]\n",
      "[Epoch 0/10] [Batch 960/1875] [D loss: 3.2959] [G loss: 2.3235] [GAN: 2.1973] [CONST: 0.0002] [TID: 0.0951] [TV: 0.1513]\n",
      "[Epoch 0/10] [Batch 961/1875] [D loss: 3.2959] [G loss: 2.3249] [GAN: 2.1973] [CONST: 0.0002] [TID: 0.0983] [TV: 0.1512]\n",
      "[Epoch 0/10] [Batch 962/1875] [D loss: 3.2959] [G loss: 2.3362] [GAN: 2.1973] [CONST: 0.0003] [TID: 0.1062] [TV: 0.1511]\n",
      "[Epoch 0/10] [Batch 963/1875] [D loss: 3.2959] [G loss: 2.3257] [GAN: 2.1973] [CONST: 0.0002] [TID: 0.0973] [TV: 0.1511]\n",
      "[Epoch 0/10] [Batch 964/1875] [D loss: 3.2959] [G loss: 2.3277] [GAN: 2.1973] [CONST: 0.0002] [TID: 0.0985] [TV: 0.1512]\n",
      "[Epoch 0/10] [Batch 965/1875] [D loss: 3.2959] [G loss: 2.3233] [GAN: 2.1973] [CONST: 0.0002] [TID: 0.0958] [TV: 0.1500]\n",
      "[Epoch 0/10] [Batch 966/1875] [D loss: 3.2959] [G loss: 2.3350] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.1054] [TV: 0.1504]\n",
      "[Epoch 0/10] [Batch 967/1875] [D loss: 3.2959] [G loss: 2.3113] [GAN: 2.1973] [CONST: 0.0002] [TID: 0.0840] [TV: 0.1498]\n",
      "[Epoch 0/10] [Batch 968/1875] [D loss: 3.2959] [G loss: 2.3328] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1009] [TV: 0.1505]\n",
      "[Epoch 0/10] [Batch 969/1875] [D loss: 3.2959] [G loss: 2.3316] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.1043] [TV: 0.1514]\n",
      "[Epoch 0/10] [Batch 970/1875] [D loss: 3.2959] [G loss: 2.3511] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1183] [TV: 0.1504]\n",
      "[Epoch 0/10] [Batch 971/1875] [D loss: 3.2959] [G loss: 2.3146] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0825] [TV: 0.1499]\n",
      "[Epoch 0/10] [Batch 972/1875] [D loss: 3.2959] [G loss: 2.3293] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0999] [TV: 0.1502]\n",
      "[Epoch 0/10] [Batch 973/1875] [D loss: 3.2959] [G loss: 2.3221] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0970] [TV: 0.1508]\n",
      "[Epoch 0/10] [Batch 974/1875] [D loss: 3.2959] [G loss: 2.3269] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0974] [TV: 0.1501]\n",
      "[Epoch 0/10] [Batch 975/1875] [D loss: 3.2959] [G loss: 2.3220] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0933] [TV: 0.1500]\n",
      "[Epoch 0/10] [Batch 976/1875] [D loss: 3.2959] [G loss: 2.3068] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0798] [TV: 0.1497]\n",
      "[Epoch 0/10] [Batch 977/1875] [D loss: 3.2959] [G loss: 2.3194] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0850] [TV: 0.1484]\n",
      "[Epoch 0/10] [Batch 978/1875] [D loss: 3.2959] [G loss: 2.3256] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0919] [TV: 0.1499]\n",
      "[Epoch 0/10] [Batch 979/1875] [D loss: 3.2959] [G loss: 2.3198] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0893] [TV: 0.1504]\n",
      "[Epoch 0/10] [Batch 980/1875] [D loss: 3.2959] [G loss: 2.3167] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0863] [TV: 0.1502]\n",
      "[Epoch 0/10] [Batch 981/1875] [D loss: 3.2959] [G loss: 2.3094] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0818] [TV: 0.1496]\n",
      "[Epoch 0/10] [Batch 982/1875] [D loss: 3.2959] [G loss: 2.3234] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0942] [TV: 0.1502]\n",
      "[Epoch 0/10] [Batch 983/1875] [D loss: 3.2959] [G loss: 2.3431] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1107] [TV: 0.1500]\n",
      "[Epoch 0/10] [Batch 984/1875] [D loss: 3.2959] [G loss: 2.3077] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0795] [TV: 0.1491]\n",
      "[Epoch 0/10] [Batch 985/1875] [D loss: 3.2959] [G loss: 2.3100] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0829] [TV: 0.1519]\n",
      "[Epoch 0/10] [Batch 986/1875] [D loss: 3.2959] [G loss: 2.3180] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0875] [TV: 0.1505]\n",
      "[Epoch 0/10] [Batch 987/1875] [D loss: 3.2959] [G loss: 2.3276] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0991] [TV: 0.1499]\n",
      "[Epoch 0/10] [Batch 988/1875] [D loss: 3.2959] [G loss: 2.3056] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0786] [TV: 0.1504]\n",
      "[Epoch 0/10] [Batch 989/1875] [D loss: 3.2959] [G loss: 2.3102] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0852] [TV: 0.1499]\n",
      "[Epoch 0/10] [Batch 990/1875] [D loss: 3.2959] [G loss: 2.3222] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0929] [TV: 0.1493]\n",
      "[Epoch 0/10] [Batch 991/1875] [D loss: 3.2959] [G loss: 2.3357] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.1085] [TV: 0.1498]\n",
      "[Epoch 0/10] [Batch 992/1875] [D loss: 3.2959] [G loss: 2.3100] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0847] [TV: 0.1494]\n",
      "[Epoch 0/10] [Batch 993/1875] [D loss: 3.2959] [G loss: 2.3337] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.1079] [TV: 0.1493]\n",
      "[Epoch 0/10] [Batch 994/1875] [D loss: 3.2959] [G loss: 2.3110] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0810] [TV: 0.1502]\n",
      "[Epoch 0/10] [Batch 995/1875] [D loss: 3.2959] [G loss: 2.3362] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.1069] [TV: 0.1501]\n",
      "[Epoch 0/10] [Batch 996/1875] [D loss: 3.2959] [G loss: 2.3150] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0820] [TV: 0.1501]\n",
      "[Epoch 0/10] [Batch 997/1875] [D loss: 3.2959] [G loss: 2.3126] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0815] [TV: 0.1479]\n",
      "[Epoch 0/10] [Batch 998/1875] [D loss: 3.2959] [G loss: 2.3191] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.0894] [TV: 0.1491]\n",
      "[Epoch 0/10] [Batch 999/1875] [D loss: 3.2959] [G loss: 2.3345] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.1043] [TV: 0.1508]\n",
      "[Epoch 0/10] [Batch 1000/1875] [D loss: 3.2959] [G loss: 2.3298] [GAN: 2.1972] [CONST: 0.0003] [TID: 0.0992] [TV: 0.1487]\n",
      "logged samples\n",
      "âœ… Checkpoint saved: checkpoints/dtn_checkpoint_batch_1000.pth\n",
      "ğŸ¯ Reached target batch 1000, saving checkpoint and stopping...\n",
      "âœ… Checkpoint saved: checkpoints/dtn_checkpoint_batch_1000.pth\n",
      "ğŸš€ Training completed! Final checkpoint: checkpoints/dtn_checkpoint_batch_1000.pth\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>loss/CONST</td><td>â–ˆâ–†â–†â–†â–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>loss/D</td><td>â–ˆâ–…â–„â–‚â–ƒâ–‚â–„â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–â–â–‚â–‚â–â–â–â–â–ƒâ–ƒâ–‚â–„â–‚â–â–â–â–â–â–â–â–â–â–‚â–â–</td></tr><tr><td>loss/G</td><td>â–ˆâ–†â–…â–…â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–â–â–â–‚â–â–â–‚â–‚â–â–â–â–â–â–â–â–</td></tr><tr><td>loss/GAN</td><td>â–ˆâ–„â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>loss/TID</td><td>â–ˆâ–†â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–‚â–â–â–‚â–â–‚</td></tr><tr><td>loss/TV</td><td>â–ˆâ–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>loss/CONST</td><td>0.00026</td></tr><tr><td>loss/D</td><td>3.29587</td></tr><tr><td>loss/G</td><td>2.32977</td></tr><tr><td>loss/GAN</td><td>2.19723</td></tr><tr><td>loss/TID</td><td>0.09923</td></tr><tr><td>loss/TV</td><td>0.14873</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Emoji Creation new</strong> at: <a href='https://wandb.ai/khuengv2007-institut-polytechnique-de-paris/DTN/runs/f9inu44f' target=\"_blank\">https://wandb.ai/khuengv2007-institut-polytechnique-de-paris/DTN/runs/f9inu44f</a><br> View project at: <a href='https://wandb.ai/khuengv2007-institut-polytechnique-de-paris/DTN' target=\"_blank\">https://wandb.ai/khuengv2007-institut-polytechnique-de-paris/DTN</a><br>Synced 5 W&B file(s), 126 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250817_150745-f9inu44f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/10] [Batch 0/1875] [D loss: 3.2959] [G loss: 2.3298] [GAN: 2.1972] [CONST: 0.0002] [TID: 0.1012] [TV: 0.1487]\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "You must call wandb.init() before wandb.log()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/2503615459.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         }\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatches_done\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatches_done\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/preinit.py\u001b[0m in \u001b[0;36mpreinit_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m ) -> Callable:\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreinit_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"You must call wandb.init() before {name}()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mpreinit_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: You must call wandb.init() before wandb.log()"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Táº¡o face features cá»‘ Ä‘á»‹nh Ä‘á»ƒ theo dÃµi cháº¥t lÆ°á»£ng qua cÃ¡c epoch\n",
    "# Sample má»™t batch tá»« source Ä‘á»ƒ táº¡o fixed features\n",
    "fixed_faces, _ = next(iter(source_loader))\n",
    "fixed_faces = fixed_faces[:16].to(device)\n",
    "with torch.no_grad():\n",
    "    z_fixed = f_net(fixed_faces)  # Fixed face features\n",
    "\n",
    "# âœ… THÃŠM: Fixed target images Ä‘á»ƒ so sÃ¡nh\n",
    "fixed_targets, _ = next(iter(target_loader))\n",
    "fixed_targets = fixed_targets[:16].to(device)\n",
    "\n",
    "print(f\"Fixed features shape: {z_fixed.shape}\")\n",
    "\n",
    "# Sá»­a training loop Ä‘á»ƒ dá»«ng á»Ÿ batch 1000:\n",
    "TARGET_BATCH = 1000\n",
    "\n",
    "# Training loop theo style mong muá»‘n\n",
    "for epoch in range(opt.n_epochs):\n",
    "    for i, ((source_imgs, _), (target_imgs, _)) in enumerate(zip(source_loader, target_loader)):\n",
    "\n",
    "        # LÆ°u áº£nh máº«u sau má»—i batches Ä‘Æ°á»£c xá»­ lÃ­\n",
    "        batches_done = epoch * len(source_loader) + i\n",
    "\n",
    "        # Chuáº©n bá»‹ dá»¯ liá»‡u\n",
    "        batch_size = min(source_imgs.size(0), target_imgs.size(0))\n",
    "        source_imgs = source_imgs[:batch_size].to(device)\n",
    "        target_imgs = target_imgs[:batch_size].to(device)\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        #if batches_done % 3 != 0:\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Generate emoji from face features\n",
    "        g_loss, loss_dict = criterion.generator_loss(\n",
    "            d_net, f_net, g_net,\n",
    "            source_imgs, target_imgs, target_imgs\n",
    "        )\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        #if batches_done % 2 == 0:\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            f_x_s = f_net(source_imgs)\n",
    "            f_x_t = f_net(target_imgs)\n",
    "            g_f_x_s = g_net(f_x_s)\n",
    "            g_f_x_t = g_net(f_x_t)\n",
    "\n",
    "            g_f_x_s_up = F.interpolate(g_f_x_s, size=(152, 152), mode='bilinear')\n",
    "            g_f_x_t_up = F.interpolate(g_f_x_t, size=(152, 152), mode='bilinear')\n",
    "\n",
    "        d_loss = criterion.discriminator_loss(\n",
    "            d_net, g_f_x_s_up, g_f_x_t_up, target_imgs\n",
    "        )\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # In ra thÃ´ng tin vá» quÃ¡ trÃ¬nh huáº¥n luyá»‡n\n",
    "        print(\n",
    "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %.4f] [G loss: %.4f] [GAN: %.4f] [CONST: %.4f] [TID: %.4f] [TV: %.4f]\"\n",
    "            % (epoch, opt.n_epochs, i, len(source_loader),\n",
    "                d_loss.item(),\n",
    "                g_loss.item(),\n",
    "                loss_dict[\"gan\"], loss_dict[\"const\"],\n",
    "                loss_dict[\"tid\"], loss_dict[\"tv\"])\n",
    "        )\n",
    "\n",
    "        # Wandb log loss\n",
    "        log_dict = {\n",
    "            \"loss/G\": g_loss.item(),\n",
    "            \"loss/D\": d_loss.item(),\n",
    "            \"loss/GAN\": loss_dict[\"gan\"],\n",
    "            \"loss/CONST\": loss_dict[\"const\"],\n",
    "            \"loss/TID\": loss_dict[\"tid\"],\n",
    "            \"loss/TV\": loss_dict[\"tv\"],\n",
    "            \"epoch\": epoch,\n",
    "        }\n",
    "        wandb.log(log_dict, step=batches_done)\n",
    "\n",
    "        if batches_done % opt.sample_interval == 0:\n",
    "            # Generate current batch samples\n",
    "            with torch.no_grad():\n",
    "                face_features = f_net(source_imgs[:16])\n",
    "                gen_imgs = g_net(face_features)\n",
    "                gen_fixed = g_net(z_fixed)\n",
    "                \n",
    "                # g(f(g(x))) reconstruction\n",
    "                # Tá»« generated images â†’ features â†’ reconstruction \n",
    "                target_up = F.interpolate(target_imgs, size=(152, 152), mode='bilinear')\n",
    "                target_imgs_feature = f_net(target_up)\n",
    "                reconstructed_target = g_net(target_imgs_feature)\n",
    "\n",
    "            # # LÆ°u áº£nh vÃ o local storage\n",
    "            # save_image(gen_imgs.data, \"images/%d.png\" % batches_done, nrow=4, normalize=True)\n",
    "\n",
    "            # Log grid cá»§a áº£nh Ä‘Æ°á»£c gen hiá»‡n táº¡i\n",
    "            gen_imgs_vis = (gen_imgs + 1) / 2  # [-1,1] -> [0,1]\n",
    "            grid_current = make_grid(gen_imgs_vis, nrow=4)\n",
    "            wandb.log({\n",
    "                \"samples/current_batch\": wandb.Image(grid_current, caption=f\"step {batches_done}\")\n",
    "            }, step=batches_done)\n",
    "\n",
    "            # Log grid áº£nh tá»« features cá»‘ Ä‘á»‹nh\n",
    "            with torch.no_grad():\n",
    "                gen_fixed = g_net(z_fixed)\n",
    "            gen_fixed_vis = (gen_fixed + 1)/2\n",
    "            grid_fixed = make_grid(gen_fixed_vis, nrow=4)\n",
    "            wandb.log({\n",
    "                \"samples/fixed_faces\": wandb.Image(grid_fixed, caption=f\"fixed faces @ step {batches_done}\")\n",
    "            }, step=batches_done)\n",
    "\n",
    "            # Log source faces for comparison\n",
    "            source_vis = (fixed_faces + 1) / 2\n",
    "            grid_source = make_grid(source_vis, nrow=4)\n",
    "            wandb.log({\n",
    "                \"samples/fixed_source_faces\": wandb.Image(grid_source, caption=f\"fixed_source faces\")\n",
    "            }, step=batches_done)\n",
    "\n",
    "            # Log source faces for comparison\n",
    "            source_cur = (source_imgs[:16] + 1) / 2\n",
    "            grid_source = make_grid(source_cur, nrow=4)\n",
    "            wandb.log({\n",
    "                \"samples/current_source_faces\": wandb.Image(grid_source, caption=f\"current_source faces\")\n",
    "            }, step=batches_done)\n",
    "\n",
    "            # Log target faces for comparison  \n",
    "            #target_vis = (fixed_targets + 1) / 2\n",
    "            #grid_target = make_grid(target_vis, nrow=4)\n",
    "            #wandb.log({\n",
    "            #    \"samples/fixed_target\": wandb.Image(grid_target, \n",
    "            #        caption=\"Fixed target emoji (reference)\")\n",
    "            #}, step=batches_done)\n",
    "            \n",
    "            # Log current batch target for comparison\n",
    "            current_target_vis = (target_imgs[:16] + 1) / 2\n",
    "            grid_current_target = make_grid(current_target_vis, nrow=4)\n",
    "            wandb.log({\n",
    "                \"samples/current_batch_target\": wandb.Image(grid_current_target,\n",
    "                    caption=f\"Current batch targets @ step {batches_done}\")\n",
    "            }, step=batches_done)\n",
    "\n",
    "            reconstructed_target_vis = (reconstructed_target + 1) / 2\n",
    "            grid_reconstructed_target = make_grid(reconstructed_target_vis, nrow=4)\n",
    "            wandb.log({\n",
    "                \"samples/emoji_to_emoji\": wandb.Image(grid_reconstructed_target,\n",
    "                    caption=f\"Current batch targets @ step {batches_done}\")\n",
    "            }, step=batches_done)\n",
    "\n",
    "            print(\"logged samples\")\n",
    "\n",
    "        # # Break early if we've processed enough batches for demo\n",
    "        # if i >= 100:  # Limit for faster demo\n",
    "        #     break\n",
    "        if batches_done % 100 == 0 and batches_done > 0:\n",
    "            save_checkpoint(g_net, d_net, f_net, optimizer_G, optimizer_D, batches_done, loss_dict)\n",
    "            \n",
    "        # THÃŠM: Check if reached target batch\n",
    "        if batches_done >= TARGET_BATCH:\n",
    "            print(f\"Reached target batch {TARGET_BATCH}, saving checkpoint and stopping...\")\n",
    "            final_checkpoint = save_checkpoint(\n",
    "                g_net, d_net, f_net, \n",
    "                optimizer_G, optimizer_D, \n",
    "                batches_done, loss_dict\n",
    "            )\n",
    "            print(f\"Training completed! Final checkpoint: {final_checkpoint}\")\n",
    "            wandb.finish()\n",
    "            exit()  # Dá»«ng training\n",
    "            break\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-17T15:32:37.548478Z",
     "iopub.status.idle": "2025-08-17T15:32:37.548815Z",
     "shell.execute_reply": "2025-08-17T15:32:37.548657Z",
     "shell.execute_reply.started": "2025-08-17T15:32:37.548640Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import thÆ° viá»‡n cáº§n thiáº¿t\n",
    "import os\n",
    "from IPython.display import FileLink, display\n",
    "\n",
    "# ÄÆ°á»ng dáº«n Ä‘áº¿n checkpoint file\n",
    "checkpoint_path = \"checkpoints/dtn_checkpoint_batch_200.pth\"\n",
    "\n",
    "# Method 1: Direct download náº¿u file Ä‘Ã£ trong /kaggle/working\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Found checkpoint: {checkpoint_path}\")\n",
    "    \n",
    "    # Táº¡o download link trá»±c tiáº¿p\n",
    "    display(FileLink(checkpoint_path))\n",
    "    \n",
    "    print(\"ğŸ”— Click vÃ o link phÃ­a trÃªn Ä‘á»ƒ download file!\")\n",
    "else:\n",
    "    print(f\"Checkpoint not found at: {checkpoint_path}\")\n",
    "    # List files trong thÆ° má»¥c checkpoints\n",
    "    if os.path.exists(\"checkpoints\"):\n",
    "        print(\"Available checkpoints:\")\n",
    "        for f in os.listdir(\"checkpoints\"):\n",
    "            print(f\"  - {f}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1071415,
     "sourceId": 1803299,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1307206,
     "sourceId": 2177371,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
